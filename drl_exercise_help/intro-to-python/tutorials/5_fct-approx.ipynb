{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurableMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(ConfigurableMLP, self).__init__()\n",
    "        layers = []\n",
    "        input_size = config['input_size']\n",
    "        \n",
    "        for layer_idx, hidden_size in enumerate(config['hidden_sizes']):\n",
    "            layers.append(nn.Linear(input_size, hidden_size))\n",
    "            activation = config['activations'][layer_idx]\n",
    "            if activation == 'relu':\n",
    "                layers.append(nn.ReLU())\n",
    "            elif activation == 'tanh':\n",
    "                layers.append(nn.Tanh())\n",
    "            elif activation == 'sigmoid':\n",
    "                layers.append(nn.Sigmoid())\n",
    "            elif activation == 'none':  # No activation\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported activation: {activation}\")\n",
    "\n",
    "            input_size = hidden_size\n",
    "        \n",
    "        # Final output layer\n",
    "        layers.append(nn.Linear(input_size, config['output_size']))\n",
    "        final_activation = config.get('final_activation', 'none')\n",
    "        if final_activation == 'softmax':\n",
    "            layers.append(nn.Softmax(dim=1))\n",
    "        elif final_activation == 'none':\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation: {final_activation}\")\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "        print(self)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurableCNN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(ConfigurableCNN, self).__init__()\n",
    "        layers = []\n",
    "        input_channels = config['input_channels']\n",
    "        \n",
    "        # CNN layers\n",
    "        for _, layer_config in enumerate(config['conv_layers']):\n",
    "            layers.append(nn.Conv2d(\n",
    "                input_channels,\n",
    "                layer_config['out_channels'],\n",
    "                kernel_size=layer_config['kernel_size'],\n",
    "                stride=layer_config.get('stride', 1),\n",
    "                padding=layer_config.get('padding', 0)\n",
    "            ))\n",
    "            activation = layer_config['activation']\n",
    "            if activation == 'relu':\n",
    "                layers.append(nn.ReLU())\n",
    "            elif activation == 'tanh':\n",
    "                layers.append(nn.Tanh())\n",
    "            elif activation == 'sigmoid':\n",
    "                layers.append(nn.Sigmoid())\n",
    "            elif activation == 'none':\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported activation: {activation}\")\n",
    "            \n",
    "            input_channels = layer_config['out_channels']\n",
    "        \n",
    "        self.conv = nn.Sequential(*layers)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(config['flattened_size'], config['output_size'])\n",
    "        )\n",
    "        final_activation = config.get('final_activation', 'none')\n",
    "        if final_activation == 'softmax':\n",
    "            self.fc.add_module(\"final_softmax\", nn.Softmax(dim=1))\n",
    "        elif final_activation == 'none':\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation: {final_activation}\")\n",
    "\n",
    "        print(self)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note on Loss Calculation**\n",
    " - regression tasks -> e.g. MSELoss (Mean Squared Error)\n",
    " - classification tasks -> e.g. CrossEntropyLoss (Cross Entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "def train_model(model, loader, criterion, optimizer, epochs=5):\n",
    "    model.train()   # make sure that weights are not frozen\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        # Loop over each batch\n",
    "        for inputs, targets in loader:\n",
    "            optimizer.zero_grad()                   # clear all gradients before backpropagation\n",
    "            outputs = model(inputs)                 # forward pass\n",
    "            loss = criterion(outputs, targets)      # calculate the loss (error) using a specified loss function\n",
    "            loss.backward()                         # compute the gradients of the loss with respect to all the model's parameters using backpropagation\n",
    "            optimizer.step()                        # update the models weights and biases\n",
    " \n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(loader):.4f}\")\n",
    "\n",
    "\n",
    "# Create dummy data for regression\n",
    "def create_dummy_data(input_shape, num_samples, output_size):\n",
    "    x = torch.rand(num_samples, *input_shape)\n",
    "    y = torch.randint(0, output_size, (num_samples,)) if output_size > 1 else torch.rand(num_samples, 1)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Training Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConfigurableMLP(\n",
      "  (network): Sequential(\n",
      "    (0): Linear(in_features=20, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Training MLP for Regression...\n",
      "Epoch 1, Loss: 0.4986\n",
      "Epoch 2, Loss: 0.4207\n",
      "Epoch 3, Loss: 0.2991\n",
      "Epoch 4, Loss: 0.1791\n",
      "Epoch 5, Loss: 0.1034\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Regression Example\n",
    "##\n",
    "\n",
    "# Generate Dummy Data \n",
    "mlp_regression_X, mlp_regression_y = create_dummy_data((20,), 100, 1)  # 20 input features, 1 continuous output\n",
    "mlp_regression_dataset = TensorDataset(mlp_regression_X, mlp_regression_y)\n",
    "mlp_regression_loader = DataLoader(mlp_regression_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Setup Training\n",
    "mlp_regression_config = {\n",
    "    'input_size': 20,                   # Number of input features\n",
    "    'hidden_sizes': [64, 32],           # Two hidden layers with 64 and 32 neurons\n",
    "    'activations': ['relu', 'relu'],    # Activation for each hidden layer\n",
    "    'output_size': 1,                   # Regression task: Single continuous output\n",
    "    'final_activation': 'none'          # No activation for the final layer\n",
    "}\n",
    "\n",
    "mlp_regression_model = ConfigurableMLP(mlp_regression_config)\n",
    "mlp_regression_criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
    "mlp_regression_optimizer = optim.Adam(mlp_regression_model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the MLP\n",
    "print(\"Training MLP for Regression...\")\n",
    "train_model(mlp_regression_model, mlp_regression_loader, mlp_regression_criterion, mlp_regression_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 20])\n",
      "ConfigurableMLP(\n",
      "  (network): Sequential(\n",
      "    (0): Linear(in_features=20, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=32, out_features=3, bias=True)\n",
      "    (5): Softmax(dim=1)\n",
      "  )\n",
      ")\n",
      "Training MLP for Classification...\n",
      "Epoch 1, Loss: 1.0990\n",
      "Epoch 2, Loss: 1.0989\n",
      "Epoch 3, Loss: 1.0963\n",
      "Epoch 4, Loss: 1.0960\n",
      "Epoch 5, Loss: 1.0950\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Classification Example\n",
    "##\n",
    "\n",
    "# Generate Dummy Data \n",
    "mlp_x, mlp_y = create_dummy_data((20,), 100, 3)  # 20 input features, 3 output classes\n",
    "print(mlp_x.shape)\n",
    "mlp_dataset = TensorDataset(mlp_x, mlp_y)\n",
    "mlp_loader = DataLoader(mlp_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Setup Training\n",
    "mlp_config = {\n",
    "    'input_size': 20,\n",
    "    'hidden_sizes': [64, 32],           # Two hidden layers\n",
    "    'activations': ['relu', 'relu'],    # Activation for each hidden layer\n",
    "    'output_size': 3,                   # For classification, 3 classes\n",
    "    'final_activation': 'softmax'       # Final activation is softmax\n",
    "}\n",
    "mlp_model = ConfigurableMLP(mlp_config)\n",
    "mlp_criterion = nn.CrossEntropyLoss()  # Suitable for classification\n",
    "mlp_optimizer = optim.Adam(mlp_model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the MLP\n",
    "print(\"Training MLP for Classification...\")\n",
    "train_model(mlp_model, mlp_loader, mlp_criterion, mlp_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConfigurableCNN(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): Tanh()\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=32768, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Training CNN for Regression...\n",
      "Epoch 1, Loss: 0.6076\n",
      "Epoch 2, Loss: 0.2010\n",
      "Epoch 3, Loss: 0.1165\n",
      "Epoch 4, Loss: 0.0643\n",
      "Epoch 5, Loss: 0.0330\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Regression Example\n",
    "##\n",
    "\n",
    "# Generate dummy regression data\n",
    "cnn_regression_X, cnn_regression_y = create_dummy_data((3, 32, 32), 100, 1)     # 3-channel 32x32 images, 1 continuous output\n",
    "cnn_regression_dataset = TensorDataset(cnn_regression_X, cnn_regression_y)\n",
    "cnn_regression_loader = DataLoader(cnn_regression_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "cnn_regression_config = {\n",
    "    'input_channels': 3,                # Number of input channels (e.g., RGB images)\n",
    "    'conv_layers': [\n",
    "        {'out_channels': 16, 'kernel_size': 3, 'stride': 1, 'padding': 1, 'activation': 'relu'},\n",
    "        {'out_channels': 32, 'kernel_size': 3, 'stride': 1, 'padding': 1, 'activation': 'tanh'}\n",
    "    ],\n",
    "    'flattened_size': 32 * 32 * 32,     # Input size flattened after convolution (32x32 input size)\n",
    "    'output_size': 1,                   # Regression task: Single continuous output\n",
    "    'final_activation': 'none'          # No activation for the final layer\n",
    "}\n",
    "\n",
    "cnn_regression_model = ConfigurableCNN(cnn_regression_config)\n",
    "cnn_regression_criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
    "cnn_regression_optimizer = optim.Adam(cnn_regression_model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the CNN\n",
    "print(\"\\nTraining CNN for Regression...\")\n",
    "train_model(cnn_regression_model, cnn_regression_loader, cnn_regression_criterion, cnn_regression_optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConfigurableCNN(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): Tanh()\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=32768, out_features=3, bias=True)\n",
      "    (final_softmax): Softmax(dim=1)\n",
      "  )\n",
      ")\n",
      "\n",
      "Training CNN for Classification...\n",
      "Epoch 1, Loss: 1.1947\n",
      "Epoch 2, Loss: 1.0755\n",
      "Epoch 3, Loss: 1.0473\n",
      "Epoch 4, Loss: 0.9272\n",
      "Epoch 5, Loss: 0.9047\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Classification Example\n",
    "##\n",
    "\n",
    "# Create Dummy Data\n",
    "cnn_x, cnn_y = create_dummy_data((3, 32, 32), 100, 3)               # 3 channels, 32x32 images, 3 output classes\n",
    "cnn_dataset = TensorDataset(cnn_x, cnn_y)\n",
    "cnn_loader = DataLoader(cnn_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Setup Training\n",
    "cnn_config = {\n",
    "    'input_channels': 3,\n",
    "    'conv_layers': [\n",
    "        {'out_channels': 16, 'kernel_size': 3, 'stride': 1, 'padding': 1, 'activation': 'relu'},\n",
    "        {'out_channels': 32, 'kernel_size': 3, 'stride': 1, 'padding': 1, 'activation': 'tanh'}\n",
    "    ],\n",
    "    'flattened_size': 32 * 32 * 32,     # Input size (32x32) flattened after final conv layer\n",
    "    'output_size': 3,                   # For classification, 3 classes\n",
    "    'final_activation': 'softmax'       # Final activation is softmax\n",
    "}\n",
    "cnn_model = ConfigurableCNN(cnn_config)\n",
    "cnn_criterion = nn.CrossEntropyLoss()  # Suitable for classification\n",
    "cnn_optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"\\nTraining CNN for Classification...\")\n",
    "train_model(cnn_model, cnn_loader, cnn_criterion, cnn_optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Multi-Armed Bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 1.1: Der K-armige Bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a distribution super class so that we can implement different kinds of distributions later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Distribution(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def __init__(self, *args):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_value(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_params(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_name(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def adapt(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the super class, create two specific distributions, the Bernoulli and Normal Distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernoulliDistribution(Distribution):\n",
    "    def __init__(self, *args):\n",
    "        if len(args) == 1:\n",
    "            self.p = args[0]\n",
    "        else:\n",
    "            self.p = round(random.random(), 1)\n",
    "\n",
    "\n",
    "    def get_value(self):\n",
    "        \"\"\"\n",
    "        Returns 1 with probability p, which has been instantiated randomly\n",
    "        \"\"\"\n",
    "        random_value = random.random()\n",
    "        hit = random_value < self.p\n",
    "        return float(hit)\n",
    "    \n",
    "    def get_params(self):\n",
    "        return [self.p]\n",
    "    \n",
    "    def get_name(self):\n",
    "        return 'Bernoulli'\n",
    "    \n",
    "    def adapt(self):\n",
    "        self.p += np.random.normal(0.0, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalDistribution(Distribution):\n",
    "    def __init__(self, *args):\n",
    "        if len(args) == 2:\n",
    "            self.mean = args[0]\n",
    "            self.standard_deviation = args[1]\n",
    "        else:\n",
    "            self.mean = random.uniform(1, 5)\n",
    "            self.standard_deviation = random.uniform(0.2, 3)\n",
    "\n",
    "    def get_value(self):\n",
    "        return np.random.normal(self.mean, self.standard_deviation)\n",
    "    \n",
    "    def get_params(self):\n",
    "        return [self.mean, self.standard_deviation]\n",
    "    \n",
    "    def get_name(self):\n",
    "        return 'Normal'\n",
    "    \n",
    "    def adapt(self):\n",
    "        self.mean += np.random.normal(0.0, 2)\n",
    "        self.standard_deviation += max(np.random.normal(0.0, 0.5), 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create the bandit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class kArmedBandit(object):\n",
    "    \n",
    "    def __init__(self, epsilon : float, arms : list, initial_Q_value = 0.0):\n",
    "        \n",
    "        self.initial_epsilon = epsilon\n",
    "        self.arms = arms\n",
    "\n",
    "        k = len(arms)\n",
    "\n",
    "        self.Q = np.array([initial_Q_value] * k)\n",
    "        self.N = np.zeros(k)\n",
    "\n",
    "        self.G = 0\n",
    "        self.regret = 0\n",
    "\n",
    "        self.true_expected_values = [dist.get_params()[0] for dist in arms]\n",
    "        self.a_star_index = np.argmax(self.true_expected_values)\n",
    "        self.num_picks_a_star = 0\n",
    "\n",
    "        print(f'Initialized bandit using {arms[0].get_name()} distribution, initial epsilon: {self.initial_epsilon}, true expected values: {np.round(self.true_expected_values, 1)}')\n",
    "\n",
    "        self.first_pick = True\n",
    "\n",
    "    def get_new_epsilon(self, num_picks : int):\n",
    "        \"\"\"\n",
    "        Get a new value for epsilon. If you don't want to change epsilon, just type return self.initial_epsilon\n",
    "        \"\"\"\n",
    "        #return self.initial_epsilon * np.exp(-0.005*num_picks)\n",
    "        return self.initial_epsilon\n",
    "\n",
    "    def pick(self, verbose = 0):\n",
    "        \n",
    "        # decide on explore vs exploit\n",
    "        index = int(round(random.uniform(0, len(self.arms)-1), 0))\n",
    "\n",
    "        epsilon = self.get_new_epsilon(np.sum(self.N))\n",
    "        exploit = (random.random() > epsilon)\n",
    "        if exploit and not self.first_pick:\n",
    "            index = np.argmax(self.Q)\n",
    "\n",
    "        self.first_pick = False\n",
    "\n",
    "        # update percentual best hit\n",
    "        if index == self.a_star_index:\n",
    "            self.num_picks_a_star += 1\n",
    "\n",
    "        # check whether we got a hit\n",
    "        a = self.arms[index]\n",
    "        R = a.get_value()\n",
    "        \n",
    "        # update cumulative metrics\n",
    "        self.G += R\n",
    "        self.regret += self.true_expected_values[self.a_star_index] - self.true_expected_values[index]\n",
    "\n",
    "        # update Q\n",
    "        self.N[index] += 1\n",
    "        self.Q[index] = self.Q[index] + 1/self.N[index] * (R - self.Q[index])\n",
    "\n",
    "        if verbose >= 1:\n",
    "            print(f'Picked from arm {index} and got reward {R}, Exploited: {exploit}')\n",
    "            if verbose >= 2:\n",
    "                print(f'Q: {self.Q}, \\nN: {self.N}')\n",
    "            print('-'*10)\n",
    "\n",
    "\n",
    "        return self.regret, self.G, self.num_picks_a_star / np.sum(self.N), self.G / np.sum(self.N)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of distributions, the bandits \"arms\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "#arms = [NormalDistribution() for _ in range(k)]\n",
    "arms = [NormalDistribution(2.62, 2.78),\n",
    "        NormalDistribution(1.35, 0.93),\n",
    "        NormalDistribution(4.62, 2.19),\n",
    "        NormalDistribution(3.31, 1.28)]\n",
    "epsilon_values = [0.01, 0.1, 0.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create the bandit(s) with the before specified epsilon values and let them pick:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandits = [kArmedBandit(epsilon, arms, 0.0) for epsilon in epsilon_values]\n",
    "\n",
    "num_rolls = 1000\n",
    "\n",
    "averaged_cumulative_regret = []\n",
    "averaged_cumulative_reward = []\n",
    "averaged_percentual_best_pick = []\n",
    "\n",
    "num_runs = 1\n",
    "\n",
    "for i in range(num_runs):\n",
    "    cumulative_regret = [[] for _ in bandits]\n",
    "    cumulative_reward = [[] for _ in bandits]\n",
    "    percentual_best_pick = [[] for _ in bandits]\n",
    "\n",
    "    for _ in range(num_rolls):\n",
    "        for bandit_index in range(len(bandits)):\n",
    "            bandit = bandits[bandit_index]\n",
    "            regret, G, current_percentual_best_pick, avg_reward = bandit.pick(verbose = 0)\n",
    "            \n",
    "            cumulative_regret[bandit_index].append(regret)\n",
    "            cumulative_reward[bandit_index].append(G)\n",
    "            percentual_best_pick[bandit_index].append(current_percentual_best_pick)\n",
    "    \n",
    "    averaged_cumulative_regret.append(cumulative_regret)\n",
    "    averaged_cumulative_reward.append(cumulative_reward)\n",
    "    averaged_percentual_best_pick.append(percentual_best_pick)\n",
    "\n",
    "\n",
    "averaged_cumulative_regret = np.average(averaged_cumulative_regret, axis=0)\n",
    "averaged_cumulative_reward = np.average(averaged_cumulative_reward, axis=0)\n",
    "averaged_percentual_best_pick = np.average(averaged_percentual_best_pick, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "t = np.linspace(0, num_rolls, num_rolls)\n",
    "\n",
    "fig, ax = plt.subplots(3, figsize=(15,10))\n",
    "\n",
    "for bandit_index in range(len(bandits)):\n",
    "    ax[0].plot(t, averaged_cumulative_reward[bandit_index], label = f'Epsilon = {epsilon_values[bandit_index]}')\n",
    "    ax[1].plot(t, averaged_cumulative_regret[bandit_index], label = f'Epsilon = {epsilon_values[bandit_index]}')\n",
    "    ax[2].plot(t, averaged_percentual_best_pick[bandit_index], label = f'Epsilon = {epsilon_values[bandit_index]}')\n",
    "\n",
    "ax[0].set_xlabel('Steps')\n",
    "ax[0].set_ylabel('Cumulative Reward')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].set_xlabel('Steps')\n",
    "ax[1].set_ylabel('Cumulative Regret')\n",
    "ax[1].legend()\n",
    "\n",
    "ax[2].set_xlabel('Steps')\n",
    "ax[2].set_ylabel('Percentual best pick')\n",
    "ax[2].legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 1.2: Nicht-Stationärer Bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ziel ist es, den oben dargestellten Banditen-Algorithmus so anzupassen, dass er auf das nicht-stationäre Banditenproblem anwendbar ist. Das nicht-stationäre Problem meint, dass sich die Wahrscheinlichkeitsverteilungen der einzelnen Möglkichkeiten über die Zeit ändern können. Da die oben aufgeführten Klassen zur Modellierung der Wahrscheinlichkeitsverteilungen bereits eine Methode zur Veränderung bereitstellen, können diese Klassen hier weiterverwendet werden, nur der Bandit muss angepasst werden, um die Veränderunng der Verteilungen zu triggern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NSkArmedBandit(object):\n",
    "    \n",
    "    def __init__(self, epsilon : float, arms : list, initial_Q_value = 0.0, change_interval = 5):\n",
    "        \n",
    "        self.initial_epsilon = epsilon\n",
    "        self.arms = arms\n",
    "\n",
    "        k = len(arms)\n",
    "\n",
    "        self.Q = np.array([initial_Q_value] * k)\n",
    "        self.N = np.zeros(k)\n",
    "\n",
    "        self.G = 0\n",
    "        self.regret = 0\n",
    "\n",
    "        self.true_expected_values = [dist.get_params()[0] for dist in arms]\n",
    "        self.a_star_index = np.argmax(self.true_expected_values)\n",
    "        self.num_picks_a_star = 0\n",
    "\n",
    "        print(f'Initialized bandit using {arms[0].get_name()} distribution, initial epsilon: {self.initial_epsilon}, true expected values: {np.round(self.true_expected_values, 1)}')\n",
    "\n",
    "        self.first_pick = True\n",
    "        self.change_interval = change_interval\n",
    "\n",
    "    def update_a_star_index(self):\n",
    "        self.true_expected_values = [dist.get_params()[0] for dist in self.arms]\n",
    "        self.a_star_index = np.argmax(self.true_expected_values)\n",
    "\n",
    "    def pick(self, verbose = 0):\n",
    "        \n",
    "        # decide on explore vs exploit\n",
    "        index = int(round(random.uniform(0, len(self.arms)-1), 0))\n",
    "\n",
    "        epsilon = self.initial_epsilon\n",
    "        exploit = (random.random() > epsilon)\n",
    "        if exploit and not self.first_pick:\n",
    "            index = np.argmax(self.Q)\n",
    "\n",
    "        self.first_pick = False\n",
    "\n",
    "        # update percentual best hit\n",
    "        if index == self.a_star_index:\n",
    "            self.num_picks_a_star += 1\n",
    "\n",
    "        # check whether we got a hit\n",
    "        a = self.arms[index]\n",
    "        R = a.get_value()\n",
    "        \n",
    "        # update cumulative metrics\n",
    "        self.G += R\n",
    "        self.regret += self.true_expected_values[self.a_star_index] - self.true_expected_values[index]\n",
    "\n",
    "        # update Q\n",
    "        self.N[index] += 1\n",
    "        self.Q[index] = self.Q[index] + 1/self.N[index] * (R - self.Q[index])\n",
    "\n",
    "        a_star_index_before_update = self.a_star_index\n",
    "        # model the non-stationarity\n",
    "        if np.sum(self.N) % self.change_interval == 0:\n",
    "            if verbose == 1:\n",
    "                print('Changing probability distributions...')\n",
    "            for arm in self.arms:\n",
    "                arm.adapt()\n",
    "            self.update_a_star_index()\n",
    "\n",
    "        # optional print statements\n",
    "        if verbose >= 1:\n",
    "            print(f'Picked from arm {index} and got reward {R}, Exploited: {exploit}')\n",
    "            if verbose >= 2:\n",
    "                print(f'Q: {self.Q}, \\nN: {self.N}')\n",
    "            print('-'*10)\n",
    "\n",
    "\n",
    "        return self.N, R, self.G, index, a_star_index_before_update, self.Q, self.true_expected_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_stationary_bandit = NSkArmedBandit(epsilon=0.1, arms=arms, initial_Q_value=0.0, change_interval=5)\n",
    "\n",
    "num_rolls = 1000\n",
    "\n",
    "relative_picks = [[] for _ in range(k)]\n",
    "rewards = []\n",
    "cumulative_rewards = []\n",
    "best_index = []\n",
    "picked_index = []\n",
    "expected_estimates = [[] for _ in range(k)]\n",
    "true_expected = [[] for _ in range(k)]\n",
    "\n",
    "for _ in range(num_rolls):\n",
    "    N, R, G, index, a_star_index, Q, true_expected_values = non_stationary_bandit.pick(verbose = 0)\n",
    "    \n",
    "    total_num_picks = np.sum(N)\n",
    "    for num_picks_index in range(len(N)):\n",
    "        relative_picks_per_index = N[num_picks_index] / total_num_picks\n",
    "        relative_picks[num_picks_index].append(relative_picks_per_index)\n",
    "\n",
    "    for Q_index in range(len(Q)):\n",
    "        expected_estimates[Q_index].append(Q[Q_index])\n",
    "        true_expected[Q_index].append(true_expected_values[Q_index])\n",
    "\n",
    "    rewards.append(R)\n",
    "    cumulative_rewards.append(G)\n",
    "    best_index.append(a_star_index)\n",
    "    picked_index.append(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "t = np.linspace(0, num_rolls, num_rolls)\n",
    "\n",
    "fig, ax = plt.subplots(4, figsize=(20,15))\n",
    "\n",
    "ax[0].plot(t, rewards)\n",
    "ax[1].plot(t, cumulative_rewards)\n",
    "\n",
    "for index in range(len(relative_picks)):\n",
    "    picks_per_index = np.multiply(relative_picks[index], 100)\n",
    "    ax[2].plot(t, picks_per_index, label = f'Index = {index}')\n",
    "\n",
    "ax[3].plot(t, best_index, label = 'Index of best choice')\n",
    "ax[3].plot(t, picked_index, label = 'Actually picked index')\n",
    "\n",
    "ax[0].set_xlabel('Steps')\n",
    "ax[0].set_ylabel('Reward per Step')\n",
    "\n",
    "ax[1].set_xlabel('Steps')\n",
    "ax[1].set_ylabel('Cumulative Reward')\n",
    "\n",
    "ax[2].set_xlabel('Steps')\n",
    "ax[2].set_ylabel('Relateive distributions of index picks [%]')\n",
    "ax[2].legend()\n",
    "\n",
    "ax[3].set_xlabel('Steps')\n",
    "ax[3].set_ylabel('Index')\n",
    "ax[3].legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the true expected value vs. the estimated expected value per index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "t = np.linspace(0, num_rolls, num_rolls)\n",
    "\n",
    "fig, ax = plt.subplots(4, figsize=(20,15))\n",
    "\n",
    "for idx in range(len(expected_estimates)):\n",
    "    ax[idx].plot(t, expected_estimates[idx], label='Estimated')\n",
    "    ax[idx].plot(t, true_expected[idx], label='True')\n",
    "    ax[idx].set_xlabel('Steps')\n",
    "    ax[idx].set_ylabel(f'Reward for choice {idx}')\n",
    "    ax[idx].legend()\n",
    "\n",
    "#ax[4].plot(t, best_index)\n",
    "#ax[4].set_xlabel('Steps')\n",
    "#ax[4].set_ylabel('Index of best choice')\n",
    "#ax[4].grid()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_reinforcement",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

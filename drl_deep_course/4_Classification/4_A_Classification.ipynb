{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 4 A - MNIST Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.A.1\n",
    "\n",
    "### Introduction to Classification\n",
    "\n",
    "Classification is a supervised machine learning task where the goal is to categorize data into predefined classes or labels. In a classification problem, the model learns from labeled data to assign labels to new, unseen instances (generalization) based on their features. We will will use neural network based models that output probabilities for possible classes, helping make informed decisions based on the predicted categories.\n",
    "\n",
    "### The MNIST Dataset\n",
    "\n",
    "The MNIST dataset (Modified National Institute of Standards and Technology) is a classic dataset widely used for training and testing machine learning models in image classification tasks. It consists of 70,000 grayscale images of handwritten digits from 0 to 9, split into:\n",
    "\n",
    "- 60,000 training images\n",
    "- 10,000 test images\n",
    "\n",
    "Each image in the dataset is 28x28 pixels, unrolled into a 784-dimensional vector for modeling (our input space). This dataset serves as a benchmark for machine learning algorithms, as it presents a relatively simple, yet meaningful task: identifying handwritten digits. Despite its simplicity, the MNIST dataset is a great starting point for building and evaluating classification models.\n",
    "\n",
    "**Explanation:** \n",
    "\n",
    "* Loading the Dataset: The `fetch_openml` function fetches the MNIST dataset with 784 features per sample (28x28 pixels).\n",
    "* `as_frame=False` ensures that the data is loaded as arrays, which is easier to work with for image processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "# The astype forces a conversion to a class as an int (not a string)\n",
    "X, y = mnist[\"data\"], mnist[\"target\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task:\n",
    "\n",
    "* Understand the structure of the data (How many images are there? What are they dimensions? What is the order of the images? See above for answers.)\n",
    "* Visualize for each class 10 examples in a plot that shows $10 \\times 10$ images\n",
    "* Extract from the data a set that only contains two classes (e.g., $0$ and $1$) and\n",
    "* Split this into a training and a test set (you can, but don't have to stick to the MNIST test data for your test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HINT: When you plot many images, you probably will want to use a subplot grid:\n",
    "#fig, axes = plt.subplots(10, 10, figsize=(12, 12))\n",
    "#fig.suptitle(\"MNIST Digits (10 examples per digit)\", fontsize=16)\n",
    "\n",
    "# Plot the image in the corresponding subplot\n",
    "# ax = axes[x, y]\n",
    "# x.imshow(image, cmap=\"gray\")\n",
    "# ax.axis(\"off\")\n",
    "# with image as below.\n",
    "\n",
    "# TODO: Iterate over the ten different classes and pick ten random examples for each class.\n",
    "\n",
    "# Choose a random index\n",
    "print(X.shape)\n",
    "random_index = np.random.randint(0, X.shape[0])\n",
    "\n",
    "# Reshape the 784-pixel data to a 28x28 image\n",
    "image = X[random_index].reshape(28, 28)\n",
    "\n",
    "# Get the corresponding label\n",
    "label = y[random_index]\n",
    "\n",
    "# Plot the image with its label\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "plt.title(f\"Label: {label}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_two_classes(X, y, digit1, digit2, num_train=1000, num_test=1000):\n",
    "    \"\"\"\n",
    "    Filter MNIST data for two specified digits and split into train and test sets.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: np.array, feature data for all samples (e.g., from MNIST)\n",
    "    - y: np.array, target labels for all samples (e.g., from MNIST)\n",
    "    - digit1: int, first digit to filter (0-9)\n",
    "    - digit2: int, second digit to filter (0-9)\n",
    "    - num_train: int, number of samples to return in the training set (default: 1000)\n",
    "    - num_test: int, number of samples to return in the test set (default: 1000)\n",
    "    \n",
    "    Returns:\n",
    "    - X_train, y_train: training data and labels\n",
    "    - X_test, y_test: test data and labels\n",
    "    \"\"\"\n",
    "    # Filter the data for the specified digits\n",
    "    filter_mask = (y == digit1) | (y == digit2)\n",
    "    X_filtered = X[filter_mask]\n",
    "    y_filtered = y[filter_mask]\n",
    "    \n",
    "    # TODO: Re-label y to binary (0 for digit1, 1 for digit2)\n",
    "    # Iterate over the filtered data - and do a binary labelling\n",
    "    y_binary ...\n",
    "\n",
    "    # TODO: Shuffle and split into training and test sets\n",
    "    # You might use indices = np.random.permutation(len(X_filtered))\n",
    "    \n",
    "    X_train, y_train = ...\n",
    "    X_test, y_test = ... \n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function to filter digits 0 and 1\n",
    "X_train, y_train, X_test, y_test = load_two_classes(X, y, 1, 4, num_train=1000, num_test=1000)\n",
    "\n",
    "# Visualize examples from each class in the training set\n",
    "fig, axes = plt.subplots(2, 10, figsize=(10, 3))\n",
    "fig.suptitle(\"Random Examples of Digits 0 and 1\")\n",
    "\n",
    "# Plot 10 random images for each digit in the training set\n",
    "for digit in [0, 1]:\n",
    "    digit_class = 0 if digit == 0 else 1\n",
    "    digit_indices = np.where(y_train == digit_class)[0]\n",
    "    selected_indices = np.random.choice(digit_indices, 10, replace=False)\n",
    "    \n",
    "    for i, idx in enumerate(selected_indices):\n",
    "        ax = axes[digit, i]\n",
    "        ax.imshow(X_train[idx].reshape(28, 28), cmap=\"gray\")\n",
    "        ax.axis(\"off\")\n",
    "        if i == 0:\n",
    "            ax.set_ylabel(f\"Digit {digit}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.8)\n",
    "plt.show()\n",
    "\n",
    "print('Shapes of the data sets: ', X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4.A.2 Activation Function\n",
    "\n",
    "The sigmoid activation function is a commonly used activation function in neural networks. It maps any input value to an output between $0$ and $1$, making it particularly useful for binary classification tasks. The function is defined by the formula:\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "where $x$ is the input to the function.\n",
    "\n",
    "#### Why Use the Sigmoid Function?\n",
    "\n",
    "1. **Range-Bounded**: The output of the sigmoid function is always between 0 and 1, which makes it useful for probabilities.\n",
    "2. **Differentiable**: The sigmoid function is smooth and differentiable, allowing us to compute gradients for optimization.\n",
    "3. **Non-linear**: It introduces non-linearity into the network, enabling the model to learn complex patterns.\n",
    "\n",
    "#### Derivative of the Sigmoid Function\n",
    "\n",
    "The derivative of the sigmoid function with respect to its input $x$ is:\n",
    "\n",
    "$$\n",
    "\\sigma'(x) = \\sigma(x) \\cdot (1 - \\sigma(x))\n",
    "$$\n",
    "\n",
    "This derivative is crucial in backpropagation, as it helps calculate the gradient of the loss function with respect to the weights in the network.\n",
    "\n",
    "#### Task: Implement a Sigmoid Activation Function Class\n",
    "\n",
    "Your goal is to create a class `SigmoidActivationFunction` that implements the sigmoid activation function and its derivative. \n",
    "\n",
    "This class should have the following methods:\n",
    "\n",
    "* `__call__(self, x)`: This method should calculate and return the output of the sigmoid function for a given input $x$.\n",
    "* `derivative(self, x)`: This method should calculate and return the derivative of the sigmoid function for a given input \\( x \\).\n",
    "\n",
    "In addition:\n",
    "\n",
    "* visualize and plot the sigmoid function and the derivative of the sigmoid (using 'linspace' and consider different ranges);\n",
    "\n",
    "**Questions**\n",
    "\n",
    "* What kind of problems or drawbacks could we experience when using a sigmoid function? Where does the computation might become difficult?\n",
    "* For the derivative: Consider the form of the function -- how does this influence learning when using gradient descent (which is based on the derivative)? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Hint: Problems of sigmoid activation function</summary>\n",
    "    <p>Sigmoid functions can lead to **vanishing gradients** when the input values become very large or very small, making the gradients close to zero. In this case, the gradient does not push the learner towards better solutions.</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SigmoidActivationFunction:\n",
    "    \"\"\"\n",
    "    A class implementing the sigmoid activation function and its derivative.\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Calculate the sigmoid activation function for a given input x.\n",
    "\n",
    "        Parameters:\n",
    "        x : np.array or float\n",
    "            The input value(s) to the sigmoid function.\n",
    "\n",
    "        Returns:\n",
    "        np.array or float\n",
    "            The sigmoid function output for each element in x.\n",
    "        \"\"\"\n",
    "        # TODO: Implement sigmoid function\n",
    "        return (x) # TODO\n",
    "\n",
    "    def derivative(self, x):\n",
    "        \"\"\"\n",
    "        Calculate the derivative of the sigmoid function for a given input x.\n",
    "\n",
    "        Parameters:\n",
    "        x : np.array or float\n",
    "            The input value(s) to the sigmoid function.\n",
    "\n",
    "        Returns:\n",
    "        np.array or float\n",
    "            The derivative of the sigmoid function for each element in x.\n",
    "        \"\"\"\n",
    "        # TODO: Implement derivative of sigmoid.\n",
    "        return (x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Instantiate the SigmoidActivationFunction class\n",
    "sigmoid_function = SigmoidActivationFunction()\n",
    "\n",
    "# Define the range of x values\n",
    "x = np.linspace(-10, 10, 100)\n",
    "\n",
    "# Use the class to calculate the sigmoid function and its derivative\n",
    "sigmoid = sigmoid_function(x)\n",
    "sigmoid_derivative = sigmoid_function.derivative(x)\n",
    "\n",
    "# Plot the sigmoid function\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, sigmoid, label='Sigmoid Function', color='blue')\n",
    "\n",
    "# Plot the derivative of the sigmoid function\n",
    "plt.plot(x, sigmoid_derivative, label='Sigmoid Derivative', color='red')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Input (x)')\n",
    "plt.ylabel('Output')\n",
    "plt.title('Sigmoid Function and Its Derivative')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4.A.3 Single Layer Neural Network Model\n",
    "\n",
    "As a next step, we are now setting up our neural network. The neural network model is a simple, single-layer neural network, also known as a **perceptron**. It consists of two main components:\n",
    "\n",
    "1. **Weighted Sum**: Each input feature is multiplied by a weight, and the results are summed together along with a bias term. This produces a linear combination of the inputs, which we can think of as a linear regression model.\n",
    "   \n",
    "   Mathematically, for an input vector \\( X = [x_1, x_2, \\dots, x_n] \\) and weight vector \\( W = [w_1, w_2, \\dots, w_n] \\), the output before activation, \\( z \\), is calculated as:\n",
    "   \\[\n",
    "   z = W \\cdot X + b\n",
    "   \\]\n",
    "   where \\( b \\) is the bias term.\n",
    "\n",
    "2. **Activation Function**: After computing the weighted sum, we apply a **non-linear activation function** (in this case, the sigmoid function) to introduce non-linearity into the model. The sigmoid function is defined as:\n",
    "   \\[\n",
    "   \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "   \\]\n",
    "   This activation function maps \\( z \\) to a value between 0 and 1, making it useful for tasks like binary classification.\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "Your task is to take the given linear regression model and adapt it into a neural network model by introducing an activation function.\n",
    "\n",
    "* Add the Activation Function: Modify the forward pass of the model to apply the sigmoid activation function after computing the weighted sum.\n",
    "* Implement Backpropagation with Activation: Update the model’s fitting process by including the derivative of the activation function in the backward pass. This will allow the model to adjust the weights based on the sigmoid-activated output.\n",
    "* Train your model and measure training and test error. Provide these in a plot.\n",
    "* **Task B** Implement your `fit` method in a batch fashion, i.e, updating the gradient after seeing a couple (= the batch size) of samples from the training data set.\n",
    "\n",
    "The goal is to adapt a linear regression model to a neural network with non-linear output, allowing it to learn patterns that a simple linear model cannot capture.\n",
    "\n",
    "**Further observations:**\n",
    "\n",
    "Choose different combinations of digits for the two classes:\n",
    "\n",
    "* one that should be easy to discriminate,\n",
    "* one that should be very difficult to discriminate\n",
    "\n",
    "Why do you consider these easy or difficult? What are the training and test error? You might have to tune the learning parameters again!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of Gradient Calculation in the Backward Pass\n",
    "\n",
    "In a single-layer neural network with sigmoid activation, our goal is to minimize the **Mean Squared Error (MSE)** loss by adjusting the weights using gradient descent. Here’s a breakdown of how we calculate the gradient of the MSE with respect to the weights, applying the chain rule due to the non-linear sigmoid activation, working our way **backwards** through the network and calculation. In a way, for each step in our calculation in the forward pass, we are considering how this has to change in a way that the error is getting smaller (negative gradient is pointing in the direction of smaller errors).\n",
    "\n",
    "We define:\n",
    "- $y$ as the true labels\n",
    "- $\\hat{y}$ as the predicted output from the model, after applying the activation function: $\\hat{y} = \\sigma(z) = \\sigma(W \\cdot X + b )$\n",
    "  \n",
    "where:\n",
    "  - $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ is the sigmoid activation function\n",
    "  - $z = W \\cdot X + b$ represents the linear combination of inputs $X$ with weights $W$ and bias $b$.\n",
    "\n",
    "To train the model, we need to calculate the gradient of the **MSE loss** with respect to each weight in $W$ (including the bias term $b$).\n",
    "\n",
    "The **Mean Squared Error** is defined as:\n",
    "$$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "where $n$ is the number of samples. To minimize this loss, we need to compute the gradient of the MSE with respect to each weight in $W$, which requires the chain rule due to the non-linear sigmoid activation.\n",
    "\n",
    "### Step-by-Step Gradient Derivation (going backwards through our network)\n",
    "\n",
    "1. **Gradient of the Activation Function (Sigmoid)**:\n",
    "\n",
    "   Since our output $\\hat{y}$ is computed by applying the sigmoid activation function to $z = W \\cdot X + b$, the derivative of $\\hat{y} = \\sigma(z)$ with respect to $z$ is:\n",
    "   \n",
    "   $$\n",
    "   \\frac{d \\sigma}{d z} = \\sigma(z) \\cdot (1 - \\sigma(z))\n",
    "   $$\n",
    "   \n",
    "   This derivative, $\\sigma(z) \\cdot (1 - \\sigma(z))$, scales the gradient during backpropagation. It represents how the output of the activation function changes with respect to its input $z$.\n",
    "\n",
    "2. **Gradient of the MSE with Respect to the Predictions $\\hat{y}$**:\n",
    "\n",
    "   Next, we calculate the gradient of the MSE with respect to $\\hat{y}$, our predictions. This is the partial derivative of the MSE loss with respect to each predicted output:\n",
    "   \n",
    "   $$\n",
    "   \\frac{\\partial \\text{MSE}}{\\partial \\hat{y}} = \\frac{2}{n} (\\hat{y} - y)\n",
    "   $$\n",
    "   \n",
    "   The factor of 2 can be omitted as we are interested in the direction and relative magnitude of gradients (it is usually simply absorbed into the learning rate), leaving:\n",
    "   \n",
    "   $$\n",
    "   \\frac{\\partial \\text{MSE}}{\\partial \\hat{y}} = \\hat{y} - y\n",
    "   $$\n",
    "\n",
    "3. **Applying the Chain Rule**:\n",
    "\n",
    "   To find the gradient of the MSE with respect to the linear combination $z$ (the input to the sigmoid), we apply the chain rule:\n",
    "   \n",
    "   $$\n",
    "   \\frac{\\partial \\text{MSE}}{\\partial z} = \\frac{\\partial \\text{MSE}}{\\partial \\hat{y}} \\cdot \\frac{d \\hat{y}}{d z}\n",
    "   $$\n",
    "   \n",
    "   Substituting from the previous steps:\n",
    "   \n",
    "   $$\n",
    "   \\frac{\\partial \\text{MSE}}{\\partial z} = (\\hat{y} - y) \\cdot \\sigma(z) \\cdot (1 - \\sigma(z))\n",
    "   $$\n",
    "   \n",
    "   where:\n",
    "   - $\\frac{\\partial \\text{MSE}}{\\partial \\hat{y}} = \\hat{y} - y$\n",
    "   - $\\frac{d \\hat{y}}{d z} = \\sigma(z) \\cdot (1 - \\sigma(z))$\n",
    "\n",
    "4. **Gradient of the MSE with Respect to the Weights \\( W \\)**:\n",
    "\n",
    "   Finally, we need the gradient of the MSE with respect to each weight $w_j \\in W$. Using the chain rule again:\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial \\text{MSE}}{\\partial W} = \\frac{\\partial \\text{MSE}}{\\partial z} \\cdot \\frac{\\partial z}{\\partial W}\n",
    "   $$\n",
    "\n",
    "   Since $z = W \\cdot X + b$, the derivative of $z$ with respect to $W$ is simply $X$. Therefore:\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial \\text{MSE}}{\\partial W} = (\\hat{y} - y) \\cdot \\sigma(z) \\cdot (1 - \\sigma(z)) \\cdot X\n",
    "   $$\n",
    "\n",
    "   This represents the gradient of the MSE loss with respect to each weight. By calculating this across all samples and averaging, we obtain the mean gradient, which can then be used to update the weights during gradient descent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class SingleLayerNeuralNetwork:\n",
    "    \"\"\"\n",
    "    A simplified neural network model with one layer that performs gradient descent.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, learning_rate=0.01, n_iterations=1000):\n",
    "        self.learning_rate = learning_rate  # Set learning rate for gradient descent\n",
    "        self.n_iterations = n_iterations  # Set number of iterations for gradient descent\n",
    "        self.weights = np.random.rand(1, input_dim + 1) * 0.0001  # Initialize weights with input dimension (+1 for bias)\n",
    "        self.mse_history = []  # List to store MSE at each iteration\n",
    "        self.activation_function = SigmoidActivationFunction()  # Use the SigmoidActivationFunction class\n",
    "\n",
    "    def add_bias_column(self, X):\n",
    "        \"\"\"\n",
    "        Add a bias column (of ones) to the input data.\n",
    "        \n",
    "        Parameters:\n",
    "        X : np.array, shape (n_samples, input_dim)\n",
    "            The input feature values.\n",
    "            \n",
    "        Returns:\n",
    "        X_with_bias : np.array, shape (n_samples, input_dim + 1)\n",
    "            The input with an added bias column.\n",
    "        \"\"\"\n",
    "        X_with_bias = np.c_[np.ones(X.shape[0]), X]\n",
    "        return X_with_bias\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Perform the forward pass through the single layer.\n",
    "        \n",
    "        Parameters:\n",
    "        X : np.array, shape (n_samples, input_dim)\n",
    "            The input feature values.\n",
    "            \n",
    "        Returns:\n",
    "        layer_output : np.array\n",
    "            Output of the layer after applying the activation function.\n",
    "        \"\"\"\n",
    "        # TODO: Implement the two steps =\n",
    "        # 1. weighted sum of inputs \n",
    "        # 2. Applying Activation function\n",
    "        # Use the activation function class\n",
    "        return layer_output\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions using the trained model.\n",
    "        \"\"\"\n",
    "        X_with_bias = self.add_bias_column(X)  # Add bias column\n",
    "        return self.forward(X_with_bias)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the model using gradient descent, adjusting weights iteratively.\n",
    "        \"\"\"\n",
    "        y = y.reshape(-1, 1)  # Ensure y is a column vector (n_samples, 1)\n",
    "        X_with_bias = self.add_bias_column(X)\n",
    "        for _ in range(self.n_iterations):    \n",
    "            # Use the forward pass to get the predictions\n",
    "            layer_output = self.forward(X_with_bias)\n",
    "\n",
    "            # Calculate Mean Squared Error and store it\n",
    "            mse = mean_squared_error(y, layer_output)\n",
    "            self.mse_history.append(mse)\n",
    "            \n",
    "            # Backward pass: calculate the gradient for the weights\n",
    "            layer_input = np.dot(X_with_bias, self.weights.T)  # Linear combination\n",
    "            # TODO: Calculate the backward pass using the chain rule\n",
    "            # 1. Derive the derivative of the error wrt. what was comming out of the calculation above\n",
    "            # = the output of the activation function\n",
    "            # This delta consists (as in the linear case) of the derivative of the MSE \n",
    "            # but remember that there is now an activation function involved\n",
    "            # -> therefore, you have to use the inner derivative as well of this function\n",
    "            # Step backwards through the activation function (using the derivative) \n",
    "            delta = ...\n",
    "\n",
    "            # 2. Now, calculate this to the first part of the forward step,\n",
    "            # i.e. the weighted sum (as before)\n",
    "            # Compute the gradient of the weights (nabla), averaged across the data\n",
    "            gradient = np.dot(delta.T, X_with_bias) / X.shape[0]\n",
    "\n",
    "            # Update weights by moving in the direction of the negative gradient\n",
    "            self.weights -= self.learning_rate * gradient\n",
    "\n",
    "    \n",
    "    def fit_batch(self, X, y, batch_size=32):\n",
    "        \"\"\"\n",
    "        THIS IS USED LATER. TASK B\n",
    "        Fit the model using mini-batch gradient descent with specified batch size.\n",
    "        \n",
    "        Parameters:\n",
    "        X : np.array, shape (n_samples, input_dim)\n",
    "            The input feature values.\n",
    "        y : np.array, shape (n_samples,)\n",
    "            The true target values.\n",
    "        batch_size : int\n",
    "            The size of each mini-batch for training (default is 32).\n",
    "        \"\"\"\n",
    "        y = y.reshape(-1, 1)  # Ensure y is a column vector (n_samples, 1)\n",
    "        X_with_bias = self.add_bias_column(X)  # Add bias column to the input data\n",
    "        \n",
    "        n_samples = X.shape[0]\n",
    "        for _ in range(self.n_iterations):\n",
    "            # Shuffle the data at the beginning of each epoch\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_shuffled = X_with_bias[indices]\n",
    "            y_shuffled = y[indices]\n",
    "            \n",
    "            # TODO: Process each mini-batch, run in a loop through the data set\n",
    "            \n",
    "            # Calculate Mean Squared Error over the entire dataset for monitoring\n",
    "            epoch_output = self.forward(X_with_bias)\n",
    "            mse = mean_squared_error(y, epoch_output)\n",
    "            self.mse_history.append(mse)\n",
    "\n",
    "    def get_mse_history(self):\n",
    "        \"\"\"\n",
    "        Get the history of MSE values during training.\n",
    "        \"\"\"\n",
    "        return self.mse_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize and train the SimpleNeuralNetwork model\n",
    "input_dim = X_train.shape[1]\n",
    "model = SingleLayerNeuralNetwork(input_dim=input_dim, learning_rate=0.001, n_iterations=100)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# TODO: Make predictions on the test set\n",
    "y_pred = ...\n",
    "y_pred = (y_pred > 0.5).astype(int)  # Apply threshold to convert probabilities to binary labels\n",
    "\n",
    "# Calculate accuracy \n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# TODO TASK B: Initialize and train the model with mini-batch fit method\n",
    "# Make predictions on the test set using the batch model\n",
    "\n",
    "print(f\"Accuracy on test set (standard): {accuracy * 100:.2f}%\")\n",
    "\n",
    "\n",
    "# Plot MSE history for both models\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot MSE for the standard fit method\n",
    "plt.plot(model.get_mse_history(), label=\"Standard Fit\")\n",
    "\n",
    "# Plot MSE for the batch fit method\n",
    "# TODO TASK B\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.title(\"MSE During Training\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Number of examples to display\n",
    "num_examples = 10\n",
    "\n",
    "# Select random indices from the test set\n",
    "random_indices = np.random.choice(len(X_test), num_examples, replace=False)\n",
    "\n",
    "# Get the selected test images, true labels, and predictions\n",
    "selected_images = X_test[random_indices]\n",
    "true_labels = y_test[random_indices]\n",
    "predicted_labels = (model.predict(selected_images) > 0.5).astype(int)\n",
    "\n",
    "# Plot the examples\n",
    "fig, axes = plt.subplots(1, num_examples, figsize=(15, 3))\n",
    "fig.suptitle(\"Test Examples with True and Predicted Labels\", fontsize=16)\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    # Reshape the image to 28x28 for display\n",
    "    ax.imshow(selected_images[i].reshape(28, 28), cmap=\"gray\")\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(f\"True: {true_labels[i]}\\nPred: {predicted_labels[i][0]}\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.A.4 Understanding Unbalanced Data Sets\n",
    "\n",
    "An **unbalanced data set** occurs when the classes in the data are not represented equally. For example, in our binary classification problem, if one class significantly outnumbers the other, the data set is said to be unbalanced. This imbalance can create challenges in training machine learning models because the model may become biased toward the more frequent class, potentially leading to poor performance on the underrepresented class.\n",
    "\n",
    "Your Task is to create an unbalanced data set and train our model on this unbalanced data set. Afterwards, you should analyse how this impacts behavior of the model\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "1. **Data Preparation**:\n",
    "   - Choose two digits from the MNIST data set, such as **0 and 1** (for a simpler task) or **1 and 4** (for a slightly more challenging one).\n",
    "   - Create an unbalanced data set with 1000 examples of one digit (majority class) and only 100 examples of the other digit (minority class).\n",
    "\n",
    "2. **Training and Evaluation**:\n",
    "   - Train a classifier on this unbalanced data set, using the simple neural network.\n",
    "   - Calculate accuracy and take again a look at examples.\n",
    "\n",
    "This task will help you understand the effects of unbalanced data on model training and how to assess model performance accurately in such situations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_two_classes_unbalanced(X, y, digit1, digit2, num_train_majority=1000, num_train_minority=100, num_test=1000):\n",
    "    \"\"\"\n",
    "    Filter MNIST data for two specified digits and split into an unbalanced train and balanced test set.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: np.array, feature data for all samples (e.g., from MNIST)\n",
    "    - y: np.array, target labels for all samples (e.g., from MNIST)\n",
    "    - digit1: int, first digit to filter (0-9), will be the majority class\n",
    "    - digit2: int, second digit to filter (0-9), will be the minority class\n",
    "    - num_train_majority: int, number of samples for the majority class in the training set\n",
    "    - num_train_minority: int, number of samples for the minority class in the training set\n",
    "    - num_test: int, total number of samples to return in the test set (balanced across both classes)\n",
    "    \n",
    "    Returns:\n",
    "    - X_train, y_train: unbalanced training data and labels\n",
    "    - X_test, y_test: balanced test data and labels\n",
    "    \"\"\"\n",
    "    # TODO: \n",
    "    # Filter out the samples for the two given digits. \n",
    "    # Create training data set of the appropriate size.  \n",
    "\n",
    "    majority_mask = (y == digit1)\n",
    "    minority_mask = (y == digit2)\n",
    "    \n",
    "    X_majority, y_majority = X[majority_mask], y[majority_mask]\n",
    "    X_minority, y_minority = X[minority_mask], y[minority_mask]\n",
    "    \n",
    "    # Sample a balanced test set\n",
    "    num_test_per_class = num_test // 2\n",
    "    X_test_majority = X_majority[np.random.permutation(len(X_majority))[:num_test_per_class]]\n",
    "    X_test_minority = X_minority[np.random.permutation(len(X_minority))[:num_test_per_class]]\n",
    "    \n",
    "    X_test = np.concatenate([X_test_majority, X_test_minority], axis=0)\n",
    "    y_test = np.concatenate([np.zeros(len(X_test_majority)), np.ones(len(X_test_minority))], axis=0)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# Use the function to filter digits 1 and 4, creating an unbalanced training set\n",
    "X_train, y_train, X_test, y_test = load_two_classes_unbalanced(X, y, 1, 4, num_train_majority=1000, num_train_minority=100, num_test=1000)\n",
    "\n",
    "# Visualize examples from each class in the training set\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 10, figsize=(10, 3))\n",
    "fig.suptitle(\"Random Examples of Digits 1 (Majority) and 4 (Minority)\")\n",
    "\n",
    "# Plot 10 random images for each digit in the training set\n",
    "for digit in [1, 4]:\n",
    "    digit_class = 0 if digit == 1 else 1\n",
    "    digit_indices = np.where(y_train == digit_class)[0]\n",
    "    selected_indices = np.random.choice(digit_indices, 10, replace=False)\n",
    "    \n",
    "    for i, idx in enumerate(selected_indices):\n",
    "        ax = axes[0 if digit == 1 else 1, i]\n",
    "        ax.imshow(X_train[idx].reshape(28, 28), cmap=\"gray\")\n",
    "        ax.axis(\"off\")\n",
    "        if i == 0:\n",
    "            ax.set_ylabel(f\"Digit {digit}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.8)\n",
    "plt.show()\n",
    "\n",
    "print('Shapes of the data sets: ', X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# TODO: Initialize and train the SingleLayerNeuralNetwork model\n",
    "input_dim = X_train.shape[1]\n",
    "...\n",
    "\n",
    "# TODO: Make predictions on the test set\n",
    "\n",
    "# Calculate accuracy on the test set\n",
    "accuracy_test = accuracy_score(y_test, y_pred)\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train_binary)\n",
    "print(f\"Accuracy on test set: {accuracy_test * 100:.2f}%\")\n",
    "print(f\"Accuracy on training set: {accuracy_train * 100:.2f}%\")\n",
    "\n",
    "# Plot MSE history for the model\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(model.get_mse_history(), label=\"Training MSE\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.title(\"MSE During Training\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Display 10 random examples from the test set with true and predicted labels\n",
    "num_examples = 10\n",
    "random_indices = np.random.choice(len(X_test), num_examples, replace=False)\n",
    "\n",
    "# Get the selected test images, true labels, and predictions\n",
    "selected_images = X_test[random_indices]\n",
    "true_labels = y_test[random_indices]\n",
    "predicted_labels = y_pred_train_binary[random_indices]\n",
    "\n",
    "# Plot the examples\n",
    "fig, axes = plt.subplots(1, num_examples, figsize=(15, 3))\n",
    "fig.suptitle(\"Test Examples with True and Predicted Labels\", fontsize=16)\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    # Reshape the image to 28x28 for display\n",
    "    ax.imshow(selected_images[i].reshape(28, 28), cmap=\"gray\")\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(f\"True: {true_labels[i]}\\nPred: {predicted_labels[i][0]}\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with unbalanced data:\n",
    "1. **Accuracy can be misleading**: The model may achieve high accuracy by simply predicting the majority class more often, even if it performs poorly on the minority class.\n",
    "2. **Evaluation Metrics**: Metrics like precision, recall, F1-score, and area under the ROC curve (AUC) are often better suited than accuracy to measure model performance on unbalanced data.\n",
    "3. **Strategies**: Techniques such as re-sampling, using class weights, or balancing algorithms can help mitigate the imbalance, but it’s important to understand the underlying challenge first.\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "* For a better understanding how the model fails, have a look at a confusion matrix (computed below). What does the model actually do when trained on an unbalanced data set?\n",
    "* Calculate the other mentioned metrics: precision, recall, F1-score. \n",
    "   * Understand what these mean, see [sklearn documentation](https://scikit-learn.org/1.5/auto_examples/model_selection/plot_precision_recall.html) and for [F1](https://scikit-learn.org/1.5/modules/generated/sklearn.metrics.f1_score.html). What is your expectation for the bad model from above? How will these values look?\n",
    "   * Compute the values on the test set.\n",
    "\n",
    "**Analysis**:\n",
    "   - Examine the classifier’s performance and discuss how the unbalanced data impacts the results.\n",
    "   - Consider ways to adjust the model or data to improve performance on the minority class (e.g., by using class weights, oversampling, or undersampling techniques)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Compute the confusion matrix: On the test and train set. \n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Display the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "print(y_test.shape, y_pred.shape)\n",
    "# TODO: Calculate precision, recall, and F1-score on the test set\n",
    "\n",
    "print(f\"Precision on test set: \", precision)\n",
    "print(f\"Recall on test set: \", recall)\n",
    "print(f\"F1-score on test set: \", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right;\">\n",
    "<i>This notebook has been created with the help of ChatGPT-4, 30.10.2024; Explanations were initially generated and afterwards edited;</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

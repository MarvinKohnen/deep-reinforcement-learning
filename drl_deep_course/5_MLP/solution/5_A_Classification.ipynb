{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 5 A - Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.A.1 Non-linear Classification Task\n",
    "\n",
    "In this notebook, we will explore a non-linear classification problem using a synthetic dataset. This dataset consists of two classes arranged in opposite corners of a 2D space:\n",
    "\n",
    "* Class 0 occupies the upper-left and lower-right quadrants.\n",
    "* Class 1 occupies the lower-left and upper-right quadrants.\n",
    "\n",
    "Unlike linearly separable datasets, this pattern creates a non-linear decision boundary. Traditional linear classifiers like logistic regression struggle with such data because a straight line cannot effectively separate the classes. Instead, weâ€™ll explore how adding a hidden layer in the neural network can be used to classify this dataset accurately by learning a complex, non-linear decision boundary.\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "First, we start with the data generation:\n",
    "\n",
    "* Initially, use the simple, linear separable data set. Go once through the different steps (this is the simple Neural Network model with sigmoid activation) and look at the results.\n",
    "* Second, switch to the four blob data set (in the section below, you have to uncomment the line that assigns four centers). You can switch to the non-linear classification task by adding more centers for generating the data. \n",
    "\n",
    "Input for the data is two dimensional and there are two classes (as a target value $y$). This is visualized below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define custom centers for the dataset\n",
    "centers = [(-2, 2), (-2, -2)]  # Two centers\n",
    "# TODO: Switch towards non-linear classification task.\n",
    "centers = [(-2, 2), (-2, -2), (2, 2), (2, -2)]  # Four centers\n",
    "cluster_std = 1.0  # Standard deviation for each cluster\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X, y = make_blobs(n_samples=1000, centers=centers, cluster_std=cluster_std, random_state=42)\n",
    "\n",
    "# Map clusters to two classes: 0 and 1\n",
    "# Assign class 0 to clusters 0 and 1, and class 1 to clusters 2 and 3\n",
    "y = (y == 1) | (y == 2)\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Plot the dataset\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(X_train[y_train == 0][:, 0], X_train[y_train == 0][:, 1], color='C0', alpha=0.7, label='Class 0 - Train')\n",
    "plt.scatter(X_train[y_train == 1][:, 0], X_train[y_train == 1][:, 1], color='C1', alpha=0.7, label='Class 1 - Train')\n",
    "plt.scatter(X_test[y_test == 0][:, 0], X_test[y_test == 0][:, 1], color='C0', alpha=0.3, marker='x', label='Class 0 - Test')\n",
    "plt.scatter(X_test[y_test == 1][:, 0], X_test[y_test == 1][:, 1], color='C1', alpha=0.3, marker='x', label='Class 1 - Test')\n",
    "\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.title('Scatter Plot of the Custom-Patterned Dataset')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying our simple Neural Network Model\n",
    "\n",
    "Below is the (known) activation function and the known Neural Network model which is then trained on the data sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SigmoidActivationFunction:\n",
    "    \"\"\"\n",
    "    A class implementing the sigmoid activation function and its derivative.\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Calculate the sigmoid activation function for a given input x.\n",
    "\n",
    "        Parameters:\n",
    "        x : np.array or float\n",
    "            The input value(s) to the sigmoid function.\n",
    "\n",
    "        Returns:\n",
    "        np.array or float\n",
    "            The sigmoid function output for each element in x.\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def derivative(self, x):\n",
    "        \"\"\"\n",
    "        Calculate the derivative of the sigmoid function for a given input x.\n",
    "\n",
    "        Parameters:\n",
    "        x : np.array or float\n",
    "            The input value(s) to the sigmoid function.\n",
    "\n",
    "        Returns:\n",
    "        np.array or float\n",
    "            The derivative of the sigmoid function for each element in x.\n",
    "        \"\"\"\n",
    "        sigmoid_x = self.__call__(x)\n",
    "        return sigmoid_x * (1 - sigmoid_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class NeuralNetworkClass:\n",
    "    \"\"\"\n",
    "    A neural network model with a configurable number of layers, using gradient descent for optimization.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, learning_rate=0.01, n_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        # Initialize weights as a list to allow multiple layers in the future\n",
    "        self.weights = [np.random.rand(1, input_dim + 1) * 0.0001]  # Start with one layer's weights\n",
    "        self.mse_history = []  # List to store MSE at each iteration\n",
    "        self.activation_function = SigmoidActivationFunction()  # Use the SigmoidActivationFunction class\n",
    "\n",
    "    def add_bias_column(self, X):\n",
    "        \"\"\"\n",
    "        Add a bias column (of ones) to the input data.\n",
    "        \n",
    "        Parameters:\n",
    "        X : np.array, shape (n_samples, input_dim)\n",
    "            The input feature values.\n",
    "            \n",
    "        Returns:\n",
    "        X_with_bias : np.array, shape (n_samples, input_dim + 1)\n",
    "            The input with an added bias column.\n",
    "        \"\"\"\n",
    "        X_with_bias = np.c_[np.ones(X.shape[0]), X]\n",
    "        return X_with_bias\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Perform the forward pass through the 0-th layer.\n",
    "        \n",
    "        Parameters:\n",
    "        X : np.array, shape (n_samples, input_dim + 1)\n",
    "            The input data with bias.\n",
    "        \n",
    "        Returns:\n",
    "        layer_output : np.array\n",
    "            Output of the final layer after applying the activation function.\n",
    "        \"\"\"\n",
    "        layer_input = np.dot(X, self.weights[0].T)\n",
    "        layer_output = self.activation_function(layer_input)\n",
    "        return layer_output\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions using the trained model with only the 0-th layer.\n",
    "        \"\"\"\n",
    "        X_with_bias = self.add_bias_column(X)  # Add bias column\n",
    "        return self.forward(X_with_bias)  # Forward pass through only the 0-th layer\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the model using gradient descent, adjusting weights iteratively.\n",
    "        \n",
    "        Parameters:\n",
    "        X : np.array, shape (n_samples, input_dim)\n",
    "            The input feature values.\n",
    "        y : np.array, shape (n_samples,)\n",
    "            The true target values.\n",
    "        \"\"\"\n",
    "        y = y.reshape(-1, 1)  # Ensure y is a column vector (n_samples, 1)\n",
    "        X_with_bias = self.add_bias_column(X)  # Add bias column to the input data\n",
    "        \n",
    "        for _ in range(self.n_iterations):\n",
    "            # Forward pass through the 0-th layer\n",
    "            layer_output = self.forward(X_with_bias)\n",
    "\n",
    "            # Calculate Mean Squared Error and store it\n",
    "            mse = mean_squared_error(y, layer_output)\n",
    "            self.mse_history.append(mse)\n",
    "            \n",
    "            # Backward pass for the 0-th layer\n",
    "            layer_input = np.dot(X_with_bias, self.weights[0].T)  # Linear combination\n",
    "            delta = (layer_output - y) * self.activation_function.derivative(layer_input)\n",
    "            gradient = np.dot(delta.T, X_with_bias) / X.shape[0]\n",
    "            \n",
    "            # Update weights by moving in the direction of the negative gradient\n",
    "            self.weights[0] -= self.learning_rate * gradient\n",
    "\n",
    "    def get_mse_history(self):\n",
    "        \"\"\"\n",
    "        Get the history of MSE values during training.\n",
    "        \n",
    "        Returns:\n",
    "        mse_history : list of float\n",
    "            List containing the MSE at each iteration during training.\n",
    "        \"\"\"\n",
    "        return self.mse_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize and train the NeuralNetworkClass model\n",
    "input_dim = X_train.shape[1]\n",
    "model = NeuralNetworkClass(input_dim=input_dim, learning_rate=0.02, n_iterations=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Plot the learning curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(model.get_mse_history(), label=\"Standard Fit\", color=\"blue\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.title(\"MSE During Training (Standard Fit)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_labels = (y_pred > 0.5).astype(int)  # Apply threshold to convert probabilities to binary labels\n",
    "\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_train_labels = (y_pred_train > 0.5).astype(int)  # Apply threshold to convert probabilities to binary labels\n",
    "\n",
    "# Calculate accuracy for information\n",
    "accuracy = accuracy_score(y_test, y_pred_labels)\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train_labels)\n",
    "print(f\"Accuracy on test set : {accuracy * 100:.2f}%\")\n",
    "print(f\"Accuracy on train set: {accuracy_train * 100:.2f}%\")\n",
    "\n",
    "# Separate correctly and incorrectly classified points, keeping class colors\n",
    "correct_x, correct_y, correct_colors = [], [], []\n",
    "incorrect_x, incorrect_y, incorrect_colors = [], [], []\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    if y_pred_labels[i] == y_test[i]:  # Correct classification\n",
    "        correct_x.append(X_test[i, 0])\n",
    "        correct_y.append(X_test[i, 1])\n",
    "        correct_colors.append('blue' if y_test[i] == 0 else 'orange')\n",
    "    else:  # Incorrect classification\n",
    "        incorrect_x.append(X_test[i, 0])\n",
    "        incorrect_y.append(X_test[i, 1])\n",
    "        incorrect_colors.append('blue' if y_test[i] == 0 else 'orange')\n",
    "\n",
    "# Plot decision boundary and test data points\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Plot correctly classified points (circles) and incorrectly classified points (crosses) with class color\n",
    "plt.scatter(correct_x, correct_y, c=correct_colors, marker='o', label='Correctly Classified')\n",
    "plt.scatter(incorrect_x, incorrect_y, c=incorrect_colors, marker='x', label='Incorrectly Classified')\n",
    "\n",
    "# Adding legend for color and boundary\n",
    "correct_class_label = plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='blue', markersize=10, label='Class 0 (Correct)')\n",
    "incorrect_class_label = plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='orange', markersize=10, label='Class 1 (Correct)')\n",
    "plt.legend(handles=[correct_class_label, incorrect_class_label])\n",
    "\n",
    "# Decision boundary line\n",
    "w0, w1  = model.weights[0][0][0:2]  # Extract bias and weights\n",
    "wb = model.weights[0][0][-1]\n",
    "x_vals = np.array([X_test[:, 0].min() - 1, X_test[:, 0].max() + 1])  # Range for x-axis\n",
    "y_vals = - (w0 + w1 * x_vals)/wb   # Calculate corresponding y values for decision boundary\n",
    "\n",
    "# Plot the decision boundary\n",
    "plt.plot(x_vals, y_vals, color=\"black\", linestyle=\"--\", linewidth=2, label=\"Decision Boundary\")\n",
    "\n",
    "plt.xlim(-4,4)\n",
    "plt.ylim(-4,4)\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.title(\"Decision Boundary with Correct and Incorrect Test Classifications\")\n",
    "plt.show()\n",
    "\n",
    "print(len(correct_x) + len(correct_y))\n",
    "print(len(incorrect_x) + len(incorrect_y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.A.2 Feature Selection and Generating Additional Features\n",
    "\n",
    "In the previous exercise, you worked with a dataset consisting of four blobs and attempted to classify them using a simple neural network model. You likely observed that this single-layer neural network struggled to classify the data effectively. A simple neural network with only one layer and a linear decision boundary can only separate classes that are linearly separable. This means it can only classify data effectively if the classes can be divided by straight lines (or planes in higher dimensions). However, the four blobs are arranged in a way that requires non-linear separation to classify correctly. The single-layer model simply doesnâ€™t have the capacity to capture these complex, non-linear relationships.\n",
    "\n",
    "### Approach: Derive Additional Features for Better Classification\n",
    "\n",
    "To improve our modelâ€™s performance, we can try creating new features that capture non-linear relationships between the existing features. By adding these features to our data, we can transform the dataset in ways that make it easier for a simple model to separate the classes.\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "* Create a New Feature for Non-linear Classification -- add a new dimension to the dataset based on the relationship between the existing features $X1$ and $X2$. This new feature will be computed as a specific combination of the features. Evaluate the Impact of the new Feature -- retrain and observe the model.\n",
    "* As a second task B: You are allowed to create multiple new features. But you are restricted solely on simple comparisons and logical operations involving the original features. These comparisons will help you capture relationships between $X1$ and $X2$ that might aid in classification without directly using non-linear transformations like multiplication. Experiment by adding several features that capture different combinations of conditions. Integrate these new features into your dataset as additional columns. You can use:\n",
    "    * Comparisons: Greater than (>) and less than (<)\n",
    "    * Logical Operators: AND (&) and OR (|)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom centers for the dataset\n",
    "centers = [(-2, 2), (-2, -2), (2, 2), (2, -2)]  # Four centers\n",
    "cluster_std = 1.0  # Standard deviation for each cluster\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X, y = make_blobs(n_samples=1000, centers=centers, cluster_std=cluster_std, random_state=42)\n",
    "\n",
    "# Map clusters to two classes: 0 and 1\n",
    "# Assign class 0 to clusters 0 and 1, and class 1 to clusters 2 and 3\n",
    "y = (y == 1) | (y == 2)\n",
    "\n",
    "# Add a third dimension with zero values\n",
    "X = np.hstack((X, np.zeros((X.shape[0], 1))))\n",
    "# TODO: Compute this column for your datapoints using X[:, 0] and X[:, 1]\n",
    "X[:, 2] = X[:, 0] * X[:, 1]\n",
    "# TODO TASK B: Comment the above lines and add n (as many as you like) features as columnes\n",
    "# X = np.hstack((X, np.zeros((X.shape[0], 4))))\n",
    "#X[:, 2] = (X[:, 0] > 0).astype(int) & (X[:, 1] > 0).astype(int)\n",
    "#X[:, 3] = (X[:, 0] > 0).astype(int) & (X[:, 1] < 0).astype(int)\n",
    "#X[:, 4] = (X[:, 0] < 0).astype(int) & (X[:, 1] > 0).astype(int)\n",
    "#X[:, 5] = (X[:, 0] < 0).astype(int) & (X[:, 1] < 0).astype(int)\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize and train the NeuralNetworkClass model\n",
    "input_dim = X_train.shape[1]\n",
    "model = NeuralNetworkClass(input_dim=input_dim, learning_rate=0.02, n_iterations=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Plot the learning curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(model.get_mse_history(), label=\"Standard Fit\", color=\"blue\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.title(\"MSE During Training (Standard Fit)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_labels = (y_pred > 0.5).astype(int)  # Apply threshold to convert probabilities to binary labels\n",
    "\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_train_labels = (y_pred_train > 0.5).astype(int)  # Apply threshold to convert probabilities to binary labels\n",
    "\n",
    "# Calculate accuracy for information\n",
    "accuracy = accuracy_score(y_test, y_pred_labels)\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train_labels)\n",
    "print(f\"Accuracy on test set : {accuracy * 100:.2f}%\")\n",
    "print(f\"Accuracy on train set: {accuracy_train * 100:.2f}%\")\n",
    "\n",
    "# Separate correctly and incorrectly classified points, keeping class colors\n",
    "correct_x, correct_y, correct_colors = [], [], []\n",
    "incorrect_x, incorrect_y, incorrect_colors = [], [], []\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    if y_pred_labels[i] == y_test[i]:  # Correct classification\n",
    "        correct_x.append(X_test[i, 0])\n",
    "        correct_y.append(X_test[i, 1])\n",
    "        correct_colors.append('blue' if y_test[i] == 0 else 'orange')\n",
    "    else:  # Incorrect classification\n",
    "        incorrect_x.append(X_test[i, 0])\n",
    "        incorrect_y.append(X_test[i, 1])\n",
    "        incorrect_colors.append('blue' if y_test[i] == 0 else 'orange')\n",
    "\n",
    "# Plot decision boundary and test data points\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Plot correctly classified points (circles) and incorrectly classified points (crosses) with class color\n",
    "plt.scatter(correct_x, correct_y, c=correct_colors, marker='o', label='Correctly Classified')\n",
    "plt.scatter(incorrect_x, incorrect_y, c=incorrect_colors, marker='x', label='Incorrectly Classified')\n",
    "\n",
    "# Adding legend for color and boundary\n",
    "correct_class_label = plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='blue', markersize=10, label='Class 0 (Correct)')\n",
    "incorrect_class_label = plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='orange', markersize=10, label='Class 1 (Correct)')\n",
    "plt.legend(handles=[correct_class_label, incorrect_class_label])\n",
    "\n",
    "plt.xlim(-4,4)\n",
    "plt.ylim(-4,4)\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.title(\"Decision Boundary with Correct and Incorrect Test Classifications\")\n",
    "plt.show()\n",
    "\n",
    "print(len(correct_x) + len(correct_y))\n",
    "print(len(incorrect_x) + len(incorrect_y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the third dimension: We can produce an additional visualization showing the combined features. And how this allows to fit a plane in order to separate the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_labels = (y_pred > 0.5).astype(int)  # Apply threshold to convert probabilities to binary labels\n",
    "\n",
    "# Calculate accuracy for reference\n",
    "accuracy = accuracy_score(y_test, y_pred_labels)\n",
    "print(f\"Accuracy on test set: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Separate the test data into correctly and incorrectly classified points\n",
    "correct_x, correct_y, correct_z, correct_colors = [], [], [], []\n",
    "incorrect_x, incorrect_y, incorrect_z, incorrect_colors = [], [], [], []\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    if y_pred_labels[i] == y_test[i]:  # Correct classification\n",
    "        correct_x.append(X_test[i, 0])\n",
    "        correct_y.append(X_test[i, 1])\n",
    "        correct_z.append(X_test[i, 2])  # Third dimension based on X1 * X2\n",
    "        correct_colors.append('blue' if y_test[i] == 0 else 'orange')\n",
    "    else:  # Incorrect classification\n",
    "        incorrect_x.append(X_test[i, 0])\n",
    "        incorrect_y.append(X_test[i, 1])\n",
    "        incorrect_z.append(X_test[i, 2])  # Third dimension based on X1 * X2\n",
    "        incorrect_colors.append('blue' if y_test[i] == 0 else 'orange')\n",
    "\n",
    "# Plotting in 3D\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot correctly classified points as circles and incorrectly classified as crosses\n",
    "ax.scatter(correct_x, correct_y, correct_z, c=correct_colors, marker='o', label='Correctly Classified')\n",
    "ax.scatter(incorrect_x, incorrect_y, incorrect_z, c=incorrect_colors, marker='x', label='Incorrectly Classified')\n",
    "\n",
    "# Define the decision boundary plane based on model weights\n",
    "w0, w1, w2, w3 = model.weights[0][0]  # Extract bias and weights\n",
    "xx, yy = np.meshgrid(np.linspace(-4, 4, 50), np.linspace(-4, 4, 50))\n",
    "zz = -(w0 + w1 * xx + w2 * yy) / w3  # Calculate corresponding z values\n",
    "\n",
    "# Plot the decision boundary plane\n",
    "ax.plot_surface(xx, yy, zz, color='green', alpha=0.4, edgecolor='k', linewidth=0.5)\n",
    "\n",
    "# Adding labels and legend\n",
    "ax.set_xlabel(\"X1\")\n",
    "ax.set_ylabel(\"X2\")\n",
    "ax.set_zlabel(\"X1 * X2\")\n",
    "ax.set_title(\"3D Decision Boundary with Correct and Incorrect Classifications\")\n",
    "ax.legend()\n",
    "\n",
    "# Set a custom view angle to change perspective\n",
    "ax.view_init(elev=5, azim=45)  # Adjust 'elev' and 'azim' for perspective\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.A.3 Multi-Layer Perceptron: Introducing Layers that \"Learn\" Features \n",
    "\n",
    "A Multi-Layer Perceptron (MLP) is a type of neural network that consists of an input layer, one (or more) hidden layers, and an output layer. Each layer is made up of nodes, or neurons, which are connected to the neurons in the subsequent layer. MLPs are considered \"fully connected\" neural networks, meaning every neuron in one layer connects to every neuron in the next layer. MLPs are commonly used for classification and regression tasks.\n",
    "\n",
    "![](Bishop_5_1_notation.svg)\n",
    "\n",
    "In each layer, the network processes the input data by applying a linear transformation (through weights and biases) followed by a non-linear activation function (like the sigmoid function). The model \"learns\" by adjusting the weights and biases through a process called backpropagation, which minimizes the error between the model's predictions and the true values. This is done by calculating the error gradients and using an optimization technique like gradient descent to update the weights.\n",
    "\n",
    "In this exercise, youâ€™ll implement parts of an MLP, focusing, first, on extending the forward pass to generate predictions. Secondly,  you will adapt the backward pass for learning through weight updates.\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "* Compute the Forward Pass: Given an input, extend the forward pass through the network. Use the provided weights and activation function to compute the output of each neuron. Pass the input data through each layer to calculate the final output of the network.\n",
    "* Implement the Backward Pass: Starting from the loss, which is provided as Mean Squared Error (MSE), calculate the gradients (deltas) for each layer.\n",
    "Given is already computation of the gradient of the output layer (as you have done before in the tasks). Work backwards to find the gradients of the hidden layers. Use these gradients to update the weights for each layer.\n",
    "\n",
    "**Notes:**\n",
    "\n",
    "* The gradient for the loss has already been computed, so youâ€™ll start from this point.\n",
    "* Use the chain rule to calculate the \"delta\" values that propagate backwards through each layer.\n",
    "* Ensure to update the weights at each layer based on the calculated deltas.\n",
    "\n",
    "This setup will allow you to test each layer individually to verify the accuracy of your implementation. Letâ€™s get started with the forward pass, and then move on to the backward pass for training (and test this layer wise).\n",
    "\n",
    "**Analyse:**\n",
    "\n",
    "* Test the different hyperparameters in your implementation: Learning rate, number of neurons in the hidden layer, initialization of weigths. What do you observe? And what do you learn from that? Do you have ideas how to improve the methods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define custom centers for the dataset\n",
    "centers = [(-2, 2), (-2, -2), (2, 2), (2, -2)]  # Four centers\n",
    "cluster_std = 1.0  # Standard deviation for each cluster\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X, y = make_blobs(n_samples=1000, centers=centers, cluster_std=cluster_std, random_state=42)\n",
    "\n",
    "# Map clusters to two classes: 0 and 1\n",
    "# Assign class 0 to clusters 0 and 1, and class 1 to clusters 2 and 3\n",
    "y = (y == 1) | (y == 2)\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class MLPClass:\n",
    "    \"\"\"\n",
    "    A Multi-Layer Perceptron (MLP) model with a single hidden layer and gradient descent optimization.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim=4, learning_rate=0.01, n_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.mse_history = []  # List to store MSE at each iteration\n",
    "        self.activation_function = SigmoidActivationFunction()  # Use the SigmoidActivationFunction class\n",
    "\n",
    "        # Initialize weights for input-to-hidden layer and hidden-to-output layer\n",
    "        self.weights = [\n",
    "            np.random.rand(hidden_dim, input_dim + 1),  # Weights for input-to-hidden (4 neurons, input_dim + 1 for bias)\n",
    "            np.random.rand(1, hidden_dim + 1)  # Weights for hidden-to-output (1 output neuron, hidden_dim + 1 for bias)\n",
    "        ]\n",
    "        #self.weights = [\n",
    "        #    np.random.randn(hidden_dim, input_dim + 1) * np.sqrt(1 / input_dim),  # Xavier init for hidden layer\n",
    "        #    np.random.randn(1, hidden_dim + 1) * np.sqrt(1 / hidden_dim)  # Xavier init for output layer\n",
    "        #]\n",
    "        # TODO TASK B: Comment the code below - this is an example which allows you to test\n",
    "        # the forward pass and still train the last weight matrix without considering the \n",
    "        # first weight matrix (this is a possible solution)\n",
    "        # Assign specific weights for the input-to-hidden layer (uncomment to use these values)\n",
    "        self.weights[0] = np.array([\n",
    "            [0.9068596,  0.62170851,  0.55163935],\n",
    "            [0.90959153, -0.09067262, 0.532634],\n",
    "            [0.9607063,  0.59245645,  0.17578004],\n",
    "            [0.40059766, 0.36945962,  0.16458776],\n",
    "            [0.61396506, 0.63269114,  0.3501316],\n",
    "            [0.60410684, 1.18499862, -0.32806404],\n",
    "            [0.36287036, -0.30953419, 0.81360432],\n",
    "            [0.10867067, 0.82539586, -0.2150381]\n",
    "        ])\n",
    "\n",
    "    def add_bias_column(self, X):\n",
    "        \"\"\"\n",
    "        Add a bias column (of ones) to the input data.\n",
    "        \"\"\"\n",
    "        X_with_bias = np.c_[np.ones(X.shape[0]), X]\n",
    "        return X_with_bias\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Perform the forward pass through the network with one hidden layer.\n",
    "        \"\"\"\n",
    "        # Forward pass through the hidden layer\n",
    "        X_with_bias = self.add_bias_column(X)  # Add bias to input\n",
    "        hidden_input = np.dot(X_with_bias, self.weights[0].T)  # Linear combination for hidden layer\n",
    "        hidden_output = self.activation_function(hidden_input)  # Apply activation to hidden layer output\n",
    "\n",
    "        # Forward pass through the output layer\n",
    "        hidden_output_with_bias = self.add_bias_column(hidden_output)  # Add bias to hidden layer output\n",
    "        output_input = np.dot(hidden_output_with_bias, self.weights[1].T)  # Linear combination for output layer\n",
    "        output = self.activation_function(output_input)  # Apply activation to output layer\n",
    "\n",
    "        return output, hidden_output_with_bias  # Return both output and hidden layer with bias for backward pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions using the trained model by performing a forward pass through both layers.\n",
    "        \n",
    "        Parameters:\n",
    "        X : np.array, shape (n_samples, input_dim)\n",
    "            The input feature values.\n",
    "        \n",
    "        Returns:\n",
    "        output : np.array, shape (n_samples, 1)\n",
    "            The predicted output values.\n",
    "        \"\"\"\n",
    "        output, _ = self.forward(X)  # Only return the final output, ignoring hidden layer\n",
    "        return (output > 0.5).astype(int)\n",
    "        #return output\n",
    "    \n",
    "    def binary_crossentropy(self, y_true, y_pred):\n",
    "        epsilon = 1e-12  # To avoid log(0)\n",
    "        y_pred = np.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the model using gradient descent, adjusting weights iteratively.\n",
    "        \n",
    "        Parameters:\n",
    "        X : np.array, shape (n_samples, input_dim)\n",
    "            The input feature values.\n",
    "        y : np.array, shape (n_samples,)\n",
    "            The true target values.\n",
    "        \"\"\"\n",
    "        y = y.reshape(-1, 1)  # Ensure y is a column vector (n_samples, 1)\n",
    "        X_with_bias = self.add_bias_column(X)  # Add bias column to the input data\n",
    "        \n",
    "        for _ in range(self.n_iterations):\n",
    "            # Forward pass\n",
    "            output, hidden_output_with_bias = self.forward(X)  # Get the output and hidden layer with bias\n",
    "\n",
    "            # Calculate Mean Squared Error and store it\n",
    "            mse = mean_squared_error(y, output)\n",
    "            self.mse_history.append(mse)\n",
    "\n",
    "            # ---- Step 1: Backpropagation for Output Layer ----\n",
    "            # Calculate error at the output layer\n",
    "            output_error = output - y  # Error at the output layer\n",
    "            # Correction -- delta is on derivative with respect to input to the output layer\n",
    "            output_input = np.dot(hidden_output_with_bias, self.weights[1].T)  # Linear combination at output layer\n",
    "            output_delta = output_error * self.activation_function.derivative(output_input)  # Delta for output layer\n",
    "\n",
    "            # Calculate gradient for the output layer weights\n",
    "            output_gradient = np.dot(output_delta.T, hidden_output_with_bias) / X.shape[0]\n",
    "            \n",
    "            # Update weights for the output layer\n",
    "            self.weights[1] -= self.learning_rate * output_gradient\n",
    "\n",
    "            # ---- Step 2: Backpropagation for Hidden Layer ----\n",
    "            # Calculate the delta for the hidden layer\n",
    "            # Correction -- delta is on derivative with respect to input to the output layer\n",
    "            hidden_input = np.dot(X_with_bias, self.weights[0].T)  # Linear combination at hidden layer\n",
    "            hidden_delta = np.dot(output_delta, self.weights[1][:, 1:]) * self.activation_function.derivative(hidden_input)\n",
    "\n",
    "            # Calculate gradient for the hidden layer weights\n",
    "            hidden_gradient = np.dot(hidden_delta.T, X_with_bias) / X.shape[0]\n",
    "\n",
    "            # Update weights for the hidden layer\n",
    "            self.weights[0] -= self.learning_rate * hidden_gradient\n",
    "\n",
    "    def get_mse_history(self):\n",
    "        \"\"\"\n",
    "        Get the history of MSE values during training.\n",
    "        \"\"\"\n",
    "        return self.mse_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize and train the MLPClass model\n",
    "input_dim = X_train.shape[1]\n",
    "model = MLPClass(input_dim=input_dim, hidden_dim=8, learning_rate=0.03, n_iterations=20000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Plot the learning curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(model.get_mse_history(), label=\"Training MSE\", color=\"blue\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.title(\"MSE During Training\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Make predictions on the test and train sets\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_labels = y_pred  # Apply threshold to convert probabilities to binary labels\n",
    "\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_train_labels = (y_pred_train > 0.5).astype(int)  # Apply threshold to convert probabilities to binary labels\n",
    "\n",
    "# Calculate accuracy for both test and train sets\n",
    "accuracy = accuracy_score(y_test, y_pred_labels)\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train_labels)\n",
    "print(f\"Accuracy on test set : {accuracy * 100:.2f}%\")\n",
    "print(f\"Accuracy on train set: {accuracy_train * 100:.2f}%\")\n",
    "\n",
    "# Separate correctly and incorrectly classified points, keeping class colors\n",
    "correct_x, correct_y, correct_colors = [], [], []\n",
    "incorrect_x, incorrect_y, incorrect_colors = [], [], []\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    if y_pred_labels[i] == y_test[i]:  # Correct classification\n",
    "        correct_x.append(X_test[i, 0])\n",
    "        correct_y.append(X_test[i, 1])\n",
    "        correct_colors.append('blue' if y_test[i] == 0 else 'orange')\n",
    "    else:  # Incorrect classification\n",
    "        incorrect_x.append(X_test[i, 0])\n",
    "        incorrect_y.append(X_test[i, 1])\n",
    "        incorrect_colors.append('blue' if y_test[i] == 0 else 'orange')\n",
    "\n",
    "# Plot decision boundary and test data points\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Plot correctly classified points (circles) and incorrectly classified points (crosses) with class color\n",
    "plt.scatter(correct_x, correct_y, c=correct_colors, marker='o', label='Correctly Classified')\n",
    "plt.scatter(incorrect_x, incorrect_y, c=incorrect_colors, marker='x', label='Incorrectly Classified')\n",
    "\n",
    "# Adding legend for color and boundary\n",
    "correct_class_label = plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='blue', markersize=10, label='Class 0 (Correct)')\n",
    "incorrect_class_label = plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='orange', markersize=10, label='Class 1 (Correct)')\n",
    "plt.legend(handles=[correct_class_label, incorrect_class_label])\n",
    "\n",
    "plt.xlim(-4, 4)\n",
    "plt.ylim(-4, 4)\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.title(\"Decision Boundary with Correct and Incorrect Test Classifications\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Number of correctly classified points:\", len(correct_x))\n",
    "print(\"Number of incorrectly classified points:\", len(incorrect_x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Computation for MLP\n",
    "\n",
    "### Step 1: Compute Delta for Output Layer (as before)\n",
    "\n",
    "For a single output neuron, the Mean Squared Error (MSE) loss $L$ is defined as:\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{2} (y_{\\text{pred}} - y_{\\text{true}})^2\n",
    "$$\n",
    "\n",
    "The gradient of the loss with respect to the output $ y_{\\text{pred}} $ is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial y_{\\text{pred}}} = y_{\\text{pred}} - y_{\\text{true}}\n",
    "$$\n",
    "\n",
    "The output $ y_{\\text{pred}} $ is obtained by applying the activation function (e.g., sigmoid) to the linear combination of the hidden layer outputs. Letâ€™s define:\n",
    "- $ z_{\\text{out}} $ as the linear combination before applying the activation function,\n",
    "- $ a_{\\text{out}} = \\sigma(z_{\\text{out}}) $ as the activated output, where $ \\sigma $ is the activation function.\n",
    "\n",
    "We are introducing a shorthand notation (see Bishop (2023)) for pushing the change of the error signal backwards through the different layers, (in Bishop, $i$ is a specific node and we focus on that activation: $\\delta_{i} \\coloneqq \\frac{\\partial L}{\\partial a_j}$; below we are deriving this more generally for the weight matrix). The delta for the output layer is:\n",
    "\n",
    "$$\n",
    "\\delta_{\\text{out}} = (y_{\\text{pred}} - y_{\\text{true}}) \\cdot \\sigma'(z_{\\text{out}})\n",
    "$$\n",
    "\n",
    "where $ \\sigma'(z_{\\text{out}}) $ is the derivative of the activation function at $ z_{\\text{out}} $.\n",
    "\n",
    "The gradient of the weights from the hidden layer to the output layer is then (note: in the figure, $\\delta_k$ is considered as the output layer, i.e. $\\delta_{\\text{out}}$):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_{\\text{out}}} = \\delta_{\\text{out}} \\cdot a_{\\text{hidden}}^T\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ a_{\\text{hidden}} $ is the vector of activations from the hidden layer (with bias included).\n",
    "\n",
    "![](Bishop_5_7_notation.svg)\n",
    "\n",
    "#### Step 2: Compute Delta for Hidden Layer\n",
    "\n",
    "To backpropagate the error to the hidden layer, we calculate the delta for each hidden neuron. For each hidden neuron $ i $, the delta $ \\delta_{\\text{hidden}, i} $ is given by:\n",
    "\n",
    "$$\n",
    "\\delta_{\\text{hidden}, i} = \\left( \\sum_k \\delta_{\\text{out}, k} \\cdot W_{\\text{out}, k, i} \\right) \\cdot \\sigma'(z_{\\text{hidden}, i})\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ W_{\\text{out}, k, i} $ are the weights connecting hidden neuron $ i $ to output neuron $ k $,\n",
    "- $ z_{\\text{hidden}, i} $ is the linear combination for hidden neuron $ i $ before applying the activation function (note: in the figure above, this is $z_j$),\n",
    "- $ \\sigma'(z_{\\text{hidden}, i}) $ is the derivative of the activation function at $ z_{\\text{hidden}, i} $.\n",
    "\n",
    "The gradient of the weights from the input layer to the hidden layer is then:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_{\\text{hidden}}} = \\delta_{\\text{hidden}} \\cdot a_{\\text{input}}^T\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ \\delta_{\\text{hidden}} $ is the vector of deltas for each hidden neuron (note: in the figure above, this is $\\delta_j$),\n",
    "- $ a_{\\text{input}} $ is the input vector (with bias included).\n",
    "\n",
    "### Summary\n",
    "\n",
    "1. **Output Layer Delta and Weight Gradient**:\n",
    "   $$\n",
    "   \\delta_{\\text{out}} = (y_{\\text{pred}} - y_{\\text{true}}) \\cdot \\sigma'(z_{\\text{out}})\n",
    "   $$\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial W_{\\text{out}}} = \\delta_{\\text{out}} \\cdot a_{\\text{hidden}}^T\n",
    "   $$\n",
    "\n",
    "2. **Hidden Layer Delta and Weight Gradient**:\n",
    "   $$\n",
    "   \\delta_{\\text{hidden}, i} = \\left( \\sum_k \\delta_{\\text{out}, k} \\cdot W_{\\text{out}, k, i} \\right) \\cdot \\sigma'(z_{\\text{hidden}, i})\n",
    "   $$\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial W_{\\text{hidden}}} = \\delta_{\\text{hidden}} \\cdot a_{\\text{input}}^T\n",
    "   $$\n",
    "\n",
    "These formulas provide the basis for backpropagation, where you start with the output layer delta and propagate the error backward to adjust the weights at each layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Test Data\n",
    "\n",
    "You can experiment with further test data sets, e.g., the half moon data set below.\n",
    "\n",
    "Furthermore: [Tensorflow offers a nice playground](https://playground.tensorflow.org/) to experiment (and nicely visualize) how MLPs with different numbers of layers are able to handle different data sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import pi\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N = 400\n",
    "theta = np.sqrt(np.random.rand(N))*2*pi # np.linspace(0,2*pi,100)\n",
    "\n",
    "r_a = 2*theta + pi\n",
    "data_a = np.array([np.cos(theta)*r_a, np.sin(theta)*r_a]).T\n",
    "x_a = data_a + np.random.randn(N,2)\n",
    "\n",
    "r_b = -2*theta - pi\n",
    "data_b = np.array([np.cos(theta)*r_b, np.sin(theta)*r_b]).T\n",
    "x_b = data_b + np.random.randn(N,2)\n",
    "\n",
    "res_a = np.append(x_a, np.zeros((N,1)), axis=1)\n",
    "res_b = np.append(x_b, np.ones((N,1)), axis=1)\n",
    "\n",
    "res = np.append(res_a, res_b, axis=0)\n",
    "np.random.shuffle(res)\n",
    "\n",
    "#np.savetxt(\"result.csv\", res, delimiter=\",\", header=\"x,y,label\", comments=\"\", fmt='%.5f')\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x_a[:,0],x_a[:,1])\n",
    "plt.scatter(x_b[:,0],x_b[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right;\">\n",
    "<i>This notebook has been created with the help of ChatGPT-4, 6.11.2024; Explanations were initially generated and afterwards edited;</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Tutorial: Apartment Price Prediction\n",
    "\n",
    "Supervised learning is a type of machine learning where we train a model on a labeled dataset, meaning that each example in the training set has an associated output. In regression tasks, the output is a continuous value. This tutorial will guide you through using supervised learning to predict the price of an apartment based on its size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Data\n",
    "\n",
    "**First step: Data Preparation**\n",
    "\n",
    "In this section, we will load and prepare our dataset. The data includes various features of apartments, but for the initial part of this tutorial, we will focus on using the `size` of the apartment to predict its `price`. We will prepare our dataset by converting the data into NumPy arrays, which are often used for performing efficient calculations in machine learning tasks.\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "* Load the data into a NumPy array.\n",
    "* Extract the size as the feature and price as the target in separate NumPy arrays.\n",
    "* Display the first few elements from both arrays to verify the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Hint: More information on using NumPy (Click to open!)</summary>\n",
    "    <p>For comprehensive details on NumPy and its functions, you can refer to the official <a href=\"https://numpy.org/doc/\">NumPy documentation</a>. It offers a thorough exploration of all features and functionalities of NumPy, suitable for both beginners and advanced users.</p>\n",
    "    <p>If you're looking for a more engaging and visually appealing introduction to NumPy, check out Jay Alammar's <a href=\"https://jalammar.github.io/visual-numpy/\">Visual Introduction to NumPy</a>. This guide provides an excellent visual representation of how NumPy operations work, making it easier to grasp complex concepts through diagrams and explanations.</p>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the dataset\n",
    "data = [\n",
    "    {\"size\": 19, \"price\": 440, \"rooms\": 1, \"distance_to_center\": 7, \"location\": \"Nienberge\"},\n",
    "    {\"size\": 32, \"price\": 500, \"rooms\": 1, \"distance_to_center\": 5.5, \"location\": \"Gremmendorf\"},\n",
    "    {\"size\": 69, \"price\": 810, \"rooms\": 3, \"distance_to_center\": 5.5, \"location\": \"Münster-Südost\"},\n",
    "    {\"size\": 60, \"price\": 900, \"rooms\": 2, \"distance_to_center\": 5.5, \"location\": \"Münster-West\"},\n",
    "    {\"size\": 20, \"price\": 500, \"rooms\": 1, \"distance_to_center\": 1.5, \"location\": \"Hansaviertel\"},\n",
    "    {\"size\": 18, \"price\": 660, \"rooms\": 1, \"distance_to_center\": 2.5, \"location\": \"Münster Mitte-Nordost\"},\n",
    "    {\"size\": 52, \"price\": 790, \"rooms\": 3, \"distance_to_center\": 1, \"location\": \"Münster City Center\"},\n",
    "    {\"size\": 45, \"price\": 1795, \"rooms\": 2, \"distance_to_center\": 1, \"location\": \"Boeselagerstraße\"},\n",
    "    {\"size\": 50, \"price\": 1895, \"rooms\": 2, \"distance_to_center\": 1, \"location\": \"Julius-Leber-Straße\"}\n",
    "]\n",
    "\n",
    "print(data[0]['size'])\n",
    "\n",
    "# TODO: Convert to NumPy arrays\n",
    "size = np.array([0])\n",
    "price = np.array([0])\n",
    "\n",
    "# Display the first few elements\n",
    "print(\"Sizes:\", size[:5])\n",
    "print(\"Prices:\", price[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second step: Visualizing the Data**\n",
    "\n",
    "It's essential to visually explore the data to understand the relationship between features and target, which can provide insights before building any models.\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "* Plot the relationship between the `size` of the apartments and their `price`s using a scatter plot. Try other plots as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Hint: Documentation for using matplotlib</summary>\n",
    "    <p>For additional details and assistance on creating plots, check out the excellent (in my opinion) <a href=\"https://matplotlib.org\">Matplotlib documentation</a>. The documentation is divided into various sections including tutorials, examples, and cheatsheets that you can use to enhance your plotting skills. For a specific example on creating basic plots, visit <a href=\"https://matplotlib.org/stable/plot_types/basic/plot.html#sphx-glr-plot-types-basic-plot-py\">this section</a> of the Matplotlib documentation.</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "# TODO - add the plot showing the data\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2) Introducing a (most) simple model\n",
    "\n",
    "In the second part of your notebook, you will work on a basic model class that mimics the simple structure of Scikit-Learn's [model interface](https://scikit-learn.org/stable/modules/linear_model.html#). The typical naming convention in Scikit-Learn for methods in a model class are `fit` for training the model and `predict` for making predictions. Let's create a basic model class that adheres to this interface:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel:\n",
    "    def __init__(self):\n",
    "        self.stored_value = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the model to the data.\n",
    "        Since this is a simple model, we'll just store the first value from y.\n",
    "\n",
    "        Parameters:\n",
    "        - X: array-like, shape (n_samples, n_features)\n",
    "          Training data, where n_samples is the number of samples and n_features is the number of features.\n",
    "          This model does not use `X` since it's overly simplistic.\n",
    "        - y: array-like, shape (n_samples,)\n",
    "          Target values.\n",
    "\n",
    "        Returns:\n",
    "        self: object\n",
    "          Returns an instance of itself.\n",
    "        \"\"\"\n",
    "        self.stored_value = y[0]\n",
    "        return self  # It's a good practice to return self in the fit method.\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions using the model.\n",
    "        This model returns the stored value for any input.\n",
    "\n",
    "        Parameters:\n",
    "        - X: array-like, shape (n_samples, n_features)\n",
    "          Input data for which predictions are made. This model does not use `X` to generate predictions.\n",
    "        \n",
    "        Returns:\n",
    "        predictions: list\n",
    "          A list containing the `stored_value` repeated for each input sample.\n",
    "        \"\"\"\n",
    "        return [self.stored_value for _ in X]\n",
    "\n",
    "# Example usage\n",
    "model = SimpleModel()\n",
    "model.fit(None, [10, 20, 30])  # Suppose 10 is the price based on the training data\n",
    "print(\"Predictions:\", model.predict([None, None, None]))  # Output will be [10, 10, 10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "1. *Initialization* (`__init__`): This method sets up the model. In our case, it initializes `stored_value` to `None`. \n",
    "2. *Fit* (`fit`): This method is supposed to train the model. Here, it merely stores the first element of `y`, simplifying the concept of learning. \n",
    "3. *Predict* (`predict`): This method is expected to provide predictions based on learned parameters. Our simple model just repeats the stored value for as many inputs as are in `X`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task: Fit a line through the first two data points**\n",
    "\n",
    "We will start by fitting a straight line using only the first two data points from our apartment dataset. This will provide a basic understanding of how linear models estimate relationships between variables.\n",
    "This means, we try to establish a linear relationship between the size of an apartment and its price using only the first two data points. This basic model will illustrate the concept of linear regression in its simplest form.\n",
    "\n",
    "**Task Breakdown**\n",
    "\n",
    "* Extract the tirst two Data Points: Use the size as the independent variable (`x`) and the price as the dependent variable (`y`).\n",
    "* Calculate the Slope and Intercept: Apply the formulas to compute the slope and intercept.\n",
    "* Plot the Results: To visualize how well the line fits these two points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Mathematical Formula for Line*\n",
    "The slope $(m)$ of the line connecting two points $(x_1, y_1)$ and $(x_2, y_2)$ can be calculated using:\n",
    "$[ m = \\frac{y_2 - y_1}{x_2 - x_1} ]$\n",
    "\n",
    "Once the slope is determined, the intercept $(b)$ can be computed using one of the point's coordinates (e.g., $(x_1, y_1)$):\n",
    "$[ b = y_1 - m \\cdot x_1 ]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLinearModel:\n",
    "    \"\"\"\n",
    "    A simple linear regression model that calculates the slope and intercept manually\n",
    "    from the first two data points and uses these to make predictions.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # TODO introduce the parameter required\n",
    "\n",
    "    def set_params(self, a):\n",
    "        # Set function for accessing the parameters\n",
    "        self.... = a \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the model using the first two data points from X and y.\n",
    "\n",
    "        Parameters:\n",
    "        X : list or array-like, shape (n_samples,)\n",
    "            The independent variable (e.g., size of apartments).\n",
    "        y : list or array-like, shape (n_samples,)\n",
    "            The dependent variable (e.g., price of apartments).\n",
    "\n",
    "        The method calculates the slope and intercept for a line that passes\n",
    "        through these two points.\n",
    "        \"\"\"\n",
    "        # TODO: Calculate the model parameters\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict method for making predictions using the trained model.\n",
    "\n",
    "        Parameters:\n",
    "        X : list or array-like, shape (n_samples,)\n",
    "            New data (independent variable) to predict the dependent variable.\n",
    "\n",
    "        Returns:\n",
    "        predictions : list\n",
    "            Predicted values based on the model's slope and intercept.\n",
    "        \"\"\"\n",
    "        # TODO: Use the line equation to generate predictions for the list\n",
    "        # Hint: You can use an expression of the type [x+0 for x in X] that applies the calculation to each element.\n",
    "        return [0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the simple linear model\n",
    "model = SimpleLinearModel()\n",
    "\n",
    "# Fit the model using only the first two data points for simplicity\n",
    "model.fit(size[:2], price[:2])\n",
    "\n",
    "# Let's predict prices for the same range of apartment sizes\n",
    "predicted_prices = model.predict(size)\n",
    "\n",
    "# Plotting the data and the model predictions\n",
    "plt.figure(figsize=(10, 6))\n",
    "# TODO: Add your code for plotting the scatter and the new fitted line\n",
    "plt.title('Simple Linear Regression with Manual Calculation')\n",
    "plt.xlabel('Size of Apartment (square meters)')\n",
    "plt.ylabel('Price (EUR)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3) Calculate the Error (Mean Squared Error)\n",
    "\n",
    "In this section, you will calculate how well a model performs by measuring the error between the actual values and the predicted values. One common way to compute this error is by using the Mean Squared Error (MSE).\n",
    "\n",
    "The **MSE** is calculated by taking the average of the squared differences between the actual values (ground truth) and the predicted values. A lower MSE indicates that the model's predictions are closer to the actual values.\n",
    "\n",
    "**Formula for MSE**: \n",
    "\n",
    "The **Mean Squared Error (MSE)** formula is:\n",
    "\n",
    "$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_{\\text{true}, i} - y_{\\text{pred}, i})^2\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $( y_{\\text{true}, i} )$ is the actual value.\n",
    "- $( y_{\\text{pred}, i} )$ is the predicted value.\n",
    "- $( n )$ is the number of data points.\n",
    "\n",
    "**Tasks**\n",
    "\n",
    "* Implement the Function: You'll program the MSE function to compute the error between the actual and predicted values.\n",
    "* Use the Function: After writing the function, you'll use it to calculate the error for the set of data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the Mean Squared Error between the actual and predicted values.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true: array-like, actual values (ground truth)\n",
    "    - y_pred: array-like, predicted values\n",
    "\n",
    "    Returns:\n",
    "    - mse: float, the mean squared error\n",
    "    \"\"\"\n",
    "    # Ensure inputs are NumPy arrays for element-wise operations\n",
    "    # TODO: Implement mean squared error\n",
    "    # Hint: Numpy allows to apply this to whole arrays. \n",
    "    mse = 0.\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict prices for all apartment sizes\n",
    "predicted_prices = model.predict(size)\n",
    "\n",
    "# Now calculate the MSE between the actual prices and the predicted prices\n",
    "mse_value = mean_squared_error(price, predicted_prices)\n",
    "\n",
    "print(f\"Calculated Mean Squared Error: {mse_value:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4) Interactive Visualization: Adjusting the Slope and Intercept with Error Bars\n",
    "\n",
    "In this section, your task is to build an interactive visualization that allows you to dynamically adjust the slope and intercept of a linear model using sliders. This will give you a hands-on way to understand how changes in these parameters affect the model's predictions and how well it fits the data.\n",
    "\n",
    "The visualization will allow you to manually adjust the slope and intercept of the linear regression model using sliders. It should dynamically update the plot to show how the line changes with these adjustments. Furthermore, you should show error bars between the data points and the fitted line, as well as the current MSE as a quantitative measure of how well your manually adjusted model fits the data.\n",
    "\n",
    "**Tasks**\n",
    "\n",
    "Your task is to create an interactive plot with the following features:\n",
    "\n",
    "* Sliders: Two sliders, one for the slope and one for the intercept, which can be adjusted interactively.\n",
    "* Plot: A scatter plot of the actual apartment prices vs. apartment sizes. And a line representing the model’s predictions, which will be updated based on the values of the slope and intercept you set.\n",
    "* Error Bars: Vertical error bars should be drawn between the actual prices and the predicted prices, showing the errors visually.\n",
    "* MSE Calculation: The Mean Squared Error (MSE) should be calculated and displayed on the plot to provide a numerical measure of how well the model fits the data.\n",
    "\n",
    "Below is a simple example for a widget -- and how to call this. In general, this consists of an `update function` that is initialized in the `interact(...)` call at the end -- and which is connected to a widget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "\n",
    "# Define the function that updates the plot\n",
    "def update_plot(y_position=0):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    \n",
    "    # Plot a horizontal line at y = y_position\n",
    "    plt.axhline(y=y_position, color='blue', linestyle='--', label=f'y = {y_position}')\n",
    "    \n",
    "    # Set plot limits and labels\n",
    "    plt.ylim(-10, 10)\n",
    "    plt.xlim(0, 10)\n",
    "    plt.title('Interactive Horizontal Line')\n",
    "    plt.show()\n",
    "\n",
    "    # Display the dummy variable in a text output\n",
    "    print(f\"Dummy variable: {y_position:.2f}\")\n",
    "\n",
    "# Create a slider widget for adjusting the y-position of the horizontal line\n",
    "slider = widgets.FloatSlider(min=-10, max=10, step=0.1, value=0)\n",
    "\n",
    "# Use the interact function to create the interactive widget\n",
    "interact(update_plot, y_position=slider)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "\n",
    "# Function to plot with adjustable slope and intercept\n",
    "def update_plot(slope=1, intercept=0):\n",
    "    # Use figure, plotting ...\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # TODO: Fill in your plotting\n",
    "\n",
    "    # Labels, title, and legend\n",
    "    plt.title('Interactive Linear Regression: Adjust Slope and Intercept with Error Bars')\n",
    "    plt.xlabel('Size of Apartment (square meters)')\n",
    "    plt.ylabel('Price (EUR)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Interactive widgets for slope and intercept\n",
    "# TODO: call the interact function and provide the update function as well as parameters for the update function (can come from widgets))\n",
    "interact(update_plot, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find good values that appear fitting -- and that minimize the error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Additional task for visualization**\n",
    "\n",
    "We can examine how the error (Mean Squared Error, MSE) changes when we vary the intercept $b$ and the slope $m$ individually, while keeping the other parameter fixed. This can be visualized.\n",
    "\n",
    "** Task**\n",
    "\n",
    "* We'll fix one parameter (either the slope or the intercept) and vary the other to observe how the error changes.\n",
    "* We'll create two subplots:\n",
    "    * One for varying $b$ (intercept) while keeping $m$ (slope) constant.\n",
    "    * One for varying $m$ (slope) while keeping $b$ (intercept) constant.\n",
    "* We'll compute the MSE for each variation and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of SimpleLinearModel\n",
    "model = SimpleLinearModel()\n",
    "\n",
    "# Initial values for slope (m) and intercept (b)\n",
    "fixed_m = 5  # Fixed slope for varying b\n",
    "fixed_b = 350  # Fixed intercept for varying m\n",
    "\n",
    "# Range for intercept (b) and slope (m)\n",
    "b_values = np.linspace(0, 1200, 100)  # Vary intercept b from 400 to 800\n",
    "# TODO: Create an additional linspace / list in the range of first to second value - and here 100 values.\n",
    "m_values = ...\n",
    "\n",
    "# TODO: Calculate MSE for varying b (with fixed m) - and fill a list with these values\n",
    "mse_for_b = []\n",
    "for b in b_values:\n",
    "    # TODO: set model to parameters, predict, and calculate MSE for these parameters\n",
    "    mse_for_b.append(mse)\n",
    "\n",
    "# TODO: Calculate MSE for varying m (with fixed b) - and fill a list\n",
    "...\n",
    "\n",
    "# Create two subplots\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot for varying b (intercept)\n",
    "ax[0].plot(b_values, mse_for_b, label='MSE for varying intercept (b)')\n",
    "ax[0].set_title('MSE vs Intercept (b)')\n",
    "ax[0].set_xlabel('Intercept (b)')\n",
    "ax[0].set_ylabel('Mean Squared Error (MSE)')\n",
    "ax[0].grid(True)\n",
    "\n",
    "# TODO: Plot for varying m (slope)\n",
    "...\n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "When manually adjusting the slope and intercept using sliders to find a good fit for a linear regression model, you're essentially trying to minimize the error (in this case, the Mean Squared Error (MSE)) between the predicted values and the actual values. Instead of adjusting parameters manually, we can automate this process using an iterative algorithm like gradient descent, which is commonly used for optimizing functions.\n",
    "\n",
    "# 5) Concept of Gradient Descent for Linear Regression\n",
    "\n",
    "Gradient descent is an optimization algorithm used to minimize a cost function (in this case, MSE). It works by iteratively adjusting the parameters (slope and intercept) in the direction of the steepest descent (i.e., the negative gradient of the cost function) until it finds a local minimum.\n",
    "\n",
    "How Gradient Descent Works:\n",
    "\n",
    "1. Initialize the parameters: Start with some initial values for the slope $(m)$ and intercept $(b)$.\n",
    "2. Compute the error (MSE): Calculate the MSE for the current parameters.\n",
    "3. Update the parameters: Change the parameters in such a way that the MSE is getting smaller. You can use the gradients (partial derivatives) of the cost function with respect to $m$ and $b$ to adjust the parameters in small steps. The size of the step is controlled by a learning rate $(\\alpha)$.\n",
    "4. Repeat: Continue updating the parameters iteratively until the error stops decreasing (i.e., converges).\n",
    "\n",
    "In the two following tasks, you will go through these four steps. Only step three will be different in the two versions.\n",
    "\n",
    "**First sub-task: Brute-force search for better parameters**\n",
    "\n",
    "Tasks:\n",
    "\n",
    "* Start with (small) initial values;\n",
    "* Compute error for current parameters;\n",
    "* For each of the parameter: slightly adjust each parameter individually and compute after each small adaptation the MSE again. If the error decreased, stick with the newly selected parameter.\n",
    "* repeat ...\n",
    "\n",
    "This approach is simpler than gradient descent but still allows you to iteratively find a better solution by trying out small adjustments to the parameters. You can fine-tune the step size and the number of iterations to balance between speed and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brute-force training function\n",
    "def brute_force_training(X, y, initial_m=0, initial_b=0, step_size=0.01, iterations=1000):\n",
    "    m = initial_m  # Start with an initial slope\n",
    "    b = initial_b  # Start with an initial intercept\n",
    "    n = len(X)  # Number of data points\n",
    "    \n",
    "    # TODO: Calculate predictions for data X (in numpy you can simply multiple a whole array: result = 2.*X )\n",
    "    # TODO: Compute current error current_mse for the initial parameters\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        # TODO: Try adjusting the slope (m) by a small amount\n",
    "        # TODO: Calculate predictions for new parameters for model\n",
    "        # TODO: Compute updated error\n",
    "        \n",
    "        # If the error decreased, accept the new slope\n",
    "        if new_mse_m < current_mse:\n",
    "            m = new_m\n",
    "            current_mse = new_mse_m\n",
    "        \n",
    "        # TODO: Try adjusting the intercept (b) by a small amount in the same way\n",
    "        \n",
    "        # If the error decreased, accept the new intercept\n",
    "        if new_mse_b < current_mse:\n",
    "            ...\n",
    "        \n",
    "        # Optionally, print the status every 100 iterations\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}: MSE = {current_mse:.4f}, m = {m:.4f}, b = {b:.4f}\")\n",
    "    \n",
    "    return m, b, current_mse\n",
    "\n",
    "# Run brute-force training with small initial values and small step size\n",
    "optimal_m, optimal_b, final_mse = brute_force_training(size, price, initial_m=0, initial_b=0, step_size=0.01, iterations=10000)\n",
    "\n",
    "print(f\"Optimal slope (m): {optimal_m:.4f}\")\n",
    "print(f\"Optimal intercept (b): {optimal_b:.4f}\")\n",
    "print(f\"Final MSE: {final_mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second sub-task: Gradient descent**\n",
    "\n",
    "Tasks:\n",
    "\n",
    "* Start with (small) initial values;\n",
    "* Compute error for current parameters;\n",
    "* Calculate gradients: Compute the partial derivatives (gradients) of the MSE with respect to $m$ (slope) and $b$ (intercept).\n",
    "* Update parameters: Adjust the values of $m$ and $b$ using the gradients. The amount by which you adjust the parameters is controlled by a learning rate (start with a small value, e.g., $\\alpha = 0.0001$ and run for a couple of thousand iterations).\n",
    "* Repeat: Continue adjusting the parameters iteratively until the error converges to a minimum. \n",
    "\n",
    "This approach is simpler than gradient descent but still allows you to iteratively find a better solution by trying out small adjustments to the parameters. You can fine-tune the step size and the number of iterations to balance between speed and accuracy.\n",
    "\n",
    "*Formulae for Gradient Descent* \n",
    "\n",
    "For linear regression, the MSE is defined as:\n",
    "\n",
    "$$\n",
    "\\text{MSE}(m, b) = \\frac{1}{n} \\sum_{i=1}^{n} (y_{\\text{true}, i} - y_{\\text{pred}, i})^2\n",
    "$$\n",
    "\n",
    "Where\n",
    "\n",
    "* $y_{\\text{pred}, i} = m \\cdot x_i + b$\n",
    "\n",
    "*The partial derivatives of the cost function (MSE) are:*\n",
    "\n",
    "* Gradient with respect to $m$ (slope): $\\frac{\\partial}{\\partial m} = -\\frac{2}{n} \\sum_{i=1}^{n} (y_{\\text{true}, i} - y_{\\text{pred}, i}) \\cdot x_i$\n",
    "\n",
    "* Gradient with respect to $b$ (intercept): $\\frac{\\partial}{\\partial b} = -\\frac{2}{n} \\sum_{i=1}^{n} (y_{\\text{true}, i} - y_{\\text{pred}, i})$\n",
    "\n",
    "*Gradient Descent Updates:*\n",
    "\n",
    "The update rules for the slope $m$ and intercept $b$ are:\n",
    "\n",
    "\n",
    "* $m = m - \\alpha \\cdot \\frac{\\partial}{\\partial m} \\text{MSE}(m, b)$\n",
    "\n",
    "* $b = b - \\alpha \\cdot \\frac{\\partial}{\\partial b} \\text{MSE}(m, b)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent for Linear Regression\n",
    "def gradient_descent(X, y, learning_rate=0.001, iterations=1000):\n",
    "    m = 0  # Initial slope\n",
    "    b = 0  # Initial intercept\n",
    "    n = len(X)  # Number of data points\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        # TODO: Make predictions using current m and b (leads to y_pred)\n",
    "        \n",
    "        # TODO: Calculate the gradients\n",
    "        dm = ...   # Gradient w.r.t. m\n",
    "        db = ... # Gradient w.r.t. b\n",
    "        \n",
    "        # TODO: Update the parameters\n",
    "        m = m - learning_rate * dm\n",
    "        b = ...\n",
    "        \n",
    "        # Print progress every 100 iterations\n",
    "        if i % 100 == 0:\n",
    "            mse = mean_squared_error(y, y_pred)\n",
    "            print(f\"Iteration {i}: MSE = {mse:.4f}, m = {m:.4f}, b = {b:.4f}\")\n",
    "    \n",
    "    return m, b\n",
    "\n",
    "# Perform gradient descent to find the optimal slope and intercept\n",
    "optimal_m, optimal_b = gradient_descent(size, price, learning_rate=0.0001, iterations=10000)\n",
    "\n",
    "print(f\"Optimal slope (m): {optimal_m:.4f}\")\n",
    "print(f\"Optimal intercept (b): {optimal_b:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Learning in a Simple Linear Model \n",
    "\n",
    "Refactor your code into a class called SimpleLinearModel: \n",
    "\n",
    "* the fit method should perform gradient descent,\n",
    "* and a predict method makes predictions based on the learned parameters (slope and intercept).\n",
    "* You should also add a function for the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLinearModel:\n",
    "    def __init__(self):\n",
    "        # Initialize slope and intercept\n",
    "        self.m = 0  # Slope\n",
    "        self.b = 0  # Intercept\n",
    "    \n",
    "    def mean_squared_error(self, y_true, y_pred):\n",
    "        \"\"\"Calculate Mean Squared Error.\"\"\"\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "    def fit(self, X, y, learning_rate=0.001, iterations=1000):\n",
    "        \"\"\"Fit the model using gradient descent.\"\"\"\n",
    "        n = len(X)  # Number of data points\n",
    "        self.m = 0  # Initialize slope\n",
    "        self.b = 0  # Initialize intercept\n",
    "        \n",
    "        mse_history = []  # To store MSE values for the learning curve (as a return)\n",
    "        \n",
    "        # Perform gradient descent\n",
    "        for i in range(iterations):\n",
    "            # TODO: Predict the current y values\n",
    "            y_pred = self.m * X + self.b\n",
    "            \n",
    "            # Calculate the gradients (as a hint provided here)\n",
    "            dm = (-2 / n) * np.sum((y - y_pred) * X)  # Gradient w.r.t. m\n",
    "            db = ... # TODO Gradient w.r.t. b\n",
    "            \n",
    "            # Update the parameters\n",
    "            self.m -= learning_rate * dm\n",
    "            self.b -= learning_rate * db\n",
    "            \n",
    "            # TODO: Compute and store the MSE for learning curve\n",
    "            mse = ...\n",
    "            \n",
    "            # Optionally, print progress every 100 iterations\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Iteration {i}: MSE = {mse:.4f}, m = {self.m:.4f}, b = {self.b:.4f}\")\n",
    "        \n",
    "        return mse_history\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict values using the learned slope and intercept.\"\"\"\n",
    "        return self.m * X + self.b\n",
    "    \n",
    "# Create an instance of SimpleLinearModel\n",
    "# model = SimpleLinearModel()\n",
    "\n",
    "# Test 1: Fit the model and track the MSE history\n",
    "# print(\"Running gradient descent...\")\n",
    "# mse_history = model.fit(size, price, learning_rate=0.0001, iterations=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subtask: Visualize the learning**\n",
    "\n",
    "A **learning curve** visualizes how the MSE changes during training, giving us an indication of the model's learning progress during gradient descent.\n",
    "\n",
    "Task: Plot a learning curve (you might have to change your gradient descent approach as you require to observe the error during training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the SimpleLinearModel class\n",
    "model = SimpleLinearModel()\n",
    "\n",
    "# Fit the model using gradient descent and collect the MSE history for the learning curve\n",
    "# TODO: get information on training error from the fit function\n",
    "model.fit(size, price, learning_rate=0.0001, iterations=10000)\n",
    "\n",
    "# TODO: Plot the learning curve (MSE over iterations)\n",
    "plt.figure(figsize=(10, 6))\n",
    "#plt.yscale('log') \n",
    "plt.ylim(2.2e5,2.7e5)\n",
    "plt.title('Learning Curve: MSE over Iterations')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 7) Finding the Optimal Values Analytically for Simple Linear Regression\n",
    "\n",
    "In this task, you are required to compute the optimal slope $(m)$ and intercept $(b)$ analytically for a simple linear regression problem. Instead of using iterative methods like gradient descent or brute-force optimization, you will directly calculate the values of \n",
    "$m$ and $b$ that minimize the Mean Squared Error (MSE) using closed-form solutions.\n",
    "\n",
    "**Overview:**\n",
    "\n",
    "For simple linear regression, the relationship between the independent variable $(x)$ (e.g., apartment size) and the dependent variable $(y)$ (e.g., price) is modeled as:\n",
    "$$\n",
    "y = m \\cdot x + b\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $m$ is the slope (or coefficient) that determines how much $y$ changes as $x$ changes.\n",
    "- $b$ is the intercept, which is the value of $y$ when $x = 0$.\n",
    "\n",
    "The goal is to find the values of $m$ and $b$ that minimize the *Mean Squared Error (MSE)*. This can be done analytically using the following closed-form equations.\n",
    "\n",
    "Try to find the analytical solution!\n",
    "\n",
    "**Analytical Solutions for $m$ and $b$:**\n",
    "\n",
    "The optimal slope \\(m\\) and intercept \\(b\\) for linear regression can be found using the *least squares method*:\n",
    "\n",
    "1. *Optimal Slope ($m$)*:\n",
    "$$\n",
    "   m = \\frac{n \\sum x_i y_i - \\sum x_i \\sum y_i}{n \\sum x_i^2 - \\left( \\sum x_i \\right)^2}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $n$ is the number of data points.\n",
    "* $\\sum x_i y_i$ is the sum of the products of $x_i$ and $y_i$.\n",
    "* $\\sum x_i$ is the sum of the $x$-values.\n",
    "* $\\sum y_i$ is the sum of the $y$-values.\n",
    "* $\\sum x_i^2$ is the sum of the squared $x$-values.\n",
    "\n",
    "2. *Optimal Intercept ($b$)*:\n",
    "$$\n",
    "   b = \\frac{\\sum y_i - m \\sum x_i}{n}\n",
    "$$\n",
    "\n",
    "Where $m$ is the slope computed from the formula above.\n",
    "\n",
    "**Task Instructions:**\n",
    "\n",
    "1. Compute the sums required for the formulae above.\n",
    "2. Use the formulae provided above to compute the optimal slope $m$ and intercept $b$.\n",
    "3. Once you have the analytical solutions for $m$ and $b$, use them to predict $y$-values for any given $x$-values (e.g., apartment sizes) and compare the predictions with the actual prices.\n",
    "4. Calculate the Mean Squared Error (MSE) for these predictions to verify that the analytically computed values for $m$ and $b$ minimize the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of data points\n",
    "n = len(size)\n",
    "\n",
    "# Step 1: TODO: Compute the necessary sums\n",
    "sum_x = \n",
    "...\n",
    "\n",
    "# Step 2: TODO: Calculate the optimal slope (m) and intercept (b)\n",
    "m = ...\n",
    "b = (sum_y - m * sum_x) / n\n",
    "\n",
    "# Display the results\n",
    "print(f\"Optimal slope (m): {m:.4f}\")\n",
    "print(f\"Optimal intercept (b): {b:.4f}\")\n",
    "\n",
    "# Step 3: TODO: Predict values using the analytically computed slope and intercept\n",
    "predicted_prices = ...\n",
    "\n",
    "# Step 4: Calculate the Mean Squared Error - here using numpy functions\n",
    "mse = np.mean((price - predicted_prices) ** 2)\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "# 8) Adding another input dimension\n",
    "\n",
    "As a last step, change your model and it should take two input dimensions (size and distance to city center).\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "* extract features for X and y from original data\n",
    "* change your different models (or one of these as a start): the gradient descent one and the analytical one\n",
    "* find an optimal solution\n",
    "* visualize the three dimensional data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step 1: Extract the features (size and distance_to_center) and target (price) from the dictionaries\n",
    "...\n",
    "prices = np.array([apt['price'] for apt in data])  # Target variable: price of the apartment\n",
    "\n",
    "# Step 2: Create the input matrix X with a column of 1s for the intercept\n",
    "X = np.column_stack((np.ones(sizes.shape[0]), sizes, distances))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel:\n",
    "    def __init__(self, learning_rate=0.001, iterations=1000):\n",
    "        # Initialize hyperparameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "        self.theta = None  # Coefficients (including intercept) - simplifies computation to matrix multiplication\n",
    "\n",
    "    def mean_squared_error(self, y_true, y_pred):\n",
    "        \"\"\"Calculate Mean Squared Error.\"\"\"\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the model using Stochastic Gradient Descent (SGD).\n",
    "        \n",
    "        Parameters:\n",
    "        - X: Input feature matrix (size, distance_to_center, etc.)\n",
    "        - y: Target variable (prices)\n",
    "        \"\"\"\n",
    "        # Sets the dimensions for features (kept variable)\n",
    "        n_samples, n_features = X.shape\n",
    "        # Add a column of ones for the intercept term\n",
    "        X_b = np.column_stack((np.ones(n_samples), X))\n",
    "        \n",
    "        # Initialize theta (coefficients + intercept) to zeros\n",
    "        self.theta = np.zeros(X_b.shape[1])\n",
    "        \n",
    "        # TODO: Apply your version of Stochastic Gradient Descent (SGD) now in the same way,\n",
    "        for _ in range(self.iterations):\n",
    "            # select a datapoint for updating\n",
    "            for i in range(n_samples):\n",
    "                # Pick a sample for stochastic update\n",
    "                xi = X_b[i, :]\n",
    "                yi = y[i]\n",
    "                \n",
    "                # Prediction for the current sample\n",
    "                y_pred = np.dot(xi, self.theta)\n",
    "                \n",
    "                # TODO: Compute the gradient for this sample\n",
    "                error = ...\n",
    "                gradient = ...\n",
    "                \n",
    "                # Update the parameters (theta)\n",
    "                self.theta -= self.learning_rate * gradient\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict target values using the learned coefficients (theta).\n",
    "        \n",
    "        Parameters:\n",
    "        - X: Input feature matrix (without intercept term)\n",
    "        \n",
    "        Returns:\n",
    "        - Predicted target values (y_pred)\n",
    "        \"\"\"\n",
    "        # Add a column of ones for the intercept term\n",
    "        X_b = np.column_stack((np.ones(X.shape[0]), X))\n",
    "        \n",
    "        # Predict using the learned coefficients\n",
    "        return np.dot(X_b, self.theta)\n",
    "\n",
    "model = LinearModel(learning_rate=0.00001, iterations=10000)\n",
    "model.fit(X, prices)\n",
    "# Make predictions using the model\n",
    "predicted_prices = model.predict(X)\n",
    "# Calculate the Mean Squared Error (MSE)\n",
    "mse = model.mean_squared_error(prices, predicted_prices)\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "# Print the learned parameters (theta)\n",
    "print(\"Learned parameters (theta):\")\n",
    "print(f\"Intercept (b): {model.theta[0]:.4f}\")\n",
    "print(f\"Slope for size (m1): {model.theta[1]:.4f}\")\n",
    "print(f\"Slope for distance to city (m2): {model.theta[2]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANALYTICAL SOLUTION\n",
    "# Step 3: Use the normal equation to compute the optimal parameters\n",
    "theta = np.linalg.inv(X.T @ X) @ X.T @ prices\n",
    "\n",
    "# Extract the parameters (intercept, slope for size, and slope for distance)\n",
    "b = theta[0]\n",
    "m1 = theta[1]\n",
    "m2 = theta[2]\n",
    "\n",
    "# Step 4: Display the results\n",
    "print(f\"Optimal intercept (b): {b:.4f}\")\n",
    "print(f\"Optimal slope for size (m1): {m1:.4f}\")\n",
    "print(f\"Optimal slope for distance to city (m2): {m2:.4f}\")\n",
    "\n",
    "# Step 5: Predict values using the computed parameters\n",
    "predicted_prices = X @ theta\n",
    "\n",
    "# Step 6: Calculate the Mean Squared Error (MSE)\n",
    "mse = np.mean((prices - predicted_prices) ** 2)\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right;\">\n",
    "<i>This notebook has been created with the help of ChatGPT-4, 8.10.2024; Explanations were initially generated and afterwards edited;</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

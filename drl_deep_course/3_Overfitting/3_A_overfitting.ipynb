{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3 A - Tutorial: Generating a new Data Set and a Polynomial Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.A.1\n",
    "\n",
    "Generating a new training data set.\n",
    "\n",
    "We want to explore the linear model (and more complex ones) and their behavior in training. Therefore, we now switch the perspective and start to work from synthetic data. As an advantage, as we have created this data and exactly know the properties of the data. \n",
    "\n",
    "You will create a dataset based on the sine function. The goal is to generate a specified number of data points for $ x $ values in the range $[0, 2\\pi]$, and compute the corresponding $ y $ values as the sine of $ x $ with some added random noise. This synthetic data will be used to test the performance of various models.\n",
    "\n",
    "\n",
    "\n",
    "Task: \n",
    "\n",
    "* Compute the corresponding $ y $ values - Calculate the sine of each $x$ value.\n",
    "* Add random noise to the $y$ values. The noise should be sampled from a normal distribution with mean 0 and the specified standard deviation (`noise` parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_sine_data(n_points, noise=0.2):\n",
    "    \"\"\"\n",
    "    Generates a synthetic dataset based on the sine function with added noise.\n",
    "\n",
    "    Parameters:\n",
    "    n_points (int): The number of data points to generate.\n",
    "    noise (float): The standard deviation of the noise to add to the sine values.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Two numpy arrays, x and y, where x is in the range [0, 2*pi] and\n",
    "           y is the sine of x with added noise.\n",
    "    \"\"\"\n",
    "    # Generate linearly spaced x values between 0 and 2*pi\n",
    "    x = np.linspace(0, 2 * np.pi, n_points)\n",
    "    \n",
    "    # TODO: Compute the sine on the x values and add some noise\n",
    "    # Create the y values in return.\n",
    "    # Compute the sine of each x value\n",
    "    y = \n",
    "    \n",
    "    return x, y\n",
    "\n",
    "# Example usage:\n",
    "x_data, y_data = generate_sine_data(8, 0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you should visualize your data and show in addition the sine function in the same plot.\n",
    "\n",
    "Task:\n",
    "\n",
    "* Generate the Data - create two datasets using the generate_sine_data function. The first dataset should represent the real sine function, with 100 data points and no noise. The second dataset should represent the noisy sine data, but only use $8$ data points and experiment with noise (in the end, continue to work with relatively high noise with a standard deviation of 1.).\n",
    "* Create a Figure with two Plots using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TODO: Generate real data (100 data points, no noise)\n",
    "x_real, y_real =\n",
    "# TODO: Generate noisy data (100 data points, with noise)\n",
    "x_noisy, y_noisy = \n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# TODO: Plot the real sine data and the scattered points.\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('x values')\n",
    "plt.ylabel('y values')\n",
    "plt.title('Real vs Noisy Sine Data')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.A.2 The Linear Model\n",
    "\n",
    "Below, you find the linear model slightly refactored. Basically, there are two important steps in the process:\n",
    "\n",
    "* prediction = taking inputs and generating predictions for these (using the learned parameters) -- this will be called the **forward pass**\n",
    "* fit = adapt the parameters to better fit the given output (we are in supervised learning) -- in this case, we are working backwards from the target values (the given y values) and, therefore, this is called the **backward pass**\n",
    "\n",
    "Briefly go over the code and understand, how this is implemented (you don't have to do anything else!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class GradientDescentModel:\n",
    "    \"\"\"\n",
    "    A simple linear regression model that performs gradient descent manually.\n",
    "    It uses an array of weights to represent the model parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.0001, n_iterations=1000):\n",
    "        self.weights = np.array([0.1, 0.1])  # Initialize weights for [intercept, slope]\n",
    "        self.learning_rate = learning_rate  # Set learning rate for gradient descent\n",
    "        self.n_iterations = n_iterations  # Set number of iterations for gradient descent\n",
    "        self.mse_history = []  # List to store MSE at each iteration\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the model using gradient descent. Adjust weights iteratively.\n",
    "\n",
    "        Parameters:\n",
    "        X : list or array-like, shape (n_samples,)\n",
    "            The independent variable (e.g., size of apartments).\n",
    "        y : list or array-like, shape (n_samples,)\n",
    "            The dependent variable (e.g., price of apartments).\n",
    "        \"\"\"\n",
    "        n = len(X)  # Number of data points\n",
    "        \n",
    "        for _ in range(self.n_iterations):\n",
    "            # Compute gradients for the entire dataset\n",
    "            gradient_intercept, gradient_slope = self.compute_gradient(X, y)\n",
    "\n",
    "            # Update weights using the computed gradients\n",
    "            self.weights[0] -= self.learning_rate * gradient_intercept  # Update intercept\n",
    "            self.weights[1] -= self.learning_rate * gradient_slope      # Update slope\n",
    "\n",
    "            # Calculate Mean Squared Error for the current iteration and store it\n",
    "            self.mse_history.append(mean_squared_error(y, self.predict(X)))\n",
    "\n",
    "    def compute_gradient(self, X, y):\n",
    "        \"\"\"\n",
    "        Compute the gradients for the intercept and slope based on the entire dataset.\n",
    "\n",
    "        Parameters:\n",
    "        X : list or array-like, shape (n_samples,)\n",
    "            The input feature values.\n",
    "        y : list or array-like, shape (n_samples,)\n",
    "            The target values.\n",
    "\n",
    "        Returns:\n",
    "        tuple: Gradients for the intercept and slope.\n",
    "        \"\"\"\n",
    "        n = len(X)  # Number of data points\n",
    "\n",
    "        # Predict using the current weights for all samples\n",
    "        y_pred = self.weights[1] * X + self.weights[0]\n",
    "\n",
    "        # Calculate the gradients for the intercept and slope\n",
    "        gradient_intercept = (-2 / n) * np.sum(y - y_pred)\n",
    "        gradient_slope = (-2 / n) * np.sum(X * (y - y_pred))\n",
    "\n",
    "        # Return the computed gradients\n",
    "        return gradient_intercept, gradient_slope\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict method for making predictions using the trained model (forward pass).\n",
    "\n",
    "        Parameters:\n",
    "        X : list or array-like, shape (n_samples,)\n",
    "            New data (independent variable) to predict the dependent variable.\n",
    "\n",
    "        Returns:\n",
    "        predictions : list\n",
    "            Predicted values based on the model's weights.\n",
    "        \"\"\"\n",
    "        # Forward pass: calculate the predicted value using the linear equation\n",
    "        # y_pred = weights[1] * x + weights[0]\n",
    "        return [self.weights[1] * x + self.weights[0] for x in X]\n",
    "    \n",
    "    def get_mse_history(self):\n",
    "        \"\"\"\n",
    "        Get the history of MSE values during the learning process.\n",
    "\n",
    "        Returns:\n",
    "        mse_history : list\n",
    "            A list of MSE values recorded at each iteration.\n",
    "        \"\"\"\n",
    "        return self.mse_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now can use this code to fit the model and predict corresponding values on our new data set. In addition, we visualize this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape x_noisy to a 1D array for the model (in case it's not already)\n",
    "#x_noisy = x_noisy.flatten()\n",
    "\n",
    "# Create and fit the GradientDescentModel to the noisy data\n",
    "model = GradientDescentModel(learning_rate=0.01, n_iterations=1000)\n",
    "model.fit(x_noisy, y_noisy)\n",
    "\n",
    "# Predict using the fitted model\n",
    "y_pred = model.predict(x_noisy)\n",
    "\n",
    "# Plot the noisy data and the model's predictions\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x_noisy, y_noisy, label='Noisy Data', color='C0', alpha=0.6, marker='x')\n",
    "plt.plot(x_noisy, y_pred, label='Fitted Model', color='C1')\n",
    "plt.plot(x_real, y_real, label='Original sine curve ', color='C3')\n",
    "plt.xlabel('x values')\n",
    "plt.ylabel('y values')\n",
    "plt.title('Fitting GradientDescentModel to Noisy Sine Data')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print final MSE\n",
    "final_mse = mean_squared_error(y_noisy, y_pred)\n",
    "print(f\"Final MSE for our implementation of the linear model: {final_mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next (you still don't have to implement anything ...), we will look how this is realized in the scikit-learn Framework ([documentation for the linear model can be found here](https://scikit-learn.org/stable/modules/linear_model.html)): \n",
    "\n",
    "While 'sklearn' uses the same interface as we have introduced in the previous weeks for our model implementation, it is quite general: Therefore, it always assumes input data to be represented as an 'numpy.array' of the shape '(samples, feature_dimension)'. In our case, the current input x was created simply as an array without an explicit number of features and we have to reshape this ('x_noisy.shape' was '(8,)' and will be reshaped to '(8,1)').\n",
    "\n",
    "Task:\n",
    "\n",
    "* Again, just go through the code and understand the interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Reshape x to be a 2D array for sklearn\n",
    "x_noisy_sk = x_noisy.reshape(-1, 1)\n",
    "\n",
    "# 1. Linear Regression Model\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(x_noisy_sk, y_noisy)\n",
    "y_linear_pred_sk = linear_model.predict(x_noisy_sk)\n",
    "\n",
    "# Plot the noisy data and the model's predictions\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x_noisy, y_noisy, label='Noisy Data', color='C0', alpha=0.6, marker='x')\n",
    "plt.plot(x_noisy, y_linear_pred_sk, label='Fitted Model sklearn', color='C1')\n",
    "plt.plot(x_real, y_real, label='Original sine curve ', color='C3')\n",
    "plt.xlabel('x values')\n",
    "plt.ylabel('y values')\n",
    "plt.title('Fitting GradientDescentModel to Noisy Sine Data')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Calculate Mean Squared Error for both models\n",
    "mse_linear = mean_squared_error(y_noisy, y_linear_pred_sk)\n",
    "\n",
    "# Print the MSE for comparison\n",
    "print(f\"Linear Model MSE (sklearn implementation): {mse_linear:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3.A.3\n",
    "\n",
    "Last, extend the linear model and make this a polynomial regression model (use degree 3 -- $w_3 * x^3 + w_2 * x^2 + w_1 * x + w_0$). The goal is to compare how well these models fit the data by calculating the Mean Squared Error (MSE) for each model. \n",
    "\n",
    "Your tasks:\n",
    "\n",
    "* Change the prediction step, the forward pass applying the polynomial.\n",
    "* Change the fit method -- this means you have to change the gradient (with respect to the specific weights)\n",
    "\n",
    "#### Reminder on Derivative\n",
    "\n",
    "For the derivative, we are looking at the Mean Squared Error between the target value for $y$ and the predicted value given $x$ and applying the polynomials. The gradient should be with respect to the individual weights. Given a polynomial model of degree 3:\n",
    "\n",
    "$$\n",
    "y_{\\text{pred}} = w_0 + w_1 x + w_2 x^2 + w_3 x^3\n",
    "$$\n",
    "where:\n",
    "- $w_0, w_1, w_2,$ and $w_3$ are the weights (parameters) of the model.\n",
    "- $x$ is the input feature.\n",
    "- $y_{\\text{pred}}$ is the predicted output.\n",
    "\n",
    "The **Mean Squared Error (MSE)** for a dataset with $n $ data points is given by:\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - y_{\\text{pred}, i})^2\n",
    "$$\n",
    "where:\n",
    "- $y_i$ is the true value for the $i$-th data point.\n",
    "- $y_{\\text{pred}, i}$ is the predicted value for the $i$-th data point.\n",
    "\n",
    "Substituting the expression for $y_{\\text{pred}}$, we get:\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left(y_i - (w_0 + w_1 x_i + w_2 x_i^2 + w_3 x_i^3)\\right)^2\n",
    "$$\n",
    "\n",
    "As an example: For $w_2$ we are interested the derivative of MSE with Respect to $w_2$: \n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{MSE}}{\\partial w_2}\n",
    "$$\n",
    "\n",
    "which we can compute using the chain rule.\n",
    "\n",
    "Solution: The gradient of the MSE with respect to $w_2$ is (the later part is our predicted value):\n",
    "$$\n",
    "\\frac{\\partial \\text{MSE}}{\\partial w_2} = -\\frac{2}{n} \\sum_{i=1}^{n} (y_i - (w_0 + w_1 x_i + w_2 x_i^2 + w_3 x_i^3)) x_i^2\n",
    "$$\n",
    "\n",
    "This derivative can be used in gradient descent to update the weight $w_2$ iteratively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class PolynomialGradientDescentModel:\n",
    "    \"\"\"\n",
    "    A polynomial regression model (degree 3) that performs gradient descent manually.\n",
    "    It uses an array of weights to represent the model parameters for a cubic polynomial.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.0001, n_iterations=1000):\n",
    "        self.weights = np.array([0.1, 0.1, 0.1, 0.1])  # Initialize weights for [intercept, x, x^2, x^3]\n",
    "        self.learning_rate = learning_rate  # Set learning rate for gradient descent\n",
    "        self.n_iterations = n_iterations  # Set number of iterations for gradient descent\n",
    "        self.mse_history = []  # List to store MSE at each iteration\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the model using gradient descent. Adjust weights iteratively.\n",
    "\n",
    "        Parameters:\n",
    "        X : list or array-like, shape (n_samples,)\n",
    "            The independent variable.\n",
    "        y : list or array-like, shape (n_samples,)\n",
    "            The dependent variable.\n",
    "        \"\"\"\n",
    "        n = len(X)  # Number of data points\n",
    "        \n",
    "        for _ in range(self.n_iterations):\n",
    "            # Compute gradients for the entire dataset\n",
    "            gradients = self.compute_gradient(X, y)\n",
    "\n",
    "            # Update weights using the computed gradients\n",
    "            for j in range(len(self.weights)):\n",
    "                self.weights[j] -= self.learning_rate * gradients[j]\n",
    "\n",
    "            # Calculate Mean Squared Error for the current iteration and store it\n",
    "            self.mse_history.append(mean_squared_error(y, self.predict(X)))\n",
    "\n",
    "    def compute_gradient(self, X, y):\n",
    "        \"\"\"\n",
    "        Compute the gradients for the weights based on the entire dataset.\n",
    "\n",
    "        Parameters:\n",
    "        X : list or array-like, shape (n_samples,)\n",
    "            The input feature values.\n",
    "        y : list or array-like, shape (n_samples,)\n",
    "            The target values.\n",
    "\n",
    "        Returns:\n",
    "        list: Gradients for each weight.\n",
    "        \"\"\"\n",
    "        n = len(X)  # Number of data points\n",
    "\n",
    "        # Predict using the current weights for all samples\n",
    "        y_pred = self.predict(X)\n",
    "\n",
    "        # TODO: Calculate the gradients for each weight\n",
    "        gradient_intercept = \n",
    "        gradient_w = \n",
    "        gradient_w2 = \n",
    "        gradient_w3 = \n",
    "\n",
    "        # Return the computed gradients\n",
    "        return [gradient_intercept, gradient_w, gradient_w2, gradient_w3]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict method for making predictions using the trained model (forward pass).\n",
    "\n",
    "        Parameters:\n",
    "        X : list or array-like, shape (n_samples,)\n",
    "            New data (independent variable) to predict the dependent variable.\n",
    "\n",
    "        Returns:\n",
    "        predictions : list\n",
    "            Predicted values based on the model's weights.\n",
    "        \"\"\"\n",
    "        # TODO: Forward pass: calculate the predicted value using the cubic equation\n",
    "        # y_pred = weights[0] + weights[1] * x + weights[2] * x^2 + weights[3] * x^3\n",
    "        # over the full input\n",
    "        return \n",
    "    \n",
    "    def get_mse_history(self):\n",
    "        \"\"\"\n",
    "        Get the history of MSE values during the learning process.\n",
    "\n",
    "        Returns:\n",
    "        mse_history : list\n",
    "            A list of MSE values recorded at each iteration.\n",
    "        \"\"\"\n",
    "        return self.mse_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again, we want to fit our model to the training data (this is getting much more sensitive and we have to use a smaller learning rate and longer training times!). \n",
    "\n",
    "How good is this model compared to the linear model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit the GradientDescentModel to the noisy data\n",
    "model = PolynomialGradientDescentModel(learning_rate=0.00005, n_iterations=100000) # You will need to run this for quite a longer time! Use 10^5\n",
    "model.fit(x_noisy, y_noisy)\n",
    "\n",
    "# Predict using the fitted model -- we will use x_real as it samples much more points for the fitted model\n",
    "y_poly_pred = model.predict(x_noisy)\n",
    "y_poly_real = model.predict(x_real)\n",
    "\n",
    "# Plot the noisy data and the model's predictions\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x_noisy, y_noisy, label='Noisy Data', color='C0', alpha=0.6, marker='x')\n",
    "plt.plot(x_real, y_poly_real, label='Fitted Model', color='C1')\n",
    "plt.plot(x_real, y_real, label='Original sine curve ', color='C3')\n",
    "plt.xlabel('x values')\n",
    "plt.ylabel('y values')\n",
    "plt.title('Fitting GradientDescentModel to Noisy Sine Data')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print final MSE\n",
    "final_mse = mean_squared_error(y_noisy, y_poly_pred)\n",
    "print(f\"Final MSE for our implementation of the linear model: {final_mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial in scikit-learn\n",
    "\n",
    "Last, you should as well look at the implementation of a polynomial (which looks different from our approach -- but consider why this is the same). \n",
    "In scikit-learn, learning a polynomial regression model involves two main steps:\n",
    "\n",
    "1. **Feature Transformation:** Use 'PolynomialFeatures' to transform the original input features into polynomial features up to a specified degree. For example, if the input is a single feature \n",
    "$x$ and we choose a degree of $3$, the transformation creates new features (overall $4$ as is our weight number): $1, x, x^2, x^3$. This effectively maps the original feature space into a higher-dimensional polynomial space.\n",
    "2. Linear Regression: Apply 'LinearRegression' to fit a linear model in the transformed polynomial feature space. Even though the original input was transformed into polynomial terms, the 'LinearRegression' model treats these terms as linear combinations of the weights. The model learns the coefficients for each polynomial term, enabling it to capture the polynomial relationship in the original input data.\n",
    "\n",
    "Together, these steps allow scikit-learn to learn a polynomial regression model by converting the problem into a linear regression task in a higher-dimensional feature space.\n",
    "\n",
    "Task:\n",
    "\n",
    "* Understand the implementation, for further explanation [see documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html).\n",
    "* Test different degrees of polynomials for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Reshape x to be a 2D array for sklearn\n",
    "x_noisy_sk = x_noisy.reshape(-1, 1)\n",
    "x_real_sk = x_real.reshape(-1, 1)\n",
    "\n",
    "# 1. Linear Regression Model\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(x_noisy_sk, y_noisy)\n",
    "y_linear_pred_sk = linear_model.predict(x_noisy_sk)\n",
    "\n",
    "# 2. Polynomial Regression Model (degree 3)\n",
    "# TODO: Test different degrees - what is your expectation?\n",
    "poly_features = PolynomialFeatures(degree=3)\n",
    "x_poly_sk = poly_features.fit_transform(x_noisy_sk)\n",
    "print(\"Shape of the poly features for data points: \", x_poly_sk.shape)\n",
    "x_poly_real_sk = poly_features.fit_transform(x_real_sk)\n",
    "print(\"Shape of the poly features for 100 points:  \", x_poly_real_sk.shape)\n",
    "\n",
    "\n",
    "polynomial_model = LinearRegression()\n",
    "polynomial_model.fit(x_poly_sk, y_noisy)\n",
    "y_poly_pred_sk = polynomial_model.predict(x_poly_sk)\n",
    "y_poly_real_sk = polynomial_model.predict(x_poly_real_sk)\n",
    "\n",
    "# Plot the noisy data and the model's predictions\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x_noisy, y_noisy, label='Noisy Data', color='C0', alpha=0.6, marker='x')\n",
    "plt.plot(x_noisy_sk, y_linear_pred_sk, label='Fitted Linear Model (sklearn) ', color='C1')            \n",
    "plt.plot(x_real_sk, y_poly_real_sk, label='Fitted Polynomial Model (sklearn) ', color='C2')\n",
    "plt.plot(x_real, y_real, label='Original sine curve ', color='C3')\n",
    "plt.xlabel('x values')\n",
    "plt.ylabel('y values')\n",
    "plt.title('Fitting GradientDescentModel to Noisy Sine Data')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Calculate Mean Squared Error for both models\n",
    "mse_linear = mean_squared_error(y_noisy, y_linear_pred_sk)\n",
    "mse_poly = mean_squared_error(y_noisy, y_poly_pred_sk)\n",
    "\n",
    "# Print the MSE for comparison\n",
    "print(f\"Linear Model MSE: {mse_linear:.4f}\")\n",
    "print(f\"Polynomial Model (Degree 3) MSE: {mse_poly:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right;\">\n",
    "<i>This notebook has been created with the help of ChatGPT-4, 21.10.2024; Explanations were initially generated and afterwards edited;</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2 Tutorial: Continued Apartment Price Prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the Data\n",
    "\n",
    "The first two cells below, you can simply run - they provide a starting point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the dataset\n",
    "data = [\n",
    "    {\"size\": 19, \"price\": 440, \"rooms\": 1, \"distance_to_center\": 7, \"location\": \"Nienberge\"},\n",
    "    {\"size\": 32, \"price\": 500, \"rooms\": 1, \"distance_to_center\": 5.5, \"location\": \"Gremmendorf\"},\n",
    "    {\"size\": 69, \"price\": 810, \"rooms\": 3, \"distance_to_center\": 5.5, \"location\": \"Münster-Südost\"},\n",
    "    {\"size\": 60, \"price\": 900, \"rooms\": 2, \"distance_to_center\": 5.5, \"location\": \"Münster-West\"},\n",
    "    {\"size\": 20, \"price\": 500, \"rooms\": 1, \"distance_to_center\": 1.5, \"location\": \"Hansaviertel\"},\n",
    "    {\"size\": 18, \"price\": 660, \"rooms\": 1, \"distance_to_center\": 2.5, \"location\": \"Münster Mitte-Nordost\"},\n",
    "    {\"size\": 52, \"price\": 790, \"rooms\": 3, \"distance_to_center\": 1, \"location\": \"Münster City Center\"},\n",
    "    {\"size\": 45, \"price\": 1795, \"rooms\": 2, \"distance_to_center\": 1, \"location\": \"Boeselagerstraße\"},\n",
    "    {\"size\": 50, \"price\": 1895, \"rooms\": 2, \"distance_to_center\": 1, \"location\": \"Julius-Leber-Straße\"}\n",
    "]\n",
    "\n",
    "# Step 1: Extract the features (size and distance_to_center) and target (price) from the dictionaries\n",
    "sizes = np.array([apt['size'] for apt in data])  # Feature 1: size of the apartment\n",
    "distances = np.array([apt['distance_to_center'] for apt in data])  # Feature 2: distance to city center\n",
    "prices = np.array([apt['price'] for apt in data])  # Target variable: price of the apartment\n",
    "rooms = np.array([apt['rooms'] for apt in data])  # Target variable: price of the apartment\n",
    "\n",
    "# Step 2: Create the input matrix X with a column of 1s for the intercept\n",
    "X = np.column_stack((np.ones(sizes.shape[0]), sizes, distances))\n",
    "Y = np.column_stack((prices, rooms))\n",
    "\n",
    "print(X[:,0].shape, Y.shape)\n",
    "print(\"Input: \", X[:,1])\n",
    "print(\"Output: \", Y[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the Mean Squared Error between the actual and predicted values.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true: array-like, actual values (ground truth)\n",
    "    - y_pred: array-like, predicted values\n",
    "\n",
    "    Returns:\n",
    "    - mse: float, the mean squared error\n",
    "    \"\"\"\n",
    "    # Ensure inputs are NumPy arrays for element-wise operations\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    # Calculate the squared differences\n",
    "    squared_differences = (y_true - y_pred) ** 2\n",
    "    \n",
    "    # Return the mean of the squared differences\n",
    "    mse = np.mean(squared_differences)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2.1 Gradient Descent \n",
    "\n",
    "When manually adjusting the slope and intercept (in the last tasks of the preceding week) using sliders to find a good fit for a linear regression model, you're essentially trying to minimize the error (in this case, the Mean Squared Error (MSE)) between the predicted values and the actual values. Instead of adjusting parameters manually, we can automate this process using an iterative algorithm like gradient descent, which is commonly used for optimizing functions.\n",
    "\n",
    "### Concept of Gradient Descent for Linear Regression\n",
    "\n",
    "Gradient descent is an optimization algorithm used to minimize a cost function (in this case, MSE). It works by iteratively adjusting the parameters (slope and intercept) in the direction of the steepest descent (i.e., the negative gradient of the cost function) until it finds a local minimum.\n",
    "\n",
    "How Gradient Descent Works:\n",
    "\n",
    "1. Initialize the parameters: Start with some initial values for the slope $(m)$ and intercept $(b)$.\n",
    "2. Compute the error (MSE): Calculate the MSE for the current parameters.\n",
    "3. Update the parameters: Change the parameters in such a way that the MSE is getting smaller. You can use the gradients (partial derivatives) of the cost function with respect to $m$ and $b$ to adjust the parameters in small steps. The size of the step is controlled by a learning rate $(\\alpha)$.\n",
    "4. Repeat: Continue updating the parameters iteratively until the error stops decreasing (i.e., converges).\n",
    "\n",
    "In the two following tasks, you will go through these four steps. Only step three will be different in the two versions.\n",
    "\n",
    "**First sub-task: Brute-force search for better parameters**\n",
    "\n",
    "Tasks:\n",
    "\n",
    "* Start with (small) initial values;\n",
    "* Compute error for current parameters;\n",
    "* For each of the parameter: slightly adjust each parameter individually and compute after each small adaptation the MSE again. If the error decreased, stick with the newly selected parameter.\n",
    "* repeat ...\n",
    "\n",
    "This approach is simpler than gradient descent but still allows you to iteratively find a better solution by trying out small adjustments to the parameters. You can fine-tune the step size and the number of iterations to balance between speed and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a starting point, this is the old SimpleLinearModel\n",
    "# Your Task is to replace the simple calculation of a slope \n",
    "# and implement gradient descent on all the training data\n",
    "class SimpleGradientDescentModel:\n",
    "    \"\"\"\n",
    "    A simple linear regression model that performs gradient descent manually.\n",
    "    It loops over each example, adjusts the slope (m) and intercept (b), \n",
    "    and calculates the gradient manually to update the parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
    "        self.slope = 0.\n",
    "        self.intercept = 0.\n",
    "        # NEW: We introduce a learning rate - this is how much \n",
    "        # (ideally as a scalign factor) we change in each iteration the weights of our model \n",
    "        # into the direction that the error is getting smaller. \n",
    "        # Should be a small value.\n",
    "        self.learning_rate = learning_rate  # Set learning rate for gradient descent\n",
    "        # NEW: Number of iterations to run gradient descent\n",
    "        self.n_iterations = n_iterations  # Set number of iterations for gradient descent\n",
    "        self.mse_history = []  # List to store MSE at each iteration\n",
    "\n",
    "    def set_params(self, m, b):\n",
    "        \"\"\"Set slope (m) and intercept (b) directly.\"\"\"\n",
    "        self.slope = m  # Set slope\n",
    "        self.intercept = b  # Set intercept\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the model using gradient descent. Adjust slope and intercept iteratively.\n",
    "\n",
    "        Parameters:\n",
    "        X : list or array-like, shape (n_samples,)\n",
    "            The independent variable (e.g., size of apartments).\n",
    "        y : list or array-like, shape (n_samples,)\n",
    "            The dependent variable (e.g., price of apartments).\n",
    "        \"\"\"\n",
    "        # OLD (REMOVE) simple model that calculated the slope (m)\n",
    "        self.slope = (y[1] - y[0]) / (X[1] - X[0])\n",
    "        # OLD (REMOVE) and the intercept (b)\n",
    "        self.intercept = y[0] - self.slope * X[0]\n",
    "\n",
    "        n = len(X)  # Number of data points\n",
    "\n",
    "        # TODO: Implement gradient descent\n",
    "        # We run this in a loop for n_iterations\n",
    "        for _ in range(self.n_iterations):\n",
    "            # Initially, you also can iterate over the trainings data in a loop as well\n",
    "            for i in range(n):\n",
    "                # TODO - Implement gradient descent: \n",
    "                # Calculating variations of the weights and how this affects the error\n",
    "                # Moving the weights into a good direction (learning_rate should influence how much) \n",
    "                print(\"Iterating ...\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict method for making predictions using the trained model.\n",
    "\n",
    "        Parameters:\n",
    "        X : list or array-like, shape (n_samples,)\n",
    "            New data (independent variable) to predict the dependent variable.\n",
    "\n",
    "        Returns:\n",
    "        predictions : list\n",
    "            Predicted values based on the model's slope and intercept.\n",
    "        \"\"\"\n",
    "        # Use the line equation y = mx + b to generate predictions\n",
    "        return [self.slope * x + self.intercept for x in X]\n",
    "    \n",
    "    def get_mse_history(self):\n",
    "        \"\"\"\n",
    "        Get the history of MSE values during the learning process.\n",
    "\n",
    "        Returns:\n",
    "        mse_history : list\n",
    "            A list of MSE values recorded at each iteration.\n",
    "        \"\"\"\n",
    "        return self.mse_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of learning\n",
    "\n",
    "A training curve shows how the error developed during training (it should go down). In this case, the MSE is used and decreases significantly during the early iterations as the model adjusts the slope and intercept to minimize the error. After a certain number of iterations, the MSE stabilizes and no longer decreases, indicating that the model has reached a relatively optimal solution.\n",
    "\n",
    "Characteristics of a training / learning curve:\n",
    "\n",
    "* Monitoring Convergence: The training curve helps to visualize whether the model is converging, i.e., if the error is decreasing as the number of iterations increases. A flat curve after some iterations suggests that the model has likely converged to a solution.\n",
    "* Hyperparameter Tuning: By observing the training curve, you can adjust hyperparameters like the learning rate or the number of iterations to ensure the model trains effectively without too many or too few updates.\n",
    "\n",
    "Task:\n",
    "\n",
    "* plot a learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model on our apartment data:\n",
    "model = SimpleGradientDescentModel(learning_rate=0.01, n_iterations=1000)\n",
    "model.fit(X[:,1], Y[:,0])\n",
    "\n",
    "# Retrieve the MSE history\n",
    "mse_history = model.get_mse_history()\n",
    "predictions = model.predict(X[:,1])\n",
    "\n",
    "# TODO: Plot a learning curve - how the error develops over training / time\n",
    "print(\"MSE History:\", mse_history)\n",
    "\n",
    "\n",
    "# TODO: and plot actual vs. predicted data again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second sub-task: Gradient descent**\n",
    "\n",
    "You should now use the real gradient -- we are interested in how the error (in our case the MSE) is changing depending on the current weights. Therefore, we can use the gradient of the error with respect to a weight and this gives us a direction in which the error would increase.\n",
    "\n",
    "Tasks:\n",
    "\n",
    "* Start with (small) initial values;\n",
    "* Compute error for current parameters;\n",
    "* Calculate gradients: Compute the partial derivatives (gradients) of the MSE with respect to $m$ (slope) and $b$ (intercept).\n",
    "* Update parameters: Adjust the values of $m$ and $b$ using the gradients. The amount by which you adjust the parameters is controlled by a learning rate (start with a small value, e.g., $\\alpha = 0.0001$ and run for a couple of thousand iterations).\n",
    "* Repeat: Continue adjusting the parameters iteratively until the error converges to a minimum. \n",
    "\n",
    "This approach is simpler than gradient descent but still allows you to iteratively find a better solution by trying out small adjustments to the parameters. You can fine-tune the step size and the number of iterations to balance between speed and accuracy.\n",
    "\n",
    "*Formulae for Gradient Descent* \n",
    "\n",
    "For linear regression, the MSE is defined as:\n",
    "\n",
    "$$\n",
    "\\text{MSE}(m, b) = \\frac{1}{n} \\sum_{i=1}^{n} (y_{\\text{true}, i} - y_{\\text{pred}, i})^2\n",
    "$$\n",
    "\n",
    "Where\n",
    "\n",
    "* $y_{\\text{pred}, i} = m \\cdot x_i + b$\n",
    "\n",
    "*The partial derivatives of the cost function (MSE) are:*\n",
    "\n",
    "* Gradient with respect to $m$ (slope): $\\frac{\\partial}{\\partial m} = -\\frac{2}{n} \\sum_{i=1}^{n} (y_{\\text{true}, i} - y_{\\text{pred}, i}) \\cdot x_i$\n",
    "\n",
    "* Gradient with respect to $b$ (intercept): $\\frac{\\partial}{\\partial b} = -\\frac{2}{n} \\sum_{i=1}^{n} (y_{\\text{true}, i} - y_{\\text{pred}, i})$\n",
    "\n",
    "*Gradient Descent Updates:*\n",
    "\n",
    "The update rules for the slope $m$ and intercept $b$ are:\n",
    "\n",
    "\n",
    "* $m = m - \\alpha \\cdot \\frac{\\partial}{\\partial m} \\text{MSE}(m, b)$\n",
    "\n",
    "* $b = b - \\alpha \\cdot \\frac{\\partial}{\\partial b} \\text{MSE}(m, b)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescentModel:\n",
    "    \"\"\"\n",
    "    A simple linear regression model that performs gradient descent manually.\n",
    "    It loops over each example, adjusts the slope (m) and intercept (b), \n",
    "    and calculates the gradient manually to update the parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.0001, n_iterations=1000):\n",
    "        self.slope = 0.1  # Initialize slope (m) to 0\n",
    "        self.intercept = 0.1  # Initialize intercept (b) to 0\n",
    "        self.learning_rate = learning_rate  # Set learning rate for gradient descent\n",
    "        self.n_iterations = n_iterations  # Set number of iterations for gradient descent\n",
    "        self.mse_history = []  # List to store MSE at each iteration\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the model using gradient descent. Adjust slope and intercept iteratively.\n",
    "\n",
    "        Parameters:\n",
    "        X : list or array-like, shape (n_samples,)\n",
    "            The independent variable (e.g., size of apartments).\n",
    "        y : list or array-like, shape (n_samples,)\n",
    "            The dependent variable (e.g., price of apartments).\n",
    "        \"\"\"\n",
    "        n = len(X)  # Number of data points\n",
    "        \n",
    "        for i in range(self.n_iterations):\n",
    "            # NEW: We will store gradient steps for a sample\n",
    "            # In your first test this will not do much:\n",
    "            # We still change our parameters after each sample from X,\n",
    "            # therefore the inner loop \n",
    "            gradient_slope = 0.\n",
    "            gradient_intercept = 0.\n",
    "\n",
    "            for i in range(n):\n",
    "                # TODO: Calculate gradients and change slope and intercept.\n",
    "                # Predict current sample using the current slope and intercept\n",
    "                # y_pred = \n",
    "                # TODO: Calculate the gradients for slope and intercept\n",
    "\n",
    "                # Update the slope and intercept using the gradients\n",
    "                self.slope -= self.learning_rate * gradient_slope \n",
    "                self.intercept -= self.learning_rate * gradient_intercept \n",
    "\n",
    "            # Calculate Mean Squared Error for the current iteration and store it\n",
    "            self.mse_history.append(mean_squared_error(y, self.predict(X)))\n",
    "\n",
    "            # Optionally, print the status every 100 iterations\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Iteration {i}: MSE = {mean_squared_error(y, self.predict(X)):.4f}, m = {self.slope:.4f}, b = {self.intercept:.4f}\")\n",
    "\n",
    "\n",
    "    def fit_vector(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the model using gradient descent. Adjust slope and intercept iteratively.\n",
    "\n",
    "        Parameters:\n",
    "        X : list or array-like, shape (n_samples,)\n",
    "            The independent variable (e.g., size of apartments).\n",
    "        y : list or array-like, shape (n_samples,)\n",
    "            The dependent variable (e.g., price of apartments).\n",
    "        \"\"\"\n",
    "        n = len(X)  # Number of data points\n",
    "        \n",
    "        for i in range(self.n_iterations):\n",
    "            # TODO B - LATER PART OF TASK AFTER THE FIRST VERSION IS RUNNING AND EVALUATED\n",
    "            # For stabler gradient updates we collect information on many samples\n",
    "            # - these are simply all integrated in gradient slope and \n",
    "            # afterwards we calculate the mean\n",
    "            # You can start with experimenting first in the loop  and collecting the values in these \n",
    "            # or simply calculate these directly ...\n",
    "            #gradient_slope = 0.\n",
    "            #gradient_intercept = 0.\n",
    "\n",
    "            # TODO B: Instead of a loop use a vector notation \n",
    "            # (which is much more efficient as well)\n",
    "\n",
    "            # Calculate the gradients for slope and intercept\n",
    "            # TODO B - basically adapt your code from above, without the loop index and sum all the gradients (np.sum() - \n",
    "            # and don't forget to divide by the number of summed gradients)\n",
    "\n",
    "\n",
    "            # Update the slope and intercept using the gradients\n",
    "            self.slope -= self.learning_rate * gradient_slope \n",
    "            self.intercept -= self.learning_rate * gradient_intercept \n",
    "\n",
    "            # Calculate Mean Squared Error for the current iteration and store it\n",
    "            self.mse_history.append(mean_squared_error(y, y_pred))\n",
    "\n",
    "            # Optionally, print the status every 100 iterations\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Iteration {i}: MSE = {mean_squared_error(y, self.predict(X)):.4f}, m = {self.slope:.4f}, b = {self.intercept:.4f}\")\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict method for making predictions using the trained model.\n",
    "\n",
    "        Parameters:\n",
    "        X : list or array-like, shape (n_samples,)\n",
    "            New data (independent variable) to predict the dependent variable.\n",
    "\n",
    "        Returns:\n",
    "        predictions : list\n",
    "            Predicted values based on the model's slope and intercept.\n",
    "        \"\"\"\n",
    "        return [self.slope * x + self.intercept for x in X]\n",
    "    \n",
    "    def get_mse_history(self):\n",
    "        \"\"\"\n",
    "        Get the history of MSE values during the learning process.\n",
    "\n",
    "        Returns:\n",
    "        mse_history : list\n",
    "            A list of MSE values recorded at each iteration.\n",
    "        \"\"\"\n",
    "        return self.mse_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your code for visualization from above. Important: for gradient descent the learning rate is usually a quite sensitive parameter. \n",
    "\n",
    "* Start initially with a very small value `(learning_rate=0.000005, n_iterations=100)`. What is the disadavantage of a small learning rate?\n",
    "* Try slightly increasing the learning rate -- when is this breaking down and what do you observe when getting close to that learning rate?\n",
    "\n",
    "#### Numpy Array Approach (TODO B)\n",
    "\n",
    "In the code above there is a second fit method -- `fit_vector`. You should implement this now: the task is to remove the for iteration over the training and make this more efficient. But in addition, we are not updating the weights after every single example and change them. Instead, we calculate in a way over the training data set for each example a suggestion in which direction and how much the gradients should be moved and only in the end find a consensus. \n",
    "\n",
    "Consider, how computing whole batches for a gradient update improves training:\n",
    "\n",
    "* again, test different learning rates\n",
    "* observe how training progresses and compare to using updates after each single sample.\n",
    "\n",
    "Further analysis:\n",
    "\n",
    "* Try for a good learning rate (fast learning and stable), try different initial values for the weights. What changes?\n",
    "* Can you find initial values that break again learning?\n",
    "* How do you explain this? Why is the system \"sometimes\" diverging?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model on our apartment data:\n",
    "model = GradientDescentModel(learning_rate=0.000005, n_iterations=500)\n",
    "model.fit(X[:,1], Y[:,0])\n",
    "#model.fit_vector(X[:,1], Y[:,0])\n",
    "\n",
    "# Retrieve the MSE history\n",
    "mse_history = model.get_mse_history()\n",
    "predictions = model.predict(X[:,1])\n",
    "\n",
    "# Assume mse_history contains the recorded MSE over iterations\n",
    "plt.plot(mse_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.title('Training Curve: MSE over Iterations')\n",
    "plt.show()\n",
    "\n",
    "# Assume you have predicted values and actual values\n",
    "plt.scatter(X[:,1], Y[:,0], color='blue', label='Actual Values')\n",
    "plt.plot(X[:,1], predictions, color='red', label='Predicted Values')\n",
    "plt.xlabel('X values')\n",
    "plt.ylabel('y values')\n",
    "plt.title('Predictions vs Actual Values')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2.2 Multi-dimensional model\n",
    "\n",
    "Adapt your class from above and use two input dimensions (X is a stacked input version -- check if this is still the case). A third dimension is the bias for which a constant of `1` was added to each entry (at index 0). \n",
    "\n",
    "Now build a class GradientDescentModel_vector that deals with numpy array operations (only the outer iterations loop) is still used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescentModel_vector:\n",
    "    \"\"\"\n",
    "    A linear regression model with gradient descent that handles multiple input dimensions.\n",
    "    The input X is expected to have a bias column (1s) as the first column.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.0001, n_iterations=1000):\n",
    "        self.learning_rate = learning_rate  # Set learning rate for gradient descent\n",
    "        self.n_iterations = n_iterations  # Set number of iterations for gradient descent\n",
    "        self.weights = None  # Weights vector (including bias weight)\n",
    "        self.mse_history = []  # List to store MSE at each iteration\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the model using gradient descent. Adjust weights (including bias) iteratively.\n",
    "\n",
    "        Parameters:\n",
    "        X : numpy array, shape (n_samples, n_features)\n",
    "            The independent variables with an additional bias column (first column is all 1s).\n",
    "        y : numpy array, shape (n_samples,)\n",
    "            The dependent variable (e.g., price of apartments).\n",
    "        \"\"\"\n",
    "        n, d = X.shape  # n is number of samples, d is number of features (including bias)\n",
    "\n",
    "        # TODO: Initialize weights (d values, one for each feature including the bias)\n",
    "        self.weights = \n",
    "\n",
    "        for i in range(self.n_iterations):\n",
    "            # TODO: Make predictions using the current weights\n",
    "                # Matrix-vector multiplication for predictions\n",
    "\n",
    "            # TODO: Calculate the gradient for the weights (including the bias)\n",
    "            gradient =  # Gradient vector (same length as weights)\n",
    "\n",
    "            # Update the weights using the gradient\n",
    "            self.weights -= self.learning_rate * gradient\n",
    "\n",
    "            # Calculate Mean Squared Error for the current iteration and store it\n",
    "            mse = mean_squared_error(y, y_pred)\n",
    "            self.mse_history.append(mse)\n",
    "\n",
    "            # Optionally, print the status every 100 iterations\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Iteration {i}: MSE = {mse:.4f}\")\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict method for making predictions using the trained model.\n",
    "\n",
    "        Parameters:\n",
    "        X : numpy array, shape (n_samples, n_features)\n",
    "            New data (independent variables with bias term as the first column).\n",
    "\n",
    "        Returns:\n",
    "        predictions : numpy array\n",
    "            Predicted values based on the model's weights.\n",
    "        \"\"\"\n",
    "        return X.dot(self.weights)  # Matrix-vector multiplication for predictions\n",
    "    \n",
    "    def get_mse_history(self):\n",
    "        \"\"\"\n",
    "        Get the history of MSE values during the learning process.\n",
    "\n",
    "        Returns:\n",
    "        mse_history : list\n",
    "            A list of MSE values recorded at each iteration.\n",
    "        \"\"\"\n",
    "        return self.mse_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, use your code for visualization from above. As we are now considering multiple input dimension, this doesn't lead to a single straight line anymore, but we are seeing projections onto individual planes.\n",
    "\n",
    "* For a line representation, order the sequence of inputs along the specific input dimension (is already done for first dimension below)\n",
    "* Extend it towards a second input dimension. \n",
    "\n",
    "Consider, how computing whole batches for a gradient update improves training:\n",
    "\n",
    "* Compare the predictions with the simpler model.\n",
    "* Learn for a longer time -- and test again different learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the model\n",
    "model = GradientDescentModel_vector(learning_rate=0.0005, n_iterations=2000)\n",
    "model.fit(X, Y[:, 0])  # Fit the model\n",
    "\n",
    "# Retrieve the MSE history and predictions\n",
    "mse_history = model.get_mse_history()\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# Sort the X values and corresponding predictions for smooth line plotting\n",
    "sorted_indices = np.argsort(X[:, 1])  # Get indices that would sort X[:, 1]\n",
    "sorted_X = X[sorted_indices, 1]  # Sort X[:, 1]\n",
    "sorted_predictions = predictions[sorted_indices]  # Sort predictions using the same indices\n",
    "# TODO: Adapt this for the second input dimension (distance to center)\n",
    "\n",
    "# Plot the training curve (MSE over iterations)\n",
    "plt.plot(mse_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.title('Training Curve: MSE over Iterations')\n",
    "plt.show()\n",
    "\n",
    "# Plot predictions vs actual values\n",
    "plt.scatter(X[:, 1], Y[:, 0], color='blue', label='Actual Values')  # X[:, 1] is the first feature (without bias)\n",
    "plt.plot(sorted_X, sorted_predictions, color='red', label='Predicted Values')\n",
    "plt.xlabel('X values (size)')\n",
    "plt.ylabel('y values')\n",
    "plt.title('Predictions vs Actual Values')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# TODO: Plot predictions vs actual values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D Visualization of fitted linear model\n",
    "\n",
    "Below is an example how to visualize a 3D plane in matplotlib. \n",
    "\n",
    "* Adapt it to visualize the plane spanned by the model,\n",
    "* plot the real data points and\n",
    "* include dotted lines indicating the error for the data.\n",
    "\n",
    "For more information on parameters for 3D plotting see the [documentation of matplotlib](https://matplotlib.org/stable/gallery/mplot3d/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Define the corner points for X and Y\n",
    "# This gives 4 points = 2 x 2 permutations of the two different arrays.\n",
    "# TODO: You have to adapt this for showing the model plane.\n",
    "# Consider: \n",
    "# - How many points do you need to span the linear plane?\n",
    "# - which should you use?\n",
    "x_plane = np.array([0, 100])\n",
    "y_plane = np.array([0, 10])\n",
    "\n",
    "# Create a mesh grid from the corner points\n",
    "X_plane, Y_plane = np.meshgrid(x_plane, y_plane)\n",
    "\n",
    "# Define the corresponding Z (height) values for each corner\n",
    "# TODO: And here you have to use projected values\n",
    "Z_plane = np.array([[300, 400], [400, 500]])\n",
    "\n",
    "# Create a figure for 3D plotting\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Create a surface plot, with color shading based on Z\n",
    "surf = ax.plot_surface(X_plane, Y_plane, Z_plane, alpha=0.6)\n",
    "ax.view_init(elev=20, azim=-110)\n",
    "\n",
    "\n",
    "# TODO: plot lines from prediction (on the surface) to real value \n",
    "# ax.plot([], color='green', linewidth=2)\n",
    "\n",
    "# Set labels\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z (Height)')\n",
    "\n",
    "# Set a title\n",
    "ax.set_title('3D Surface Plot of a Plane')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2.3 Finding the Optimal Values Analytically for Simple Linear Regression\n",
    "\n",
    "In this task, you are required to compute the optimal slope $(m)$ and intercept $(b)$ analytically for a simple linear regression problem (we are starting with the simple one dimensional input case again). Instead of using iterative methods like gradient descent or brute-force optimization, you will directly calculate the values of \n",
    "$m$ and $b$ that minimize the Mean Squared Error (MSE) using closed-form solutions.\n",
    "\n",
    "**Overview:**\n",
    "\n",
    "For simple linear regression, the relationship between the independent variable $(x)$ (e.g., apartment size) and the dependent variable $(y)$ (e.g., price) is modeled as:\n",
    "$$\n",
    "y = m \\cdot x + b\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $m$ is the slope (or coefficient) that determines how much $y$ changes as $x$ changes.\n",
    "- $b$ is the intercept, which is the value of $y$ when $x = 0$.\n",
    "\n",
    "The goal is to find the values of $m$ and $b$ that minimize the *Mean Squared Error (MSE)*. This can be done analytically using the following closed-form equations.\n",
    "\n",
    "Try to find the analytical solution -- i.e. for a given function you are searching for local optima (characterized by gradients that equal zero).\n",
    "\n",
    "**Analytical Solutions for $m$ and $b$:**\n",
    "\n",
    "The optimal slope \\(m\\) and intercept \\(b\\) for linear regression can be found using the *least squares method*:\n",
    "\n",
    "1. *Optimal Slope ($m$)*:\n",
    "$$\n",
    "   m = \\frac{n \\sum x_i y_i - \\sum x_i \\sum y_i}{n \\sum x_i^2 - \\left( \\sum x_i \\right)^2}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $n$ is the number of data points.\n",
    "* $\\sum x_i y_i$ is the sum of the products of $x_i$ and $y_i$.\n",
    "* $\\sum x_i$ is the sum of the $x$-values.\n",
    "* $\\sum y_i$ is the sum of the $y$-values.\n",
    "* $\\sum x_i^2$ is the sum of the squared $x$-values.\n",
    "\n",
    "2. *Optimal Intercept ($b$)*:\n",
    "$$\n",
    "   b = \\frac{\\sum y_i - m \\sum x_i}{n}\n",
    "$$\n",
    "\n",
    "Where $m$ is the slope computed from the formula above.\n",
    "\n",
    "**Task Instructions:**\n",
    "\n",
    "1. Compute the sums required for the formulae above.\n",
    "2. Use the formulae provided above to compute the optimal slope $m$ and intercept $b$.\n",
    "3. Once you have the analytical solutions for $m$ and $b$, use them to predict $y$-values for any given $x$-values (e.g., apartment sizes) and compare the predictions with the actual prices.\n",
    "4. Calculate the Mean Squared Error (MSE) for these predictions to verify that the analytically computed values for $m$ and $b$ minimize the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimalLinearModel:\n",
    "    \"\"\"\n",
    "    A simple linear regression model that directly calculates the optimal slope and intercept\n",
    "    using the closed-form solution (normal equation) for simple linear regression.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.slope = None  # Initialize slope (m)\n",
    "        self.intercept = None  # Initialize intercept (b)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the model by directly calculating the optimal slope and intercept.\n",
    "        Parameters:\n",
    "        X : list or array-like, shape (n_samples,)\n",
    "            The independent variable (e.g., size of apartments).\n",
    "        y : list or array-like, shape (n_samples,)\n",
    "            The dependent variable (e.g., price of apartments).\n",
    "        \"\"\"\n",
    "        n = len(X)  # Number of data points\n",
    "\n",
    "        # TODO: Calculate the slope (m) using the closed-form solution\n",
    "\n",
    "        # TODO: Calculate the intercept (b)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict method for making predictions using the trained model.\n",
    "\n",
    "        Parameters:\n",
    "        X : list or array-like, shape (n_samples,)\n",
    "            New data (independent variable) to predict the dependent variable.\n",
    "\n",
    "        Returns:\n",
    "        predictions : list\n",
    "            Predicted values based on the model's slope and intercept.\n",
    "        \"\"\"\n",
    "        return [self.slope * x + self.intercept for x in X]\n",
    "    \n",
    "    def mse(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        mse = mean_squared_error(y_pred, y)\n",
    "        return mse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we finally calculate the optimal values -- and visualize this optimal linear model (optimal in the sense that it minimizes MSE).\n",
    "\n",
    "Remarks on optimal solution:\n",
    "\n",
    "* No Iterative Updates: The weights are computed directly using the normal equation, which gives the optimal solution in one step.\n",
    "* Efficient for Small Data: This approach works well for small datasets.\n",
    "* But: computing the inverse of large matrices can be computationally expensive for large datasets.\n",
    "* Accurate: The normal equation provides the exact solution, so there is no need for tuning parameters like the learning rate or number of iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the optimal model\n",
    "model = OptimalLinearModel()\n",
    "model.fit(X[:,1], Y[:,0])\n",
    "\n",
    "# Retrieve the MSE history\n",
    "predictions = model.predict(X[:,1])\n",
    "print(\"MSE: \", model.mse(X[:,1], Y[:,0]))\n",
    "\n",
    "# Assume you have predicted values and actual values\n",
    "plt.scatter(X[:,1], Y[:,0], color='blue', label='Actual Values')\n",
    "plt.plot(X[:,1], predictions, color='red', label='Predicted Values')\n",
    "plt.xlabel('X values')\n",
    "plt.ylabel('y values')\n",
    "plt.title('Predictions vs Actual Values')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Solution for multidimensional case\n",
    "\n",
    "In this version of the linear regression model, we are using the **Normal Equation** to directly compute the optimal weights for a multivariate linear regression problem. The **Normal Equation** provides a closed-form solution for the parameters (weights), avoiding the need for iterative optimization methods like gradient descent. \n",
    "\n",
    "### Task List:\n",
    "- **Input Representation**: The input matrix $\\mathbf{X}$ is expected to have a bias column of ones as the first column, with subsequent columns representing the features.\n",
    "- **Weights Calculation**: The optimal weights $\\mathbf{w}$ are calculated using the following formula:\n",
    "  $$\n",
    "  \\mathbf{w} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}\n",
    "  $$\n",
    "- **Prediction**: Once the weights are calculated, the prediction for any new data $\\mathbf{X}_{\\text{new}}$ is done using matrix-vector multiplication:\n",
    "  $$\n",
    "  \\hat{\\mathbf{y}} = \\mathbf{X}_{\\text{new}} \\mathbf{w}\n",
    "  $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimalLinearModel_vector:\n",
    "    \"\"\"\n",
    "    A linear regression model that directly calculates the optimal weights \n",
    "    using the normal equation (closed-form solution) for multiple input dimensions.\n",
    "    The input X is expected to have a bias column (1s) as the first column.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.weights = None  # Weights vector (including bias weight)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the model by directly calculating the optimal weights using the normal equation.\n",
    "\n",
    "        Parameters:\n",
    "        X : numpy array, shape (n_samples, n_features)\n",
    "            The independent variables with an additional bias column (first column is all 1s).\n",
    "        y : numpy array, shape (n_samples,)\n",
    "            The dependent variable (e.g., price of apartments).\n",
    "        \"\"\"\n",
    "        # TODO: Compute the optimal weights using the normal equation\n",
    "        # np.linalg.inv()\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict method for making predictions using the trained model.\n",
    "\n",
    "        Parameters:\n",
    "        X : numpy array, shape (n_samples, n_features)\n",
    "            New data (independent variables with bias term as the first column).\n",
    "\n",
    "        Returns:\n",
    "        predictions : numpy array\n",
    "            Predicted values based on the model's weights.\n",
    "        \"\"\"\n",
    "        return X.dot(self.weights)  # Matrix-vector multiplication for predictions\n",
    "    \n",
    "    def mse(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        mse = mean_squared_error(y_pred, y)\n",
    "        return mse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we use code from above for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the model\n",
    "model = OptimalLinearModel_vector()\n",
    "model.fit(X, Y[:, 0])  # Fit the model\n",
    "\n",
    "predictions = model.predict(X)\n",
    "\n",
    "print(\"MSE multidimensional model: \", model.mse(X, Y[:, 0]))\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Define the corner points for X and Y\n",
    "# This gives 4 points = 2 x 2 permutations of the two different arrays.\n",
    "# Consider: \n",
    "# - How many points do you need to span the linear plane?\n",
    "# - which should you use?\n",
    "x_plane = np.array([20, 100])\n",
    "y_plane = np.array([1, 7])\n",
    "\n",
    "# Create a mesh grid from the corner points\n",
    "X_plane, Y_plane = np.meshgrid(x_plane, y_plane)\n",
    "\n",
    "# Define the corresponding Z (height) values for each corner\n",
    "corners = np.array([[1,x_plane[0],y_plane[0]],[1,x_plane[1],y_plane[0]],[1,x_plane[0],y_plane[1]],[1,x_plane[1],y_plane[1]]])\n",
    "predictions_plane = model.predict(corners)\n",
    "Z_plane = np.array([[predictions_plane[0], predictions_plane[1]], [predictions_plane[2], predictions_plane[3]]])\n",
    "\n",
    "# Create a figure for 3D plotting\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Create a surface plot, with color shading based on Z\n",
    "surf = ax.plot_surface(X_plane, Y_plane, Z_plane, alpha=0.6)\n",
    "ax.view_init(elev=30, azim=-70)\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "    ax.plot([X[i,1], X[i,1]], [X[i,2], X[i,2]], [Y[i,0], predictions[i]], \n",
    "        color='green', linewidth=2)\n",
    "\n",
    "\n",
    "# Set labels\n",
    "ax.set_xlabel('size')\n",
    "ax.set_ylabel('distance')\n",
    "ax.set_zlabel('Z (Height)')\n",
    "\n",
    "# Set a title\n",
    "ax.set_title('3D Surface Plot of a Plane')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Remarks / Check for your Understanding\n",
    "\n",
    "Briefly check your understanding of:\n",
    "\n",
    "* the learning curve \n",
    "* the learning rate\n",
    "    * How to find an optimal one? And what does optimal mean?\n",
    "    * What can happen with a learning rate that is too high or too low? \n",
    "    * To avoid long learning times -- what could you improve in the approach?\n",
    "* the choice of initial values\n",
    "\n",
    "Further questions:\n",
    "\n",
    "* Do you have an idea and can explain why gradient descent sometimes diverges and goes off rails?\n",
    "* What is a disadvantage of the analytical optimization approach?\n",
    "* How would you handle multiple output values (there is one further output given)?\n",
    "* Which is the best approach? And in what sense?\n",
    "* What are we actually measuring -- and what would we have to do to measure generalization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right;\">\n",
    "<i>This notebook has been created with the help of ChatGPT-4, 15.10.2024; Explanations were initially generated and afterwards edited;</i>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

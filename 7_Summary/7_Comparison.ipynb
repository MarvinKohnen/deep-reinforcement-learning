{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 7 - Summary Classification: Comparison of Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Load Data Set\n",
    "\n",
    "For the last session, we are working with a new data set - but with the same structure: The Fashion MNIST Dataset. Each image in the Fashion MNIST dataset is again 28x28 pixels, unrolled into a 784-dimensional vector for modeling (our input space). We transfer the labels to one-hot encoded targets.\n",
    "\n",
    "The original MNIST dataset (digits) serves as a benchmark for machine learning algorithms, as it presents a relatively simple, yet meaningful task: identifying handwritten digits. Despite its simplicity, the MNIST dataset is a great starting point for building and evaluating classification models.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "As a first step, you only have to go through the next steps and inspect the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images are 28x28 NumPy arrays, with pixel values ranging from 0 to 255. The *labels* are an array of integers, ranging from 0 to 9. These correspond to the *class* of clothing the image represents:\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Label</th>\n",
    "    <th>Class</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>0</td>\n",
    "    <td>T-shirt/top</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>Trouser</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>2</td>\n",
    "    <td>Pullover</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>3</td>\n",
    "    <td>Dress</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>4</td>\n",
    "    <td>Coat</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>5</td>\n",
    "    <td>Sandal</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>6</td>\n",
    "    <td>Shirt</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>7</td>\n",
    "    <td>Sneaker</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>8</td>\n",
    "    <td>Bag</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>9</td>\n",
    "    <td>Ankle boot</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "Each image is mapped to a single label. Since the *class names* are not included with the dataset, store them here to use later when plotting the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Load MNIST dataset\n",
    "#mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "#X, y = mnist[\"data\"], mnist[\"target\"].astype(int)\n",
    "\n",
    "# Fetch the Fashion-MNIST dataset\n",
    "fashion_mnist = fetch_openml('Fashion-MNIST', version=1, as_frame=False)\n",
    "# Separate the features (X) and labels (y)\n",
    "X, y = fashion_mnist['data'], fashion_mnist['target']\n",
    "# Convert labels to integers (if needed)\n",
    "y = y.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the labels\n",
    "encoder = OneHotEncoder(sparse_output=False, categories='auto')\n",
    "y_one_hot = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Split into training and test sets\n",
    "# TODO: Change train size\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, train_size=1000, test_size=10000, random_state=42, stratify=y)\n",
    "\n",
    "# Display shapes of the resulting datasets\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape, y_test.shape)\n",
    "\n",
    "#TODO: Add normalization\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "print(X_train.max())\n",
    "\n",
    "# Reshape the data to include a single channel for grayscale images (28x28x1)\n",
    "X_train_cnn = X_train.reshape(-1, 28, 28, 1)\n",
    "X_test_cnn = X_test.reshape(-1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(X_train_cnn[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(class_names[np.argmax(y_train[i])])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## 7.2 Applying the LeNet-5 Architecture\n",
    "\n",
    "We will continue to use the **LeNet-5 architecture**, a classic CNN architecture designed by Yann LeCun for handwritten digit recognition. LeNet-5 is well-suited for the MNIST dataset and includes several convolutional and pooling layers, followed by fully connected layers. \n",
    "\n",
    "![](lenet5.svg)\n",
    "\n",
    "* Input Layer: Accepts 28x28 grayscale images as input, with each image normalized to a range of 0â€“1 (we added padding).\n",
    "* Convolutional Layer 1: Applies 6 filters of size 5x5 with stride 1 and ReLU activation, producing 6 feature maps. The resulting feature maps have dimensions of 24x24.\n",
    "* Pooling Layer 1 (Average Pooling): Averages over 2x2 regions, reducing each feature map to a size of 12x12. Pooling reduces the spatial dimensions and adds robustness to small spatial translations.\n",
    "* Convolutional Layer 2: Applies 16 filters of size 5x5 with stride 1 and ReLU activation, producing 16 feature maps. The resulting feature maps have dimensions of 8x8.\n",
    "* Pooling Layer 2 (Average Pooling): Another 2x2 average pooling layer, reducing each feature map to a size of 4x4.\n",
    "* Fully Connected Layers:\n",
    "   - Fully Connected Layer 1: Flattens the output from the convolutional layers and connects to a fully connected layer with 120 neurons and ReLU activation.\n",
    "   - Fully Connected Layer 2: Connects to a fully connected layer with 84 neurons and ReLU activation.\n",
    "* Output Layer: The final layer is a fully connected layer with 10 neurons and softmax activation, used for classification into the 10 digit classes.\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "Your tasks are to try out some variations:\n",
    "\n",
    "* increasing training data (for further variations might have to go back to smaller data set again to speed up training)\n",
    "* using normalization on the input data (gray scale values are encoded from 0 to 255 = scale it into the range $[0,1]$)\n",
    "* activation function (last week we already used ReLU, but originally LeNet used sigmoid): compare ReLU and sigmoid (on smaller training data set); how does this interacts with normalization?\n",
    "* adding Dropout as regularization\n",
    "* using early stopping\n",
    "* and Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the LeNet-5 architecture\n",
    "model_lenet5 = tf.keras.models.Sequential([\n",
    "    # Convolutional Layer 1: 6 filters of size 5x5, stride 1, with ReLU activation\n",
    "    # TODO: activation can be replaced with 'relu' instead of 'sigmoid'\n",
    "    tf.keras.layers.Conv2D(6, kernel_size=(5, 5), strides=1, activation='sigmoid', input_shape=(28, 28, 1), padding='same'),\n",
    "    # TODO: Batch normalization - remove the single line for Conv2D and replace it by the three following steps\n",
    "    #tf.keras.layers.Conv2D(6, kernel_size=(5, 5), strides=1, input_shape=(28, 28, 1)),\n",
    "    #tf.keras.layers.BatchNormalization(),\n",
    "    #tf.keras.layers.Activation('relu'),\n",
    "    # Average Pooling Layer 1: pool size 2x2\n",
    "    tf.keras.layers.AveragePooling2D(pool_size=(2, 2), strides=2),\n",
    "    # Convolutional Layer 2: 16 filters of size 5x5, stride 1, with ReLU activation\n",
    "    tf.keras.layers.Conv2D(16, kernel_size=(5, 5), strides=1, activation='sigmoid', padding='same'),\n",
    "    # TODO: Batch normalization - remove the single line for Conv2D and replace it by the three following steps\n",
    "    #tf.keras.layers.Conv2D(6, kernel_size=(5, 5), strides=1),\n",
    "    #tf.keras.layers.BatchNormalization(),\n",
    "    #tf.keras.layers.Activation('relu'),\n",
    "    # Average Pooling Layer 2: pool size 2x2\n",
    "    tf.keras.layers.AveragePooling2D(pool_size=(2, 2), strides=2),\n",
    "    # Flatten layer to reshape 2D feature maps to a 1D vector\n",
    "    tf.keras.layers.Flatten(),\n",
    "    # Fully Connected Layer 1: 120 units with ReLU activation\n",
    "    tf.keras.layers.Dense(120, activation='sigmoid'),\n",
    "    # TODO: Batch normalization - remove the single line for Dense and replace it by the three following steps\n",
    "    #tf.keras.layers.Dense(120),\n",
    "    #tf.keras.layers.BatchNormalization(),\n",
    "    #tf.keras.layers.Activation('relu'),\n",
    "    # TODO: Dropout\n",
    "    #tf.keras.layers.Dropout(0.5),\n",
    "    # Fully Connected Layer 2: 84 units with ReLU activation\n",
    "    tf.keras.layers.Dense(84, activation='sigmoid'),\n",
    "    # TODO: Batch normalization - remove the single line for Dense and replace it by the three following steps\n",
    "    #tf.keras.layers.Dense(84),\n",
    "    #tf.keras.layers.BatchNormalization(),\n",
    "    #tf.keras.layers.Activation('relu'),\n",
    "    # TODO: Dropout\n",
    "    #tf.keras.layers.Dropout(0.5),\n",
    "    # Output Layer: 10 units with softmax activation for classification\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "    # Originally used Radial Basis Functions in LeNet-5\n",
    "])\n",
    "\n",
    "# Define EarlyStopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',    # Metric to monitor (e.g., validation loss)\n",
    "    patience=5,            # Number of epochs with no improvement to stop training\n",
    "    restore_best_weights=True  # Restore model weights from the epoch with the best metric value\n",
    ")\n",
    "\n",
    "# Create the Adam optimizer with a specific learning rate\n",
    "adam_optimizer = Adam()  # Set your desired learning rate (default is 0.001, learning_rate is a parameter you can set)\n",
    "\n",
    "# Compile the model\n",
    "model_lenet5.compile(optimizer=adam_optimizer,\n",
    "                     loss='categorical_crossentropy',\n",
    "                     metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history_lenet5 = model_lenet5.fit(X_train_cnn, y_train,\n",
    "                                  epochs=10,\n",
    "                                  batch_size=32,\n",
    "                                  validation_data=(X_test_cnn, y_test),\n",
    "# TODO: Test the early stopping callback\n",
    "#                                  callbacks=[early_stopping],\n",
    "                                  verbose=1)\n",
    "\n",
    "# Plot the learning curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history_lenet5.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history_lenet5.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training and Validation Accuracy for LeNet-5 Architecture\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model_lenet5.evaluate(X_test_cnn, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_lenet5.predict(X_test_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Convert predictions from probabilities to class indices\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test.argmax(axis=1), predicted_classes)\n",
    "\n",
    "# Display the confusion matrix with class names\n",
    "disp = ConfusionMatrixDisplay(conf_matrix, display_labels=class_names)\n",
    "disp.plot(cmap=plt.cm.Blues, xticks_rotation='vertical')\n",
    "plt.title(\"Confusion Matrix for LeNet Model\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remarks on different topics / Explanation\n",
    "\n",
    "### Normalization\n",
    "Normalization scales input features to a common range, typically [0, 1] or [-1, 1], ensuring that each feature contributes equally to the model's training. It is especially critical when using activation functions like sigmoid or tanh, which are sensitive to input magnitude and can saturate, leading to vanishing gradients. With ReLU, normalization becomes less critical because ReLU does not saturate for positive inputs. However, normalized inputs still help by improving convergence speed and ensuring consistent gradient magnitudes across layers.\n",
    "\n",
    "### Effect of More Data\n",
    "Adding more data can significantly improve model performance, especially by reducing overfitting and enhancing generalization. Models trained on larger datasets can better capture the underlying patterns and variability of the data, leading to improved predictions on unseen samples. However, simply adding more data without diversity or relevance may not yield benefits. High-quality, diverse data ensures that the model is exposed to a wide range of scenarios during training, boosting its ability to generalize.\n",
    "\n",
    "### Dropout and Regularization\n",
    "Dropout is a popular regularization technique that prevents overfitting by randomly setting a fraction of neurons to zero during training. This forces the network to learn more robust features, as it cannot rely on any single neuron. Dropout complements other regularization techniques, such as L2 regularization, which penalizes large weights to reduce model complexity. While dropout is effective, using it excessively can lead to underfitting, so it should be applied judiciously, especially in conjunction with Batch Normalization.\n",
    "\n",
    "### Early Stopping\n",
    "Early stopping halts training when the model's performance on a validation set stops improving for a specified number of epochs. This prevents overfitting, as the model stops learning patterns specific to the training data. By restoring the weights from the epoch with the best validation performance, early stopping ensures optimal generalization. It is particularly effective in scenarios where the model is prone to overfitting or when computational resources are limited.\n",
    "\n",
    "### Batch Normalization\n",
    "Batch Normalization (BatchNorm) normalizes the activations of each layer by adjusting and scaling them during training. This stabilizes learning, reduces the risk of vanishing or exploding gradients, and allows for higher learning rates. BatchNorm also acts as a form of regularization, reducing the need for other techniques like dropout. It is especially useful in deeper networks, where it accelerates convergence and makes the model more robust to weight initialization and learning rate choices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(i, predictions_array, true_array, img):\n",
    "  true_label, img = np.argmax(true_array[i]), img[i]\n",
    "  plt.grid(False)\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "\n",
    "  plt.imshow(img, cmap=plt.cm.binary)\n",
    "\n",
    "  predicted_label = np.argmax(predictions_array)\n",
    "  if predicted_label == true_label:\n",
    "    color = 'blue'\n",
    "  else:\n",
    "    color = 'red'\n",
    "\n",
    "  plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
    "                                100*np.max(predictions_array),\n",
    "                                class_names[true_label]),\n",
    "                                color=color)\n",
    "\n",
    "def plot_value_array(i, predictions_array, true_array):\n",
    "  true_label = np.argmax(true_array[i])\n",
    "  plt.grid(False)\n",
    "  plt.xticks(range(10))\n",
    "  plt.yticks([])\n",
    "  thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n",
    "  plt.ylim([0, 1])\n",
    "  predicted_label = np.argmax(predictions_array)\n",
    "\n",
    "  thisplot[predicted_label].set_color('red')\n",
    "  thisplot[true_label].set_color('blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first X test images, their predicted labels, and the true labels.\n",
    "# Color correct predictions in blue and incorrect predictions in red.\n",
    "num_rows = 5\n",
    "num_cols = 3\n",
    "num_images = num_rows*num_cols\n",
    "plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
    "for i in range(num_images):\n",
    "  plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
    "  plot_image(i, predictions[i], y_test, X_test_cnn)\n",
    "  plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
    "  plot_value_array(i, predictions[i], y_test)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7.3 Logistic Regression as a baseline\n",
    "\n",
    "The Fashion MNIST dataset is a more difficult task (overall, still an easy task -- could you explain why?). We want to get a first impression how difficult it actually is and simply apply logistic regression on this dataset as a baseline. And afterwards compute the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Flatten the images back to 1D for logistic regression\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Train a logistic regression model\n",
    "logistic_regression = LogisticRegression(max_iter=100, multi_class='multinomial', solver='saga', random_state=42)\n",
    "logistic_regression.fit(X_train_flat, y_train.argmax(axis=1))\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = logistic_regression.predict(X_test_flat)\n",
    "\n",
    "# Calculate and print the test accuracy\n",
    "test_accuracy = accuracy_score(y_test.argmax(axis=1), y_pred)\n",
    "print(\"Test accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test.argmax(axis=1), y_pred)\n",
    "\n",
    "# Display the confusion matrix with class names\n",
    "disp = ConfusionMatrixDisplay(conf_matrix, display_labels=class_names)\n",
    "disp.plot(cmap=plt.cm.Blues, xticks_rotation='vertical')\n",
    "plt.title(\"Confusion Matrix for Logistic Regression\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression works not bad on this dataset and seems to generalize also quite well. \n",
    "\n",
    "Why are convolutional neural networks not outperforming logistic regression? How is the structure of the data helpful for other classification methods, but would not be required for convolutional methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right;\">\n",
    "<i>This notebook has been created with the help of ChatGPT-4, 14.11.2024; Explanations were initially generated and afterwards edited;</i>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

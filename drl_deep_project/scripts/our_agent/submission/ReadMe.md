# Bomberman Challenge:

This code was written and submitted by Julius Ferber, Marvin Kohnen and Moritz Gehring

### Contents:

The files are of the following structure:

ğŸ“ our_agent/
â””â”€â”€ ğŸ“ tex
    â”œâ”€â”€ ğŸ“ images/
        â”œâ”€â”€ ğŸ“„ document.pdf     # short paper regarding our project, our attempts and results
        â”œâ”€â”€ ğŸ“„ the usual tex stuff...
â””â”€â”€ ğŸ“ simple_dqn/ 
    â””â”€â”€ ğŸ“ models/                               # Directory for saved model weights
        â”œâ”€â”€ ğŸ“„ dqn_20250114_195134.pt            # Single DQN model weights
    â””â”€â”€ ğŸ“ training_logs/                        # Directory for training stats and plots   
        â”œâ”€â”€ ğŸ“ model_20250114_195134             # Folders for multiple training runs with the same model
            â”œâ”€â”€ ğŸ“„ training_progress.png         # Plots for reward, loss, epsilon decay and episode length
            â”œâ”€â”€ ğŸ“„ training_stats.json           # File for architecture, hyperparameters and training stats used for plotting
    â”œâ”€â”€ ğŸ“„ agent.py                              # Main agent implementation with DQN/Double DQN support
    â”œâ”€â”€ ğŸ“„ utils.py                              # Training utilities and visualization tools
    â””â”€â”€ ğŸ“„ q_learning.py                         # Single DQN implementation with prioritized experience replay
â””â”€â”€ ğŸ“ our_agent/
    â”œâ”€â”€ ğŸ“„ agent.py                              # Main agent implementation with DQN/Double DQN support
    â”œâ”€â”€ ğŸ“„ double_q_learning.py                  # Double DQN implementation with prioritized experience replay
    â”œâ”€â”€ ğŸ“„ q_learning.py                         # Single DQN implementation with prioritized experience replay
    â”œâ”€â”€ ğŸ“„ utils.py                              # Training utilities and visualization tools
    â””â”€â”€ ğŸ“ models/                               # Directory for saved model weights
        â”œâ”€â”€ ğŸ“„ dqn_20250121_091828.pt            # Single DQN model weights
    â””â”€â”€ ğŸ“ training_logs/                        # Directory for training stats and plots
        â”œâ”€â”€ ğŸ“ model_20250121_163646             # Folder for each model   
            â”œâ”€â”€ ğŸ“ run_20250121_163646           # Folders for multiple training runs with the same model
                â”œâ”€â”€ ğŸ“„ training_progress.png     # Plots for reward, loss, epsilon decay and episode length
                â”œâ”€â”€ ğŸ“„ training_stats.json       # File for architecture, hyperparameters and training stats used for plotting        
            â”œâ”€â”€ ğŸ“ run_20250121_212644           # Folders for multiple training runs with the same model
                â”œâ”€â”€ ğŸ“„ training_progress.png     # Plots for reward, loss, epsilon decay and episode length
                â”œâ”€â”€ ğŸ“„ training_stats.json       # File for architecture, hyperparameters and training stats used for plotting

ğŸ“„ requirements.txt                          # Requirements for development environment 

    

Key Components:
- The agent supports both single DQN and double DQN architectures
- Uses prioritized experience replay for better sample efficiency
- Includes comprehensive training logging and visualization
- Model weights are saved with timestamps for version control
- Training runs are visualized and training stats are saved
- Added training and evaluating different agents by introducing new flags
- Implements reward shaping and state optimization for the Bomberman environment

### Instructions on how to train, run and test the agent:

Added flags, other than the ones given are:

--weights: Specify 'fresh' for new weights or timestamp (e.g., '20240315_143022') to load specific checkpoint
--use-double-dqn: Use double DQN instead of single DQN 

Otherwise, training goes as specified:

``` python scripts/main.py --train```

plus additional flags as seen above

For running and testing the model, do:

``` python scripts/main.py --weights <timestamp>```


### Development environment

We used the following versions of these most important libraries:

- Python 3.12.8
- PyTorch 2.5.1
- Numpy 2.2.1

For a detailed version of all packages used, take a look at requirements.txt

Disclaimer: Helper code like utils.py was coded with the help of Claude-3.5-Sonnet!

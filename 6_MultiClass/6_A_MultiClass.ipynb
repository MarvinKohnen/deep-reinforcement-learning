{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 6 A - Multi-Class Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.A.1 Non-linear Classification Task\n",
    "\n",
    "In a classification problem, the model learns from labeled data to assign labels to new, unseen instances (generalization) based on their features. We will will use neural network based models that output probabilities for possible classes, helping make informed decisions based on the predicted categories. We will further work with the MNIST dataset (Modified National Institute of Standards and Technology) which is a classic dataset widely used for training and testing machine learning models in image classification tasks. It consists of 70,000 grayscale images of handwritten digits from 0 to 9, split into 60,000 training images and 10,000 test images.\n",
    "\n",
    "Each image in the dataset is 28x28 pixels, unrolled into a 784-dimensional vector for modeling (our input space). This dataset serves as a benchmark for machine learning algorithms, as it presents a relatively simple, yet meaningful task: identifying handwritten digits. Despite its simplicity, the MNIST dataset is a great starting point for building and evaluating classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "# The astype forces a conversion to a class as an int (not a string)\n",
    "X, y = mnist[\"data\"], mnist[\"target\"].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding \n",
    "\n",
    "In multi-class classification, often one-hot encoding is used. This is a technique used to represent categorical data as binary vectors. In the context of classification, it converts each class label into a binary vector, where only the index corresponding to the class is set to 1, and all other indices are set to 0. This format is useful in machine learning because it provides a standardized way to handle categorical labels in numerical computations.\n",
    "\n",
    "For example, in the MNIST dataset with digits 0 to 9, the label \"3\" would be represented as:\n",
    "\n",
    "`[0,0,0,1,0,0,0,0,0,0]`\n",
    "\n",
    "This encoding is beneficial in training neural networks, as it enables the model to distinguish among all classes during optimization.\n",
    "\n",
    "Task:\n",
    "\n",
    "* Transform y into one-hot encoding\n",
    "* split this into a training and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def load_all_classes(X, y, num_train=60000, num_test=10000):\n",
    "    \"\"\"\n",
    "    Load MNIST data for all digits (0-9) and split into train and test sets with one-hot encoding.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: np.array, feature data for all samples (e.g., from MNIST)\n",
    "    - y: np.array, target labels for all samples (e.g., from MNIST)\n",
    "    - num_train: int, number of samples to return in the training set (default: 60000)\n",
    "    - num_test: int, number of samples to return in the test set (default: 10000)\n",
    "    \n",
    "    Returns:\n",
    "    - X_train, y_train: training data and one-hot encoded labels\n",
    "    - X_test, y_test: test data and one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: \n",
    "    # One-hot encode the labels in y (from range 0 to 9) into a numpy array with the shape [samples,10] as \n",
    "    # a one-hot encoded vector.\n",
    "    # Later: you can simplify this using the OneHotEncoder from sklearn.\n",
    "    y_one_hot = \n",
    "\n",
    "    # TODO: Shuffle and split into training and test sets - using train_test_split from sklearn\n",
    "    X_train, X_test, y_train, y_test = \n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# Use the function to load and split the MNIST dataset\n",
    "X_train, y_train, X_test, y_test = load_all_classes(X, y, num_train=1000, num_test=1000)\n",
    "\n",
    "# Display shapes of the resulting datasets\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "* For One-Hot Encoding: `OneHotEncoder` from `sklearn.preprocessing` is used to convert the labels into a one-hot encoded format, where each label is represented by a binary vector of length 10.\n",
    "* Data Splitting: `train_test_split` is used to split the dataset into training and test sets, with stratification on y to ensure balanced representation of each class in both sets.\n",
    "* Return Values: The function returns `X_train, y_train, X_test`, and `y_test`, with `y_train` and `y_test` in a one-hot encoded format.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "Adapt the old routine for visualization, now using the one-hot encoded targets (`y_train`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "\n",
    "# Set up a 10x10 grid of subplots\n",
    "fig, axes = plt.subplots(10, 10, figsize=(12, 12))\n",
    "fig.suptitle(\"MNIST Digits (10 examples per digit)\", fontsize=16)\n",
    "\n",
    "# TODO \n",
    "# - Search for each digit a couple of examples\n",
    "# - Plot these as images\n",
    "\n",
    "image = X_train[0].reshape(28, 28)\n",
    "        \n",
    "# Plot the image in the corresponding subplot\n",
    "ax = axes[1, 1]\n",
    "ax.imshow(image, cmap=\"gray\")\n",
    "ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.92)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6.A.2 Multi-Layer Perceptron Model\n",
    "\n",
    "In this task, we implement and evaluate a Multi-Layer Perceptron (MLP) model using TensorFlow. We will use a simple MLP architecture with one hidden layer and test how varying the hidden layer size affects model performance on the MNIST dataset. The MNIST dataset consists of grayscale images of handwritten digits (0-9), and our goal is to classify each image correctly.\n",
    "\n",
    "Using TensorFlow for this task allows us to define, train, and evaluate MLP models efficiently, providing insights into how model capacity (controlled by hidden layer size) impacts accuracy and generalization.\n",
    "\n",
    "**Explanation:**\n",
    "- **1. Model Definition and Compilation**:\n",
    "    - Define an MLP model in TensorFlow with:\n",
    "      - A single hidden layer of specified size.\n",
    "      - `relu` activation for the hidden layer.\n",
    "      - `softmax` activation for the output layer (for 10-class classification).\n",
    "    - Compile the model with the Adam optimizer and categorical cross-entropy loss.\n",
    "\n",
    "- **2. Data Preparation**:\n",
    "    - Use the MNIST dataset, with one-hot encoded labels, to train and evaluate the model.\n",
    "  \n",
    "- **3. Train the Model**:\n",
    "    - Train the model on the MNIST training data for a set number of epochs (e.g., 10).\n",
    "    - Monitor both training and validation loss and accuracy throughout training.\n",
    "\n",
    "- **4. Evaluate Model Performance**:\n",
    "    - After training, calculate and display a confusion matrix for the test set to understand the classification performance across different classes.\n",
    "    - Visualize the loss and accuracy trends during training to assess convergence and potential overfitting.\n",
    "\n",
    "By analyzing the model's performance across different configurations, this task demonstrates the impact of hidden layer size and provides a foundational understanding of how MLPs learn to classify images in TensorFlow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the model architecture\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(X_train.shape[1],)),  # Input layer\n",
    "    tf.keras.layers.Dense(32, activation='relu'),                 # Hidden layer with 64 units\n",
    "    tf.keras.layers.Dense(10, activation='softmax')               # Output layer with 10 units for 10 classes\n",
    "])\n",
    "\n",
    "# Compile the model with optimizer, loss function, and metrics\n",
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model and store training history\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=20,  # You can adjust the number of epochs as needed\n",
    "                    batch_size=32, \n",
    "                    validation_data=(X_test, y_test), \n",
    "                    verbose=1)\n",
    "\n",
    "# Plot the loss and accuracy over epochs\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label=\"Training Loss\")\n",
    "plt.plot(history.history['val_loss'], label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss During Training\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label=\"Training Accuracy\")\n",
    "plt.plot(history.history['val_accuracy'], label=\"Validation Accuracy\", color=\"orange\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy During Training\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:**\n",
    "\n",
    "* Create a confusion matrix for the multi-class classifier (see Task 4.A.$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Convert one-hot encoded y_test to class labels\n",
    "y_test_labels = np.argmax(y_test, axis=1)  # True labels\n",
    "y_pred = ...\n",
    "# TODO\n",
    "y_pred_labels = ...\n",
    "\n",
    "# Compute the confusion matrix for the test set\n",
    "cm_test = confusion_matrix(y_test_labels, y_pred_labels)\n",
    "\n",
    "# Display the confusion matrix for the test set\n",
    "disp_test = ConfusionMatrixDisplay(confusion_matrix=cm_test, display_labels=np.arange(10))\n",
    "disp_test.plot(cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix (Test Set)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment: Evaluating the Impact of Hidden Layer Size on MLP Performance\n",
    "\n",
    "In this experiment, we aim to understand how the size of the hidden layer affects the performance of a simple Multi-Layer Perceptron (MLP) on the MNIST digit classification task. We will train MLP models with varying numbers of neurons in the hidden layer and measure both training and testing accuracy after 10 epochs for each configuration.\n",
    "\n",
    "By comparing different hidden layer sizes, we hope to gain insight into the model's ability to capture patterns in the data with limited complexity and evaluate its generalization performance.\n",
    "\n",
    "**Tasks:**\n",
    "- **1. Define Hidden Layer Sizes**: Specify a list of hidden layer sizes to test: `[1, 2, 4, 8, 16, 32, 64, 128]`.\n",
    "  \n",
    "- **2. Model Setup and Compilation**:\n",
    "    - For each hidden layer size, define an MLP model with:\n",
    "      - One hidden layer of the specified size.\n",
    "      - `relu` activation for the hidden layer.\n",
    "      - `softmax` activation for the output layer (for 10-class classification).\n",
    "    - Compile the model with the Adam optimizer and categorical cross-entropy loss.\n",
    "  \n",
    "- **3. Train the Model**: Train each model for 10 epochs on the MNIST training set, using a batch size of 32.\n",
    "\n",
    "- **4. Evaluate Accuracy**:\n",
    "    - Calculate both the training and test accuracy for each model configuration.\n",
    "    - Store the accuracies to compare how hidden layer size impacts model performance.\n",
    "  \n",
    "- **5. Visualize Results**:\n",
    "    - Plot the training and test accuracy for each hidden layer size to visualize the relationship between hidden layer size and model performance.\n",
    "\n",
    "This approach will allow us to analyze the trade-offs in model complexity and generalization across various hidden layer sizes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define hidden layer sizes to test, starting with 1 and 2\n",
    "hidden_layer_sizes = [1, 2, 4, 8, 16, 32, 64, 128]\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "# TODO: Based on the code from above, \n",
    "# - vary the hidden layer size \n",
    "# - for each one build a model, train it (try only a couple epochs)\n",
    "# - append the accuracies to the different lists \n",
    "# Loop over different hidden layer sizes\n",
    "\n",
    "\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(hidden_layer_sizes, train_accuracies, marker='o', label=\"Train Accuracy\")\n",
    "plt.plot(hidden_layer_sizes, test_accuracies, marker='o', label=\"Test Accuracy\")\n",
    "plt.xlabel(\"Hidden Layer Size\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Train and Test Accuracy for Different Hidden Layer Sizes (10 Epochs)\")\n",
    "plt.xticks(hidden_layer_sizes)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Model Complexity by Hidden Layer Size\n",
    "\n",
    "In this task, we aim to understand how the number of parameters in a Multi-Layer Perceptron (MLP) model changes with different hidden layer sizes. By testing a variety of hidden layer sizes on an MLP for the MNIST dataset, we can evaluate the trade-offs in model complexity, computation requirements, and potential learning capacity.\n",
    "\n",
    "**Explanation of Parameter Calculation**\n",
    "\n",
    "Each MLP model in this experiment has:\n",
    "1. **Input Layer to Hidden Layer**: The number of parameters (weights and biases) connecting the input layer to the hidden layer can be calculated as:\n",
    "   $$\n",
    "   \\text{Parameters} = (\\text{Input Dimension} + 1) \\times \\text{Hidden Layer Size}\n",
    "   $$\n",
    "   where `+1` accounts for the bias term.\n",
    "\n",
    "2. **Hidden Layer to Output Layer**: The number of parameters connecting the hidden layer to the output layer (with 10 output neurons for MNIST’s 10 classes) is calculated as:\n",
    "   $$\n",
    "   \\text{Parameters} = (\\text{Hidden Layer Size} + 1) \\times \\text{Output Dimension}\n",
    "   $$\n",
    "   where `+1` accounts for the bias term in the output layer.\n",
    "\n",
    "3. **Total Parameters**: The total number of parameters is the sum of the parameters from the input-to-hidden and hidden-to-output connections.\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "1. **Loop through Hidden Layer Sizes**: Define a list of hidden layer sizes to test, such as `[1, 2, 4, 8, 16, 32, 64, 128]`.\n",
    "\n",
    "2. **Define and Compile Model**:\n",
    "    - For each hidden layer size, define an MLP model with:\n",
    "      - One hidden layer of the specified size.\n",
    "      - `relu` activation for the hidden layer.\n",
    "      - `softmax` activation for the output layer (for 10-class classification).\n",
    "    - Use TensorFlow’s `.summary()` method to display the number of parameters in each layer and confirm calculations.\n",
    "\n",
    "3. **Calculate and Store Parameters**:\n",
    "    - Use `model.count_params()` to calculate the total number of parameters for each model configuration.\n",
    "\n",
    "4. **Plot the Results**:\n",
    "    - Visualize the total number of parameters for each hidden layer size to understand how model complexity scales with layer size.\n",
    "\n",
    "This approach provides a clear visualization of how hidden layer size influences model complexity, helping us analyze the trade-offs between model capacity and computational cost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the input and output dimensions for MNIST\n",
    "input_dim = 784  # 28x28 images flattened\n",
    "output_dim = 10  # 10 classes for digits 0-9\n",
    "\n",
    "# Define hidden layer sizes to test\n",
    "hidden_layer_sizes = [1, 2, 4, 8, 16, 32, 64, 128]\n",
    "total_params_list = []\n",
    "\n",
    "# Loop through each hidden layer size, create a model, and calculate parameters\n",
    "for hidden_size in hidden_layer_sizes:\n",
    "    # TODO\n",
    "    # Calculate for each layer size how many neurons are required \n",
    "\n",
    "    # As an alternative: Simply rebuild the model and inspect these\n",
    "    # Compare the values\n",
    "    # Define the model with one hidden layer of `hidden_size`\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=(input_dim,)),\n",
    "        tf.keras.layers.Dense(hidden_size, activation='relu'),\n",
    "        tf.keras.layers.Dense(output_dim, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Display the model summary to see the parameters for each layer\n",
    "    print(f\"\\nModel with hidden layer size {hidden_size}:\")\n",
    "    model.summary()\n",
    "    \n",
    "    # Calculate total parameters using the model's .count_params() method\n",
    "    total_params = model.count_params()\n",
    "    total_params_list.append(total_params)\n",
    "\n",
    "# Plot the total number of parameters for each hidden layer size\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(hidden_layer_sizes, total_params_list, marker='o')\n",
    "plt.xlabel(\"Hidden Layer Size\")\n",
    "plt.ylabel(\"Total Parameters\")\n",
    "plt.title(\"Total Parameters for Different Hidden Layer Sizes\")\n",
    "plt.xticks(hidden_layer_sizes)\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6.A.3 Regularization\n",
    "\n",
    "In this task, we explore regularization techniques (for example: L2 regularization or dropout) to prevent overfitting in a neural network with 128 hidden neurons. Regularization helps improve generalization by reducing the model's sensitivity to the training data, encouraging it to learn patterns that generalize better to new data.\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "1. Implement an MLP Model with Regularization:\n",
    "    - Define an MLP with 128 hidden neurons, applying L2 or dropout regularization on the weights.  \n",
    "2. Train the Model:\n",
    "    - Train the model for a set number of epochs (e.g., 20) and monitor training and validation loss.\n",
    "3. Visualize Learning Curves:\n",
    "    - Plot the training and validation loss for the model to compare the effects of regularization on the model's learning dynamics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define model architecture parameters\n",
    "input_dim = 784  # 28x28 images flattened to 784 features\n",
    "hidden_neurons = 128  # Hidden layer with 128 neurons\n",
    "output_dim = 10  # Output layer for 10 classes\n",
    "epochs = 10  # Number of epochs\n",
    "\n",
    "# Model 1: MLP without any regularization (baseline)\n",
    "model_no_reg = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(input_dim,)),\n",
    "    tf.keras.layers.Dense(hidden_neurons, activation='relu'),\n",
    "    tf.keras.layers.Dense(output_dim, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the baseline model without regularization\n",
    "model_no_reg.compile(optimizer='adam',\n",
    "                     loss='categorical_crossentropy',\n",
    "                     metrics=['accuracy'])\n",
    "\n",
    "# Train the baseline model without regularization\n",
    "history_no_reg = model_no_reg.fit(X_train, y_train,\n",
    "                                  epochs=epochs,\n",
    "                                  batch_size=32,\n",
    "                                  validation_data=(X_test, y_test),\n",
    "                                  verbose=1)\n",
    "\n",
    "# TODO: Build a model with L2 regularization\n",
    "# You have to add a kernel_regularizer as an option in the dense layer\n",
    "# (see above for the full model)\n",
    "# Model 2: MLP with L2 regularization\n",
    "model_l2 = ...\n",
    "\n",
    "# Compile the L2-regularized model\n",
    "model_l2.compile(optimizer='adam',\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "# Train the L2-regularized model\n",
    "history_l2 = model_l2.fit(X_train, y_train,\n",
    "                          epochs=epochs,\n",
    "                          batch_size=32,\n",
    "                          validation_data=(X_test, y_test),\n",
    "                          verbose=1)\n",
    "\n",
    "# TODO: Include a Dropout layer in-between the last two dense layers with a dropout of 0.5\n",
    "# Model 3: MLP with Dropout regularization\n",
    "model_dropout = ...\n",
    "\n",
    "# Compile the Dropout-regularized model\n",
    "model_dropout.compile(optimizer='adam',\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "# Train the Dropout-regularized model\n",
    "history_dropout = model_dropout.fit(X_train, y_train,\n",
    "                                    epochs=epochs,\n",
    "                                    batch_size=32,\n",
    "                                    validation_data=(X_test, y_test),\n",
    "                                    verbose=1)\n",
    "\n",
    "# Plot the learning curves for all three models (accuracy)\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot training and validation accuracy for L2 regularization\n",
    "plt.plot(history_l2.history['accuracy'], label='Train Accuracy (L2)', linestyle='--', color='blue')\n",
    "plt.plot(history_l2.history['val_accuracy'], label='Val Accuracy (L2)', color='blue')\n",
    "\n",
    "# Plot training and validation accuracy for Dropout regularization\n",
    "plt.plot(history_dropout.history['accuracy'], label='Train Accuracy (Dropout)', linestyle='--', color='orange')\n",
    "plt.plot(history_dropout.history['val_accuracy'], label='Val Accuracy (Dropout)', color='orange')\n",
    "\n",
    "# Plot training and validation accuracy for the model without regularization\n",
    "plt.plot(history_no_reg.history['accuracy'], label='Train Accuracy (No Reg)', linestyle='--', color='green')\n",
    "plt.plot(history_no_reg.history['val_accuracy'], label='Val Accuracy (No Reg)', color='green')\n",
    "\n",
    "# Add plot details\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training and Validation Accuracy for Different Regularization Techniques\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6.A.4 Convolutional Neural Network\n",
    "\n",
    "We now explore a minimal Convolutional Neural Network (CNN) architecture designed to classify handwritten digits in the MNIST dataset. Initially, the network consists of a single convolutional layer, a pooling layer, and a fully connected (dense) layer. Despite its simplicity, this CNN can achieve competitive accuracy (around 89% on the test set), similar to the best performance observable with a Multi-Layer Perceptron (MLP).\n",
    "\n",
    "### Architecture:\n",
    "1. **Convolutional Layer**:\n",
    "   - Applies 32 filters of size 3x3 with ReLU activation to the input images, detecting local patterns such as edges and shapes.\n",
    "\n",
    "2. **Max-Pooling Layer**:\n",
    "   - A 2x2 max-pooling layer downsamples the feature maps, retaining key information while reducing spatial dimensions. This process helps prevent overfitting and reduces computation.\n",
    "\n",
    "3. **Flatten Layer**:\n",
    "   - Transforms the 2D feature maps into a 1D vector, enabling connection to the fully connected layer.\n",
    "\n",
    "4. **Fully Connected (Dense) Layer**:\n",
    "   - A dense layer with 128 neurons and ReLU activation provides the model's learning capacity. \n",
    "   - The output layer uses softmax activation with 10 neurons (one for each digit class), making the model suitable for multi-class classification.\n",
    "\n",
    "**Training and Evaluation**:\n",
    "\n",
    "- **Training**: The model is trained for 10 epochs with a batch size of 32. Both training and validation accuracy are monitored to observe learning progression.\n",
    "- **Evaluation**: The model achieves around 89% accuracy on the test set, demonstrating that even a minimal CNN can capture the essential features for MNIST classification.\n",
    "\n",
    "This minimal CNN provides a powerful yet efficient baseline for image classification and can be extended with additional layers or regularization for improved performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reshape the data to include a single channel for grayscale images (28x28x1)\n",
    "X_train_cnn = X_train.reshape(-1, 28, 28, 1)\n",
    "X_test_cnn = X_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# Define the minimal CNN architecture\n",
    "model_cnn = tf.keras.models.Sequential([\n",
    "    # Convolutional layer with 32 filters, kernel size 3x3, and ReLU activation\n",
    "    tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    \n",
    "    # Max pooling layer with pool size 2x2\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    # Flatten layer to reshape 2D feature maps to 1D feature vector\n",
    "    tf.keras.layers.Flatten(),\n",
    "    \n",
    "    # Fully connected (dense) layer with 128 units and ReLU activation\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    \n",
    "    # Output layer with 10 units (one for each class) and softmax activation\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the CNN model\n",
    "model_cnn.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Train the CNN model\n",
    "history_cnn = model_cnn.fit(X_train_cnn, y_train,\n",
    "                            epochs=10,\n",
    "                            batch_size=32,\n",
    "                            validation_data=(X_test_cnn, y_test),\n",
    "                            verbose=1)\n",
    "\n",
    "# Plot the training and validation accuracy for the CNN\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history_cnn.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history_cnn.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training and Validation Accuracy for the Minimal CNN\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model_cnn.evaluate(X_test_cnn, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the LeNet-5 Architecture\n",
    "\n",
    "In this task, we will adapt our minimal Convolutional Neural Network (CNN) to implement the **LeNet-5 architecture**, a classic CNN architecture designed by Yann LeCun for handwritten digit recognition. LeNet-5 is well-suited for the MNIST dataset and includes several convolutional and pooling layers, followed by fully connected layers. \n",
    "\n",
    "![](solution/lenet5.svg)\n",
    "\n",
    "1. **Input Layer**: Accepts 28x28 grayscale images as input, with each image normalized to a range of 0–1.\n",
    "\n",
    "2. **Convolutional Layer 1**:\n",
    "   - Applies 6 filters of size 5x5 with stride 1 and ReLU activation, producing 6 feature maps.\n",
    "   - The resulting feature maps have dimensions of 24x24.\n",
    "\n",
    "3. **Pooling Layer 1 (Average Pooling)**:\n",
    "   - Averages over 2x2 regions, reducing each feature map to a size of 12x12.\n",
    "   - Pooling reduces the spatial dimensions and adds robustness to small spatial translations.\n",
    "\n",
    "4. **Convolutional Layer 2**:\n",
    "   - Applies 16 filters of size 5x5 with stride 1 and ReLU activation, producing 16 feature maps.\n",
    "   - The resulting feature maps have dimensions of 8x8.\n",
    "\n",
    "5. **Pooling Layer 2 (Average Pooling)**:\n",
    "   - Another 2x2 average pooling layer, reducing each feature map to a size of 4x4.\n",
    "\n",
    "6. **Fully Connected Layers**:\n",
    "   - **Fully Connected Layer 1**: Flattens the output from the convolutional layers and connects to a fully connected layer with 120 neurons and ReLU activation.\n",
    "   - **Fully Connected Layer 2**: Connects to a fully connected layer with 84 neurons and ReLU activation.\n",
    "   - **Output Layer**: The final layer is a fully connected layer with 10 neurons and softmax activation, used for classification into the 10 digit classes.\n",
    "\n",
    "**Tasks:**\n",
    "- **1. Implement LeNet-5 Architecture**: Define a CNN model in TensorFlow that replicates the LeNet-5 architecture as described above.\n",
    "- **2. Compile and Train**: Compile the model using categorical cross-entropy loss and the Adam optimizer. Train the model on the MNIST dataset for a set number of epochs.\n",
    "- **3. Evaluate Performance**: Plot the training and validation accuracy over epochs and calculate the final test accuracy. Compare this performance with the minimal CNN and MLP results from previous tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reshape the data to include a single channel for grayscale images (28x28x1)\n",
    "X_train_cnn = X_train.reshape(-1, 28, 28, 1)\n",
    "X_test_cnn = X_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# Define the LeNet-5 architecture\n",
    "model_lenet5 = tf.keras.models.Sequential([\n",
    "    # TODO: Add the LeNet-5 model layers\n",
    "    # Output Layer: 10 units with softmax activation for classification\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_lenet5.compile(optimizer='adam',\n",
    "                     loss='categorical_crossentropy',\n",
    "                     metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history_lenet5 = model_lenet5.fit(X_train_cnn, y_train,\n",
    "                                  epochs=20,\n",
    "                                  batch_size=32,\n",
    "                                  validation_data=(X_test_cnn, y_test),\n",
    "                                  verbose=1)\n",
    "\n",
    "# Plot the learning curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history_lenet5.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history_lenet5.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training and Validation Accuracy for LeNet-5 Architecture\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model_lenet5.evaluate(X_test_cnn, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right;\">\n",
    "<i>This notebook has been created with the help of ChatGPT-4, 14.11.2024; Explanations were initially generated and afterwards edited;</i>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

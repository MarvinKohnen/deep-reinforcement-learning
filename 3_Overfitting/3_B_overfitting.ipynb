{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3 B - Tutorial: Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we generate again a data set for the sine curve (8 data points with noise with a standard deviation of 1; see tasks 3-A). Simply execute this first cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the course: The notebooks are in colab as well -- we will use this link for sharing solutions and presenting your results. Only access and update these when asked to!\n",
    "\n",
    "* [notebook A](https://colab.research.google.com/drive/106gCfEiCnkV6n3elKUXBgysae809GxLg?usp=sharing)\n",
    "* [notebook B](https://colab.research.google.com/drive/18V9wZ48ZMirDAHxf_HRj3qrv0s8QMvjK?usp=sharing)\n",
    "\n",
    "If we will switch to screensharing, we will use this zoom room:\n",
    "[zoom link](https://uni-ms.zoom-x.de/j/5915556541?pwd=UlfnH7oCYzHBQgDAD4NvCJbG23lXCN.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_sine_data(n_points, noise=0.2):\n",
    "    \"\"\"\n",
    "    Generates a synthetic dataset based on the sine function with added noise.\n",
    "\n",
    "    Parameters:\n",
    "    n_points (int): The number of data points to generate.\n",
    "    noise (float): The standard deviation of the noise to add to the sine values.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Two numpy arrays, x and y, where x is in the range [0, 2*pi] and\n",
    "           y is the sine of x with added noise.\n",
    "    \"\"\"\n",
    "    # Generate linearly spaced x values between 0 and 2*pi\n",
    "    x = np.linspace(0, 2 * np.pi, n_points)\n",
    "    \n",
    "    # TODO: Compute the sine on the x values and add some noise\n",
    "    # Create the y values in return.\n",
    "    # Compute the sine of each x value\n",
    "    y = np.sin(x)\n",
    "    # Add random noise to y values\n",
    "    y += np.random.normal(0, noise, size=n_points)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "# Generate Data Set\n",
    "x_train, y_train = generate_sine_data(8, 0.3)\n",
    "x_sine, y_sine = generate_sine_data(100, 0.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial in scikit-learn\n",
    "\n",
    "Next, we use the implementation of a polynomial from scikit-learn. This includes two parts:\n",
    "\n",
    "1. **Feature Transformation:** Use 'PolynomialFeatures' to transform the original input features into polynomial features up to a specified degree.\n",
    "2. **Linear Regression**: Apply 'LinearRegression' to fit a linear model in the transformed polynomial feature space. The model learns the coefficients for each polynomial term, enabling it to capture the polynomial relationship in the original input data. For further explanation [see documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html).\n",
    "\n",
    "Together, these steps allow scikit-learn to learn a polynomial regression model by converting the problem into a linear regression task in a higher-dimensional feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Reshape x to be a 2D array for sklearn\n",
    "x_train = x_train.reshape(-1, 1)\n",
    "x_sine = x_sine.reshape(-1, 1)\n",
    "\n",
    "# Polynomial Regression Model (for example, degree 3)\n",
    "poly_features = PolynomialFeatures(degree=3)\n",
    "x_train_poly = poly_features.fit_transform(x_train)\n",
    "x_sine_poly = poly_features.fit_transform(x_sine)\n",
    "\n",
    "polynomial_model = LinearRegression()\n",
    "polynomial_model.fit(x_train_poly, y_train)\n",
    "y_pred = polynomial_model.predict(x_train_poly)\n",
    "y_sine_pred = polynomial_model.predict(x_sine_poly)\n",
    "\n",
    "# Plot the noisy data and the model's predictions\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x_train, y_train, label='Training Data', color='C0', alpha=0.6, marker='x')\n",
    "plt.plot(x_sine, y_sine_pred, label='Fitted Polynomial Model (sklearn) ', color='C2')            \n",
    "plt.plot(x_sine, y_sine, label='Original sine curve ', color='C3')\n",
    "plt.xlabel('x values')\n",
    "plt.ylabel('y values')\n",
    "plt.title('Fitting Polynomial Model to Noisy Sine Data')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Calculate Mean Squared Error for both models\n",
    "mse_poly = mean_squared_error(y_train, y_pred)\n",
    "_, y_train_no_noise = generate_sine_data(8, 0.0)\n",
    "mse_real_comp = mean_squared_error(y_train, y_train_no_noise)\n",
    "\n",
    "# Print the MSE for comparison\n",
    "print(f\"Polynomial Model (Degree 3) MSE    : {mse_poly:.4f}\")\n",
    "print(f\"MSE between y_train and real values: {mse_real_comp:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3.B.1 Comparison of fitting of different polynomials\n",
    "\n",
    "The goal is to evaluate how the polynomial degree affects the model's ability to fit the training data by varying the polynomial degree from 1 to a higher number (e.g., 10 -- which would be sufficient for the $8$ data points in the example?). We will calculate the Mean Squared Error (MSE) for each polynomial degree and visualize the results in a plot. This will help understand how model complexity influences the fit and the training error.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "Loop through polynomial degrees: Vary the polynomial degree from 1 to 10.\n",
    "\n",
    "* In each iteration: Train a polynomial regression model for each degree using 'PolynomialFeatures' and 'LinearRegression'.\n",
    "* Calculate the MSE for the training data predictions and store the MSE values for each degree.\n",
    "* Plot the MSE values against the polynomial degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array to store MSE values for different polynomial degrees\n",
    "mse_values = []\n",
    "\n",
    "# Loop over polynomial degrees from 1 to 10\n",
    "for degree in range(1, 11):\n",
    "    # TODO: Create polynomial features (see above ...)\n",
    "    poly_features\n",
    "    x_train_poly \n",
    "\n",
    "    # TODO: Train the polynomial regression model\n",
    "    # This works the same way as before for a multidimensional model\n",
    "    polynomial_model = LinearRegression()\n",
    "\n",
    "\n",
    "    # TODO: Calculate MSE between the original y_train and the predicted values from x_train\n",
    "    # and collect the MSE.\n",
    "    # Predict using the trained model\n",
    "\n",
    "    # Calculate the MSE for the current degree\n",
    "    mse = mean_squared_error(y_train, y_pred)\n",
    "    mse_values.append(mse)\n",
    "\n",
    "    # Print MSE for each degree (optional)\n",
    "    print(f\"Degree {degree}: MSE = {mse:.4f}\")\n",
    "\n",
    "# Plot the MSE values for different polynomial degrees\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, 11), mse_values, marker='o', linestyle='-', color='C0')\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.title('MSE vs. Polynomial Degree for Training Data')\n",
    "plt.xticks(range(1, 11))\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, there is already code for plotting the polynomial fitted curve. Test different degrees of polynoms and fit these to the data -- what do you observe?\n",
    "\n",
    "**Task:**\n",
    "\n",
    "* What are your observations?\n",
    "    * How does the curve fit the training data?\n",
    "    * How does it fit the original sine curve in your opinion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial Regression Model (for example, degree 3)\n",
    "# TODO: Only change the degree - how does this affect the model?\n",
    "poly_features = PolynomialFeatures(degree=3)\n",
    "x_train_poly = poly_features.fit_transform(x_train)\n",
    "x_sine_poly = poly_features.fit_transform(x_sine)\n",
    "\n",
    "polynomial_model = LinearRegression()\n",
    "polynomial_model.fit(x_train_poly, y_train)\n",
    "y_pred = polynomial_model.predict(x_train_poly)\n",
    "y_sine_pred = polynomial_model.predict(x_sine_poly)\n",
    "\n",
    "# Plot the noisy data and the model's predictions\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x_train, y_train, label='Training Data', color='C0', alpha=0.6, marker='x')\n",
    "plt.plot(x_sine, y_sine_pred, label='Fitted Polynomial Model (sklearn) ', color='C2')            \n",
    "plt.plot(x_sine, y_sine, label='Original sine curve ', color='C3')\n",
    "plt.xlabel('x values')\n",
    "plt.ylabel('y values')\n",
    "plt.title('Fitting Polynomial Model to Noisy Sine Data')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Calculate Mean Squared Error for both models\n",
    "mse_poly = mean_squared_error(y_train, y_pred)\n",
    "_, y_train_no_noise = generate_sine_data(8, 0.0)\n",
    "mse_real_comp = mean_squared_error(y_train, y_train_no_noise)\n",
    "\n",
    "# Print the MSE for comparison\n",
    "print(f\"Polynomial Model (Degree 3) MSE    : {mse_poly:.4f}\")\n",
    "print(f\"MSE between y_train and real values: {mse_real_comp:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create randomly spaced training data (Optional)\n",
    "\n",
    "As one further step: In the examples above, training data was created at equally spaced positons along the x-axis. In real data, these positions will usually be affected as well by noise or are not equally spaced. Generate a new training data set (same size) from random positions on the x-axis and calculate the corresponding sine values for these.\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "* Generate random positions along the x-axis.\n",
    "* Run the same visualization from above (for different degrees) and analyze how the fitted model relates to the training data and to the real sine curve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data_random_positions(n_points, noise=0.2):\n",
    "    \"\"\"\n",
    "    Generates a synthetic dataset based on the sine function with added noise.\n",
    "\n",
    "    Parameters:\n",
    "    n_points (int): The number of data points to generate.\n",
    "    noise (float): The standard deviation of the noise to add to the sine values.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Two numpy arrays, x and y, where x is in the range [0, 2*pi] and\n",
    "           y is the sine of x with added noise.\n",
    "    \"\"\"\n",
    "    # TODO: Generate random x positions between 0 and 2*pi\n",
    "    x = \n",
    "    # You afterwards have to sort these values.\n",
    "    x.sort()\n",
    "    \n",
    "    # Create the y values in return.\n",
    "    # Compute the sine of each x value\n",
    "    y = np.sin(x)\n",
    "    # Add random noise to y values\n",
    "    y += np.random.normal(0, noise, size=n_points)\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_random_pos, y_train_random_pos = generate_training_data_random_positions(8, 0.3)\n",
    "x_train_random_pos = x_train_random_pos.reshape(-1,1)\n",
    "\n",
    "# TODO: Generate the polynomial features for different degrees\n",
    "# How does this affect the values?\n",
    "degree = 6\n",
    "poly_features = PolynomialFeatures(degree=degree)\n",
    "x_train_poly = poly_features.fit_transform(x_train_random_pos)\n",
    "x_sine_poly = poly_features.fit_transform(x_sine)\n",
    "\n",
    "# Train the polynomial regression model for degree 6\n",
    "polynomial_model = LinearRegression()\n",
    "#polynomial_model.fit(x_train_poly, y_train)\n",
    "polynomial_model.fit(x_train_poly, y_train_random_pos)\n",
    "\n",
    "# Predict the output for both the training data and sine curve\n",
    "y_train_pred = polynomial_model.predict(x_train_poly)\n",
    "y_sine_pred = polynomial_model.predict(x_sine_poly)\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot the training data\n",
    "plt.scatter(x_train_random_pos, y_train_random_pos, label='Training Data', color='C0', alpha=0.6, marker='x')\n",
    "\n",
    "# Plot the original sine curve\n",
    "plt.plot(x_sine, np.sin(x_sine), label='Original Sine Curve', color='C3', linewidth=2)\n",
    "\n",
    "# Plot the polynomial model's predictions\n",
    "plt.plot(x_sine, y_sine_pred, label=f'Polynomial Model (Degree {degree})', color='C2', linewidth=2)\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel('x values')\n",
    "plt.ylim(-5, 5)\n",
    "plt.ylabel('y values')\n",
    "plt.title('Comparison of Training Data, True Sine Curve, and Degree-6 Polynomial Model')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3.B.2 Testing Generalization Capabilities of a Model\n",
    "\n",
    "The goal is to evaluate the generalization ability of the polynomial models trained in the previous task by testing them on a new test dataset. We will generate a new test dataset from the sine function, using the original distribution, with a larger number of points (e.g., 100). The trained polynomial models (with varying degrees) will be used to make predictions on this test dataset, and we will calculate the Mean Squared Error (MSE) for each model to see how well each polynomial degree generalizes to unseen data.\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "* generate a new test data set that wasn't used for training (same noise level, e.g., 100 data points)\n",
    "* again, train polynomial models from degree $1$ up to $7$ on the 'x_train, y_train' data set and compute (as above) the training MSE for the different models,\n",
    "* in addition, always evaluate each of these models on the test data set and store these values,\n",
    "* plot both curves showing how the MSE develops in relation to the degree of the polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generate test data (100 points, noisy sine wave with noise level 0.3)\n",
    "x_test, y_test = \n",
    "x_test = x_test.reshape(-1, 1)  # Reshape for sklearn\n",
    "\n",
    "# Arrays to store MSE values for different polynomial degrees\n",
    "mse_train_values = []\n",
    "mse_test_values = []\n",
    "\n",
    "# TODO: Evaluate polynomial models of varying degrees\n",
    "for degree in range(1, 8):\n",
    "    # TODO: Create polynomial features for the current degree for both data sets!\n",
    "    poly_features = PolynomialFeatures(degree=degree)\n",
    "\n",
    "    # TODO: Train the polynomial model on the training data set.\n",
    "    polynomial_model = LinearRegression()\n",
    "\n",
    "\n",
    "    # TODO: Predict on the training and test data\n",
    "    y_train_pred =\n",
    "    y_test_pred = \n",
    "\n",
    "    # Calculate the MSE for the training and test data\n",
    "    mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "    mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "    # Store the MSE values\n",
    "    mse_train_values.append(mse_train)\n",
    "    mse_test_values.append(mse_test)\n",
    "\n",
    "    # Print MSE for each degree (optional)\n",
    "    print(f\"Degree {degree}: Train MSE = {mse_train:.4f}, Test MSE = {mse_test:.4f}\")\n",
    "\n",
    "# TODO: Plot the MSE values for both training and test data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, 8), mse_train_values, marker='o', linestyle='-', color='C0', label='Train MSE')\n",
    "plt.plot ...\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.title('Train and Test MSE vs. Polynomial Degree (Noise = 1.0)')\n",
    "plt.xticks(range(1, 8))\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3.B.3 Regularization\n",
    "\n",
    "When fitting polynomial models to data, higher-degree models can perfectly fit the training data by capturing even the smallest fluctuations. This is known as **overfitting**, where the model learns the noise in the training data rather than the underlying pattern. In such cases, the model's weights (coefficients) tend to become very large, as it needs to assign high values to certain terms to achieve a perfect fit. This results in a model that generalizes poorly to new, unseen data because it is overly sensitive to the training data's specific details.\n",
    "\n",
    "#### Why Do Weights Get Large in Overfitting?\n",
    "\n",
    "- **High-degree polynomials** introduce more flexibility in the model, allowing it to fit every data point exactly.\n",
    "- To achieve this perfect fit, the model assigns disproportionately large weights to the polynomial terms.\n",
    "- These large weights indicate that the model is fitting the noise, leading to high variance and poor generalization to new data.\n",
    "\n",
    "#### How Regularization (e.g., $L_2$) Can Help\n",
    "\n",
    "- **Regularization techniques** like $L_2$ regularization (Ridge) add a penalty term to the model's loss function based on the size of the weights.\n",
    "- In $L_2$ regularization, the penalty is proportional to the sum of the squares of the weights, discouraging the model from assigning excessively large values to any weight.\n",
    "- By controlling the weight sizes, regularization forces the model to prioritize fitting the main trends in the data rather than the noise, leading to better generalization.\n",
    "\n",
    "**Tasks:** \n",
    "\n",
    "Look at the Weights of Different Polynomial Models\n",
    "\n",
    "- By examining the weights for polynomial models of varying degrees, you can observe that as the degree increases, the weights tend to become larger, especially without regularization.\n",
    "\n",
    "When $L_2$ regularization is applied, the weights are kept smaller, even for higher-degree models, resulting in a more robust fit that generalizes better to test data (you can implement this using the ['Ridge' model from scikit-learn](https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.Ridge.html)).\n",
    "\n",
    "Regularization helps balance the model's complexity and prevents it from fitting the noise in the training data, making it more effective for real-world applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrays to store MSE values for different polynomial degrees\n",
    "mse_train_values = []\n",
    "\n",
    "# TODO: Work in a for loop for the different degrees \n",
    "poly_features = PolynomialFeatures(degree=1)\n",
    "x_train_poly = poly_features.fit_transform(x_train_random_pos)\n",
    "\n",
    "# Train the polynomial model\n",
    "polynomial_model = LinearRegression()\n",
    "polynomial_model.fit(x_train_poly, y_train_random_pos)\n",
    "\n",
    "# Predict on the training and test data\n",
    "y_train_pred = polynomial_model.predict(x_train_poly)\n",
    "\n",
    "# TODO: Access attributes of the polynomial_model object (the .coef_ and the .intercept_ = weight at position 0)\n",
    "# print these weights \n",
    "\n",
    "# Calculate the MSE for the training and test data\n",
    "mse_train = mean_squared_error(y_train_random_pos, y_train_pred)\n",
    "\n",
    "# Store the MSE values\n",
    "mse_train_values.append(mse_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization: Variation of Parameter\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "* Vary the $L_2$ weight in the code below and compare the different errors: training and test.\n",
    "* Visualize the resulting curves as well for the polynomial and the regularized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "degree = 8\n",
    "# Create polynomial features for the current degree\n",
    "poly_features = PolynomialFeatures(degree=degree)\n",
    "x_train_poly = poly_features.fit_transform(x_train_random_pos)\n",
    "x_test_poly = poly_features.transform(x_test)\n",
    "\n",
    "# Train the polynomial model\n",
    "polynomial_model = LinearRegression()\n",
    "polynomial_model.fit(x_train_poly, y_train_random_pos)\n",
    "\n",
    "# TODO: vary the regularization parameter\n",
    "alpha = 1.0\n",
    "regularized_model = Ridge(alpha=alpha)\n",
    "regularized_model.fit(x_train_poly, y_train_random_pos)\n",
    "\n",
    "# Predict on training and test data\n",
    "y_train_pred = polynomial_model.predict(x_train_poly)\n",
    "y_train_regularized_pred = regularized_model.predict(x_train_poly)\n",
    "y_test_pred = polynomial_model.predict(x_test_poly)\n",
    "y_test_regularized_pred = regularized_model.predict(x_test_poly)\n",
    "\n",
    "# Calculate the MSE for the training and test data\n",
    "mse_train = mean_squared_error(y_train_random_pos, y_train_pred)\n",
    "mse_train_regularized = mean_squared_error(y_train_random_pos, y_train_regularized_pred)\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "mse_test_regularized = mean_squared_error(y_test, y_test_regularized_pred)\n",
    "\n",
    "# Print MSE for each degree\n",
    "print(f\"Degree {degree}: Train MSE = {mse_train:.4f}, Regularized MSE = {mse_train_regularized:.4f}\")\n",
    "print(f\"Degree {degree}: Test MSE  = {mse_test:.4f}, Regularized MSE = {mse_test_regularized:.4f}\")\n",
    "    \n",
    "# Print the weight values (coefficients and intercept)\n",
    "print(f\"Coefficients: {polynomial_model.coef_}\")\n",
    "print(f\"Coeff. Regul: {regularized_model.coef_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot the training data\n",
    "plt.scatter(x_train_random_pos, y_train_random_pos, label='Training Data', color='C0', alpha=0.6, marker='x')\n",
    "\n",
    "# Plot the original sine curve\n",
    "plt.plot(x_sine, np.sin(x_sine), label='Original Sine Curve', color='C3', linewidth=2)\n",
    "\n",
    "# Plot the polynomial model's predictions\n",
    "plt.plot(x_test, y_test_pred, label=f'Polynomial Model (Degree {degree})', color='C2', linewidth=2)\n",
    "# Plot the polynomial model's predictions\n",
    "plt.plot(x_test, y_test_regularized_pred, label=f'Regularized Model (Degree {degree})', color='C4', linewidth=2)\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel('x values')\n",
    "plt.ylim(-5, 5)\n",
    "plt.ylabel('y values')\n",
    "plt.title('Comparison of Training Data, True Sine Curve, and Degree-6 Polynomial Model')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right;\">\n",
    "<i>This notebook has been created with the help of ChatGPT-4, 24.10.2024; Explanations were initially generated and afterwards edited;</i>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
